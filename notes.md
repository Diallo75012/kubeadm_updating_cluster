# Create Certificates For The Cluster
Go to source Kubernetes Documentation Which Has All Details: [Cert Creation Doc](https://kubernetes.io/docs/tasks/administer-cluster/certificates/)

## Kubeadm when initialized creates certificates automatically
- The certificates generated by kubeadm include:​
    - CA Certificates: These are the root certificates used to sign other certificates within the cluster.​
    - API Server Certificates: Used by the Kubernetes API server to establish secure communications.​
    - Kubelet Client Certificates: Used by the kubelet to authenticate to the API server.​
    - Etcd Certificates: Used for secure communication with the etcd key-value store.​

# KUBEADM ISSUE
- Kubelet not starting
- Certs are all expired for every components: `controller`, `api-server`, `scheduler`, `etcd`

1. Can renew certs using a simple command:
```bash
sudo kubeadm certs check-expiration
kubeadm certs renew all
# OR for specific ones
sudo kubeadm certs renew admin.conf
```

Which will renew the `admin.conf` embedded certs.

2. Embedded certs are base64 encoded
```bash
base64 -w 0 /etc/kubernetes/pki/ca.crt > ca.crt.base64
base64 -w 0 /etc/kubernetes/pki/admin.crt > admin.crt.base64
base64 -w 0 /etc/kubernetes/pki/admin.key > admin.key.base64
base64 -w 0 /etc/kubernetes/pki/apiserver-kubelet-client.crt > apiserver-kubelet-client.crt.base64
base64 -w 0 /etc/kubernetes/pki/apiserver-kubelet-client.key > apiserver-kubelet-client.key.base64
base64 -w 0 /etc/kubernetes/pki/front-proxy-client.crt > front-proxy-client.crt.base64
base64 -w 0 /etc/kubernetes/pki/front-proxy-client.key > front-proxy-client.key.base64
base64 -w 0 /etc/kubernetes/pki/apiserver-etcd-client.crt > apiserver-etcd-client.crt.base64
base64 -w 0 /etc/kubernetes/pki/apiserver-etcd-client.key > apiserver-etcd-client.key.base64
```

- can also manually convert those keys and past those in the configuration files:
eg:.
```bash
cat /etc/kubernetes/pki/apiserver-etcd-client.crt | base64
```

3. Cert can't be found for `admin.conf` in `../pki/..` folder:
Extract Keys from the `admin.conf` file for example to get the cert
- can also install `yq` which is a tool helping from the terminal to fetch `YAML` fields data, eg.:
```bash
sudo add-apt-repository ppa:rmescandon/yq
sudo apt update
sudo apt install yq -y
```
- then use it like that:
```bash
yq e '.users[0].user.client-certificate-data' /etc/kubernetes/admin.conf | base64 -d | sudo tee /etc/kubernetes/pki/admin.crt
```

## **Important**
- **Make Backups of certs and inital `YAML` files**

4. Documentation to understand certificates with some tables:
[Best Practices Kubernetes certificates](https://kubernetes.io/docs/setup/best-practices/certificates/)



# Controller Components Having a `.conf` File (present in `/etc/kubernetes/) Because Need To Authenticate To `Api-Server`

Note: Certs are base64 encoded and embedded directly within the admin.conf file. If needed to be extracted, decode necessary to get original cert back.

1. admin.conf
- Purpose: Provides administrative access to the Kubernetes API server, typically used by cluster administrators.​
- Default Location: /etc/kubernetes/admin.conf​
- Certificate Authority: /etc/kubernetes/pki/ca.crt​
- Client Certificate: Embedded within the admin.conf file under users.user.client-certificate-data​
- Client Key: Embedded within the admin.conf file under users.user.client-key-data​

2. controller-manager.conf
- Purpose: Used by the Kubernetes Controller Manager to authenticate with the API server.​
- Default Location: /etc/kubernetes/controller-manager.conf​
- Certificate Authority: /etc/kubernetes/pki/ca.crt​
- Client Certificate: Embedded within the controller-manager.conf file under users.user.client-certificate-data​
- Client Key: Embedded within the controller-manager.conf file under users.user.client-key-data​

3. scheduler.conf
- Purpose: Utilized by the Kubernetes Scheduler to authenticate with the API server.​
- Default Location: /etc/kubernetes/scheduler.conf​
- Certificate Authority: /etc/kubernetes/pki/ca.crt​
- Client Certificate: Embedded within the scheduler.conf file under users.user.client-certificate-data​
- Client Key: Embedded within the scheduler.conf file under users.user.client-key-data​
 
4. kubelet.conf
- Purpose: Allows the Kubelet agent, which runs on each node, to authenticate with the API server.​
devopscube.com
- Default Location: /etc/kubernetes/kubelet.conf​
- Certificate Authority: /etc/kubernetes/pki/ca.crt​
- Client Certificate: Typically located at /var/lib/kubelet/pki/kubelet-client-current.pem​
- Client Key: Typically located at /var/lib/kubelet/pki/kubelet-client-current.pem​



# Controller Components Without `.conf` file (present in `/etc/kubernetes/manifest/`) Because Need To Always Be Alive (Critical Components)

5. API Server (kube-apiserver):
- Purpose: The API server acts as the central management entity, validating and configuring data for API objects such as pods, services, and replication controllers.​
- Certificate Authority File: /etc/kubernetes/pki/ca.crt​
- Server Certificate and Key Files: /etc/kubernetes/pki/apiserver.crt and /etc/kubernetes/pki/apiserver.key​
- Client Certificates for etcd and kubelet:
  - etcd Client Certificate and Key Files: /etc/kubernetes/pki/apiserver-etcd-client.crt and /etc/kubernetes/pki/apiserver-etcd-client.key
    Description: Used by the API server to authenticate itself to the etcd server.
- kubelet Client Certificate and Key Files: /etc/kubernetes/pki/apiserver-kubelet-client.crt and /etc/kubernetes/pki/apiserver-kubelet-client.key
  Description: Used by the API server to authenticate itself to kubelets.

6. etcd:
- Purpose: etcd is a consistent and highly-available key-value store used as Kubernetes' backing store for all cluster data.​
- Certificate Authority Files: /etc/kubernetes/pki/etcd/ca.crt and /etc/kubernetes/pki/etcd/ca.key​
  Description: The root CA certificate and key for etcd, used to sign etcd server and peer certificates.​
- Server Certificate and Key Files: /etc/kubernetes/pki/etcd/server.crt and /etc/kubernetes/pki/etcd/server.key​
  Description: Used by etcd to serve secure (HTTPS) endpoints.​
- Peer Certificates Files: /etc/kubernetes/pki/etcd/peer.crt and /etc/kubernetes/pki/etcd/peer.key​
  Description: Used for secure communication between etcd peers in a cluster.​
- Client Certificates Files: /etc/kubernetes/pki/etcd/healthcheck-client.crt and /etc/kubernetes/pki/etcd/healthcheck-client.key​
  Description: Used by clients performing health checks on etcd.

# Folder Separation Of Concerns For Cluster-Wide Components and Node Components
## Folder For Cluster-Wide Components
- /etc/kubernetes/pki/ contains cluster-wide control plane certificates (API server, etcd, CA).
## Folder For Node Components
- /var/lib/kubelet/pki/ stores node-specific kubelet certificates.
Here for example the `Kubelet` client certificates will be stored as `Kubelet` on each node need a different key to authenticate to the `Api-Server`
**Here Unique `.pem` File As It Holds `crt` and `key`**: simplifies automatic renewal and storage (one file instead of two)

# `Admin.conf` and `Ca.crt`

## `Admin.conf`
- `Admin.conf`: is the same file that you are going to find in /home/<admin_user>/.kube/config
```bash
mkdir -p $HOME/.kube
sudo cp /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

## `Ca.crt` = **Root CA**
- `Ca.crt`: is the **authority** certificate for all the cluster controller components.
- Sign in all clients and server certificates.

`Admin.conf` is which is copied and changed permission to the `admin_user` home directory in `~/.kube/config`
is used by `kubectl` commands to authenticate to the cluster where `Api Server` will use the `Ca.crt` **Authority** to verify signature and permissions.

# Diagram of configs dependences on certs
(diagram)[https://excalidraw.com/#json=elokPxtHr6KkfK5rcWw0j,sOYYyGGT7QpDV1iBqvlfEg]


# MISSING KEYS, CERTS ETC....
## `sa.pub` file (/etc/kubernetes/pki/sa.pub) Missing
It is while checking into details the `yaml` files present in `/etc/kubernetes/manifest/` folder that I realized that
the `kube-apiserver.yaml` file is using also `sa.pub` that verifies signature service account tokens.
To recreate it I had to run this command:
```bash
openssl rsa -in /etc/kubernetes/pki/sa.key -pubout -out /etc/kubernetes/pki/sa.pub
```

## `controller-manager.crt and .key` (/etc/kubernetes/pki/..) Missing
- Need to recreate the keys and then to encode base64 and to update the `controller-manager.conf` file with those key/crt in the `user.client` section of the file

1. Generate a New Private Key
```bash
sudo openssl genrsa -out /etc/kubernetes/pki/controller-manager.key 2048
```

2. Create a Certificate Signing Request (CSR)
```bash
sudo openssl req -new -key /etc/kubernetes/pki/controller-manager.key \
-out /etc/kubernetes/pki/controller-manager.csr \
-subj "/CN=system:kube-controller-manager/O=system:kube-controller-manager"
```

3.  Sign the Certificate Using the Kubernetes CA
```bash
sudo openssl x509 -req -in /etc/kubernetes/pki/controller-manager.csr \
  -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key \
  -CAcreateserial -out /etc/kubernetes/pki/controller-manager.crt \
  -days 365 -sha256
Outputs:
Certificate request self-signature ok
subject=CN = system:kube-controller-manager, O = system:kube-controller-manager
```

4. Verify the Certificate
```bash
sudo openssl x509 -in /etc/kubernetes/pki/controller-manager.crt -noout -text | grep -E 'Subject:|Issuer:'
        Issuer: CN = kubernetes
        Subject: CN = system:kube-controller-manager, O = system:kube-controller-manager
Outputs:
Issuer: CN = kubernetes
        Subject: CN = system:kube-controller-manager, O = system:kube-controller-manager
```

## `admin.crt and .key` (/etc/kubernetes/pki/..) Missing
- need to recreate those following same steps as above but just changing names so:
  - Generate key
  - Create CSR
  - Sign using Kubernetes `ca.crt` and `ca.key`
  - Verify Certificate (just checking...)
```bash
sudo openssl genrsa -out /etc/kubernetes/pki/admin.key 2048
sudo openssl req -new -key /etc/kubernetes/pki/admin.key \
  -out /etc/kubernetes/pki/admin.csr \
  -subj "/CN=kubernetes-admin/O=system:masters"
sudo openssl req -new -key /etc/kubernetes/pki/admin.key \
  -out /etc/kubernetes/pki/admin.csr \
  -subj "/CN=kubernetes-admin/O=system:masters"
sudo openssl x509 -req -in /etc/kubernetes/pki/admin.csr \
  -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key \
  -CAcreateserial -out /etc/kubernetes/pki/admin.crt \
  -days 365 -sha256
Outputs:
Certificate request self-signature ok
subject=CN = kubernetes-admin, O = system:masters
sudo openssl x509 -in /etc/kubernetes/pki/admin.crt -noout -text | grep -E 'Subject:|Issuer:'
Outputs:
Issuer: CN = kubernetes
Subject: CN = kubernetes-admin, O = system:masters

```


# How to recreate `.pem` file that `kubelet` need and which is located at `/var/lib/kubelet/pki/` ?
Normally Kubernetes create it auromatically but you can use a simple command to recreate it:
- just use the `kubeadm` command:
```bash
sudo kubeadm certs renew all
sudo systemctl restart kubelet
```

OR manually (but i don't see why we do it manually when it should present in each node and it is a different .pem content for each node)
```bash
sudo openssl genrsa -out /var/lib/kubelet/pki/kubelet-client.key 2048
sudo openssl req -new -key /var/lib/kubelet/pki/kubelet-client.key \
  -out /var/lib/kubelet/pki/kubelet-client.csr \
  -subj "/CN=system:node:controller.creditizens.net/O=system:nodes"
openssl x509 -req -in /var/lib/kubelet/pki/kubelet-client.csr \
  -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key \
  -CAcreateserial -out /var/lib/kubelet/pki/kubelet-client-current.pem \
  -days 365 -sha256
sudo systemctl restart kubelet
```

**Important**:
- The `Kubelet` `.pem` file located at `/var/kubernetes/pki/` is unique in each nodes, therefore, can be manually created only on the `controller` node.
- If a worker node loses or have issues with that file, it needs to request for a new one as it got the file from when it had joined the cluster:
```bash
sudo kubeadm join --token <your-token> --discovery-token-ca-cert-hash sha256:<hash>
```
- If a worker node need a new `.pem` file it need to approve pending `csr`:
```bash
kubectl get csr
kubectl certificate approve <csr-name>
# if too many <csr-name> in `pending` mode and too many to do it manually just run this command
# which will" get all csr | filter pending one | run the cert approval for each of those
kubectl get csr | awk '/Pending/ {print $1}' | xargs kubectl certificate approve
```

# `/var/lib/kubelet/pki/` `.pem` file symlink is pointing to outdated `.pem` file
The `...current.pem` file is pointing to an outdated one, so we need to make a backup of the outdated one and use the sigle command to renew certs, see issue when `ls -la` folder:
```bash
sudo ls -la /var/lib/kubelet/pki
total 28
drwxr-xr-x 2 root root 4096 mars  12 20:52 .
drwx------ 8 root root 4096 juil.  7  2023 ..
-rw------- 1 root root 1062 mars  12 20:53 kubelet-client-2023-07-07-09-39-43.pem
-rw-r--r-- 1 root root  960 mars  12 20:53 kubelet-client.csr
lrwxrwxrwx 1 root root   59 juil.  7  2023 kubelet-client-current.pem -> /var/lib/kubelet/pki/kubelet-client-2023-07-07-09-39-43.pem
-rw------- 1 root root 1704 mars  12 20:52 kubelet-client.key
-rw-r--r-- 1 root root 2371 juil.  7  2023 kubelet.crt
-rw------- 1 root root 1675 juil.  7  2023 kubelet.key
```
**Fix:**
- we backup the outdated .epm file or just delete it
```bash
sudo mv kubelet-client-2023-07-07-09-39-43.pem BAK_kubelet-client-2023-07-07-09-39-43.pem
# OR just : sudo rm -rf kubelet-client-2023-07-07-09-39-43.pem
```
- if new file present: we can just create new symlink
```bash
sudo ln -sf /var/lib/kubelet/pki/kubelet-client-<new-date>.pem /var/lib/kubelet/pki/kubelet-client-current.pem
```
- if not what is my case (the best ever), we create a new one from scratch:
```bash
sudo kubeadm certs renew kubelet
sudo ln -sf /var/lib/kubelet/pki/kubelet-client.pem /var/lib/kubelet/pki/kubelet-client-current.pem
sudo systemctl restart kubelet
```
- i have finally deleted everything from thi `/var/lib/kubelet/pki/` folder to recreate all certs so that everything is renewed and good for learning
```bash
sudo rm -rf /var/lib/kubelet/pki/*
```
so we recreate all like that:
```bash
# recreate key
sudo openssl genrsa -out /var/lib/kubelet/pki/kubelet-client.key 2048
# check
sudo ls -lah /var/lib/kubelet/pki/
Outputs:
total 20K
drwxr-xr-x 2 root root 4,0K mars  12 21:23 .
drwx------ 8 root root 4,0K juil.  7  2023 ..
-rw------- 1 root root 1,7K mars  12 21:30 kubelet-client.key
-rw-r--r-- 1 root root 2,4K juil.  7  2023 kubelet.crt
-rw------- 1 root root 1,7K juil.  7  2023 kubelet.key
# create csr
sudo openssl req -new -key /var/lib/kubelet/pki/kubelet-client.key \
    -out /var/lib/kubelet/pki/kubelet-client.csr \
    -subj "/CN=system:node:controller.creditizens.net/O=system:nodes"
# verify the csr
sudo openssl req -in /var/lib/kubelet/pki/kubelet-client.csr -noout -text
Outputs:
Certificate Request:
    Data:
        Version: 1 (0x0)
        Subject: CN = system:node:controller.creditizens.net, O = system:nodes
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                Public-Key: (2048 bit)
                Modulus:
                    00:e6:d6:13:89:65:c9:61:c4:bc:7f:bd:3d:0e:99:
                    bb:50:9f:6f:74:48:31:20:ef:12:de:92:a6:b1:b6:
                    31:56:6f:fb:99:15:2f:b8:aa:4b:a1:d9:6d:ec:a3:
                    95:7f:45:11:6e:0b:8e:7f:2b:b0:3d:80:3d:7d:f7:
                    ce:04:61:c5:f7:79:06:d8:40:ea:7a:d2:b4:e2:c9:
                    cb:5d:84:2e:98:f5:f0:e2:9d:d8:89:87:3f:77:74:
                    ec:03:ec:25:1b:da:82:eb:d0:2a:57:42:77:d7:b0:
                    a1:88:2b:e5:43:b4:25:01:44:ec:4b:05:88:34:10:
                    b6:f6:58:9f:3e:f8:e5:73:e3:1b:6c:7b:04:14:a8:
                    27:14:36:74:f3:63:67:56:d9:d1:c6:05:19:98:18:
                    0b:fd:ea:b6:69:12:4c:99:79:aa:58:b1:5b:b5:3b:
                    1f:10:c8:47:7a:8e:6a:79:49:8b:52:8f:b8:b4:ef:
                    5f:1c:02:2f:97:ce:35:c1:3b:db:09:f6:9a:61:ef:
                    e0:88:d8:04:d6:cb:76:33:77:95:aa:19:3b:ff:80:
                    56:ed:c0:01:a6:f9:07:2b:78:4f:fa:89:7f:ab:10:
                    41:4e:d6:67:c8:65:b4:2c:a2:31:c4:67:a7:3f:56:
                    81:90:5f:e2:d4:e4:84:18:44:63:43:70:23:02:b5:
                    58:9b
                Exponent: 65537 (0x10001)
        Attributes:
            (none)
            Requested Extensions:
    Signature Algorithm: sha256WithRSAEncryption
    Signature Value:
        78:b3:b9:9d:10:a3:da:c5:c4:b5:87:e1:5f:fd:83:d7:21:27:
        f0:d7:fd:48:c7:f8:b7:e0:70:b7:de:66:00:71:2d:72:bb:e3:
        6f:25:0f:b7:f8:04:47:94:5f:5b:7a:4a:00:12:ea:b8:b7:54:
        5a:87:76:5a:07:79:68:8b:8e:d5:f8:2c:50:f9:cf:c9:97:6f:
        71:20:12:48:4a:c9:66:0d:a0:ec:41:fe:67:46:5b:9d:63:a7:
        6b:85:c1:80:92:a6:82:43:f3:0f:67:c2:08:97:05:5b:7d:f2:
        95:d1:93:1b:3f:d8:60:9f:da:3f:50:32:b3:46:3a:dd:31:58:
        c0:26:7f:f9:1f:77:ea:f5:f7:94:cb:bb:b2:4a:d8:17:02:a1:
        13:b1:0a:8c:84:5c:dd:af:df:d6:f5:e5:81:91:4f:f7:00:d6:
        58:21:33:8e:70:ce:4b:cc:2a:5a:83:ed:ea:cb:30:b3:8f:31:
        85:f1:f8:b6:8c:fb:66:0a:9b:b2:18:29:b6:7d:57:9d:52:b4:
        9e:66:f2:eb:37:8b:91:49:15:17:60:df:84:bd:5e:dd:1e:1b:
        38:4a:ec:ec:eb:5c:cf:a2:c3:b5:0c:8b:7a:a7:03:68:7e:6d:
        7e:d9:c4:ab:26:e8:a7:40:ea:67:0d:5d:7f:30:15:8e:a9:91:
        e5:e3:7f:6b
# sign the csr with Kubernetes CA
sudo openssl x509 -req -in /var/lib/kubelet/pki/kubelet-client.csr \
    -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key \
    -CAcreateserial -out /var/lib/kubelet/pki/kubelet-client-current.pem \
    -days 365
Outputs:
Certificate request self-signature ok
subject=CN = system:node:controller.creditizens.net, O = system:nodes
# change permissions
sudo chown root:root /var/lib/kubelet/pki/kubelet-client-current.pem
sudo chmod 600 /var/lib/kubelet/pki/kubelet-client-current.pem
# check cert validity
sudo openssl x509 -in /var/lib/kubelet/pki/kubelet-client-current.pem -noout -text | grep "validity"
# fix symlink
sudo ln -sfn /var/lib/kubelet/pki/kubelet-client-current.pem /var/lib/kubelet/pki/kubelet-client.pem
# check symlink
sudo ls -la /var/lib/kubelet/pki/
Outpus:
total 28
drwxr-xr-x 2 root root 4096 mars  12 21:35 .
drwx------ 8 root root 4096 juil.  7  2023 ..
-rw-r--r-- 1 root root  960 mars  12 21:31 kubelet-client.csr
-rw-r--r-- 1 root root 1062 mars  12 21:33 kubelet-client-current.pem
-rw------- 1 root root 1704 mars  12 21:30 kubelet-client.key
lrwxrwxrwx 1 root root   47 mars  12 21:35 kubelet-client.pem -> /var/lib/kubelet/pki/kubelet-client-current.pem
-rw-r--r-- 1 root root 2371 juil.  7  2023 kubelet.crt
-rw------- 1 root root 1675 juil.  7  2023 kubelet.key
# create missing `kubelet.crt` and `kubelet.key`
sudo openssl genrsa -out /var/lib/kubelet/pki/kubelet.key 2048
sudo openssl req -new -key /var/lib/kubelet/pki/kubelet.key -out /var/lib/kubelet/pki/kubelet.csr \
    -subj "/CN=kubelet"
sudo openssl x509 -req -in /var/lib/kubelet/pki/kubelet.csr \
    -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key \
    -CAcreateserial -out /var/lib/kubelet/pki/kubelet.crt -days 365
sudo chmod 600 /var/lib/kubelet/pki/kubelet.key
sudo chmod 644 /var/lib/kubelet/pki/kubelet.crt
# check
sudo openssl x509 -in /var/lib/kubelet/pki/kubelet.crt -noout -text
# restart and check kubelet service
sudo systemctl restart kubelet
sudo systemctl status kubelet
```

# SOLUTION FOR KUBELET .PEM CERT/KEY RENEWAL AND PRESENCE
- the first issue was that certificates were outdated, therefore, i have changed them all
- next issue was that the `.pem` file of kubelet wasn't holding `crt` and `key` but only the `key`: 
    - therefore, i have delete those files from the `/var/lib/kubelet/pki/` and recreated everything from scratch
      but changed the process by manually adding the two keys in the `.pem` files 
- lesson need a good understanding of all the files and permissions and where those are located, their config files. 
    That is why I have made the detailed diagram that shows where are the leys, certs and configs and used arrows to show where they pointing to.
- After when `kubelet` was back i had to wait a bit before getting the `kubectl` command back and the visibility on `nodes`, `pods` etc...
- terminal output review of what has been done for the last step to fix it:
```bash
# stop kubelet (not need it wouldn't start LOL)
sudo systemctl stop kubelet
# Remove Incorrect Files
sudo rm -rf /var/lib/kubelet/pki/kubelet-client*.pem
sudo rm -rf /var/lib/kubelet/pki/kubelet.crt /var/lib/kubelet/pki/kubelet.key
sudo rm -rf /var/lib/kubelet/pki/kubelet-client.csr
'''
- Only this left in the folder but can empty it fully and recreate all keys:
sudo ls -la /var/lib/kubelet/pki/
total 20
drwxr-xr-x 2 root root 4096 mars  12 23:24 .
drwx------ 8 root root 4096 juil.  7  2023 ..
-rw-r--r-- 1 root root  960 mars  12 23:24 kubelet-client.csr
-rw------- 1 root root 1704 mars  12 23:14 kubelet-client.key
-rw-r--r-- 1 root root  887 mars  12 21:37 kubelet.csr
'''

'''
If wanted to recreate all kubelet.key, kubelet.csr, kubelet.crt can run:
sudo openssl genrsa -out /var/lib/kubelet/pki/kubelet.key 2048
sudo openssl req -new -key /var/lib/kubelet/pki/kubelet.key -out /var/lib/kubelet/pki/kubelet.csr \
    -subj "/CN=kubelet"
sudo openssl x509 -req -in /var/lib/kubelet/pki/kubelet.csr \
    -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key \
    -CAcreateserial -out /var/lib/kubelet/pki/kubelet.crt -days 365
sudo chmod 600 /var/lib/kubelet/pki/kubelet.key
sudo chmod 644 /var/lib/kubelet/pki/kubelet.crt
'''

# Generate a New CSR (Certificate Signing Request)
sudo openssl genrsa -out /var/lib/kubelet/pki/kubelet-client.key 2048
sudo openssl req -new -key /var/lib/kubelet/pki/kubelet-client.key \
-out /var/lib/kubelet/pki/kubelet-client.csr \
-subj "/CN=system:node:controller.creditizens.net/O=system:nodes"
# Sign the CSR to Generate the Correct Certificate
sudo openssl x509 -req -in /var/lib/kubelet/pki/kubelet-client.csr \
-CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key \
-CAcreateserial -out /var/lib/kubelet/pki/kubelet-client.crt \
-days 365
# Create the Correct kubelet-client-current.pem
# This file must contain both the private key and the certificate.
# that is the super command that manually added `crt` and `key` in the `.pem` file
sudo cat /var/lib/kubelet/pki/kubelet-client.key /var/lib/kubelet/pki/kubelet-client.crt | sudo tee /var/lib/kubelet/pki/kubelet-client-current.pem
# Fix Permissions
sudo chown root:root /var/lib/kubelet/pki/kubelet-client-current.pem
sudo chmod 600 /var/lib/kubelet/pki/kubelet-client-current.pem
# restart kubelet
sudo systemctl restart kubelet
# check status (should be fine)
sudo systemctl status kubelet
# check journal eventually
sudo journalctl -u kubelet --no-pager --lines=50
# wait a bit and then start running kubectl commands
kubectl get pods
kubectl get nodes
...etc...
```

# For worker nodes how to fix the issue of certificate of kubelet expired
- make sure you have an ssh connection between worker node and controller node as we are going to more files to be signed there as CA authority `.key` file is only there
```bash
# on worker and controller
sudo apt install ssh
sudo systemctl status ssh
ssh-keygen
# then from controller to worker copy the key so that controller is part of the known hosts
ssh-copy-id -i ~/.ssh/id_rsa.pub creditizens@node1.creditizens.net
```
- on the worker node you can delete everything from the folder `/var/lib/kubelet/pki/` then restart `kubelet service`: `sudo systemctl restart kubelet`.
  Then this will automatically populate the folder `var/lib/kubelet/pki/` with new `kubelet.crt` and `kubelet.key` that you are going to use for next command:
```bash
sudo openssl req -new -key /var/lib/kubelet/pki/kubelet.key \
    -out /var/lib/kubelet/pki/kubelet-client.csr \
    -subj "/CN=system:node:node1-creditizens.net/O=system:nodes"
```

- Copy the CSR to the Controller Node
Since the worker node doesn’t have /etc/kubernetes/pki/ca.key, we must sign the CSR on the controller.
Run this command on the worker node to transfer the CSR file:
```bash
scp /var/lib/kubelet/pki/kubelet-client.csr creditizens@controller.creditizens.net>:/tmp/
```

- Sign the CSR on the Controller Node
Now, on the controller node, sign the CSR using the CA:
```bash
sudo openssl x509 -req -in /tmp/kubelet-client.csr \
    -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key \
    -CAcreateserial -out /tmp/kubelet-client.crt \
    -days 365
```
This signs the CSR, producing kubelet-client.crt, which must be combined with the key.

- Copy the Signed Certificate Back to the Worker Node
On the controller, transfer the signed certificate back to the worker:
```bash
scp /tmp/kubelet-client.crt creditizens@node1.creditizens.net:/tmp/
```

- Now, on the worker node, move it into place:
```bash
sudo mv /tmp/kubelet-client.crt /var/lib/kubelet/pki/kubelet-client.crt
```

- Create the Correct kubelet-client-current.pem
The kubelet-client-current.pem file must contain both the certificate and the private key.
Run this command on the worker node to combine them into a proper PEM file:
```bash
sudo cat /var/lib/kubelet/pki/kubelet-client.crt /var/lib/kubelet/pki/kubelet.key | sudo tee /var/lib/kubelet/pki/kubelet-client-current.pem
```

-  Set the Correct Permissions
```bash
sudo chown root:root /var/lib/kubelet/pki/kubelet-client-current.pem
sudo chmod 600 /var/lib/kubelet/pki/kubelet-client-current.pem
```

- Restart Kubelet
```bash
sudo systemctl restart kubelet
sudo systemctl status kubelet
```


# ISSUE WITH SCHEDULER CRASHBACKLOOP BECAUSE OF CERTIFICATE EXPIRED
- state of Scheduler in Cluster before fix
```bash
# kubectl get pods -n kube-system
kube-scheduler-controller.creditizens.net            0/1     CrashLoopBackOff   57 (2m27s ago)   615d
# logs
kubectl logs kube-scheduler-controller.creditizens.net -n kube-system
I0313 17:04:47.970993       1 serving.go:348] Generated self-signed cert in-memory
E0313 17:04:47.972942       1 run.go:74] "command failed" err="error loading config file \"/etc/kubernetes/scheduler.conf\": illegal base64 data at input byte 25"
```

- Check authority certificate expiry date
```bash
sudo openssl x509 -in /etc/kubernetes/pki/front-proxy-ca.crt -noout -dates
Outputs:
notBefore=Jul  7 07:39:41 2023 GMT
notAfter=Jul  4 07:39:41 2033 GMT  STILL VALID!
```

- Check client certiifcate expiry date
```bash
sudo openssl x509 -in /etc/kubernetes/pki/front-proxy-client.crt -noout -dates
Outputs:
notBefore=Jul  7 07:39:41 2023 GMT
notAfter=Mar 10 18:21:00 2026 GMT EXPIRED NEED CREATE A NEW ONE!
```

### HOW TO CREATE A NEW ONE
```bash
# Delete scheduler client keys
sudo rm -rf /etc/kubernetes/pki/front-proxy-client.crt /etc/kubernetes/pki/front-proxy-client.key
```
```bash
# generate a new key
sudo openssl genrsa -out /etc/kubernetes/pki/front-proxy-client.key 2048
```
```bash
# create certificate signing request
sudo openssl req -new -key /etc/kubernetes/pki/front-proxy-client.key -subj "/CN=front-proxy-client" -out /etc/kubernetes/pki/front-proxy-client.csr
```
```bash
# sign the key with the authority certificate
sudo openssl x509 -req -in /etc/kubernetes/pki/front-proxy-client.csr -CA /etc/kubernetes/pki/front-proxy-ca.crt -CAkey /etc/kubernetes/pki/front-proxy-ca.key -CAcreateserial -out /etc/kubernetes/pki/front-proxy-client.crt -days 365
```

# SCHEDULER IS RUNNING FINE NOW BUT LOGS SHOW TLS ISSUES

- Check Authority CA expiry date:
```bash
openssl x509 -in /etc/kubernetes/pki/ca.crt -noout -dates
Outputs:
notBefore=Jul  7 07:39:41 2023 GMT
notAfter=Jul  4 07:39:41 2033 GMT
```

- Copy the Authority CA cert to `/usr/local/share...` and update certificates:
```bash
sudo cp -f /etc/kubernetes/pki/ca.crt /usr/local/share/ca-certificates/kubernetes.crt
# for cert to be trusted server wide this need to be done on each `controller` nodes
sudo update-ca-certificates
```

# TO DELETE ALL PENDING OR RUNNING POD WITH ONE COMMAND
- Pending
```bash
kubectl delete pods -n kube-system --field-selector=status.phase=Pending
```
- Running
```bash
kubectl delete pods -n kube-system --field-selector=status.phase=Running
```

____________________________________________________________________________

After all of those struggles finally found the right way to do it:

# Utilities command to check cert expiry and more
- check certificates expiry dates
```bash
(sudo) openssl x509 -in <crt_file_path> -noout -dates
```
- check journal of a specify service
```bash
journalctl -u <service_name> -n <number_of_line_(end of file)_logs> --no-pager
```

# On All Worker Nodes:
Those files will be recreated at the end when we re-join worker to the cluster

1. delete all keys and config files
```bash
# maybe need to do those one by one manually and make sure to do the symlinked file first before it linked file
# otherwise you will need to create a dummy one until you delete that file and then delete your dummy file
sudo rm -rf /var/lib/kubelet/pki/*
# delete ca.crt and kubelet config
sudo rm -rf /etc/kubernetes/pki/ca/crt
sudo rm -rf /etc/kubernetes/kubelet.conf
```

# On the Controller Node
We used `kubeadm` to create the cluster so let's use it's utilities commands to renew all certs and config files
We will also delete all files that are not needed but needs to be regenreated

1. Remove expired kubelet certificates/config
```bash
# do it manually for all files in the folder and delete symlinked file (the file with `->`) before the other
sudo rm -f /var/lib/kubelet/pki/*
sudo rm -f /etc/kubernetes/kubelet.conf
```

2. Recreate kubelet.conf for control-plane
```bash
sudo kubeadm init phase kubeconfig all
# if `kubelet.conf` is not recreated use the following command to target it alone (it will recreate it)
sudo kubeadm init phase kubeconfig kubelet
```

3. Ensure rotation enabled (should already be done)
```
# ensure rotateCertificates: true
sudo nano /var/lib/kubelet/config.yaml
```

4. Restart kubelet
```
sudo systemctl restart kubelet
```

5. Check kubelet and cluster status
```bash
sudo systemctl status kubelet
kubectl get nodes
```

Now `kubectl` command should work and controller node should appear fine
but the other `worker` nodes should be `NotReady`.

It is time now to regenerate a token to join the cluster again
which will recreate certs and config files in `worker` nodes.


6. create the token that will be applied on worker node to join the cluster
```bash
kubeadm token create --print-join-command
```

# On Each Worker Node
7. past the command to join the cluster
    after having made sure that you have deleted all `.crt`, `.key` and `.pem`
    from `/var/lib/kubelet/pki/` and `/etc/kubernetes/pki/`
    and `.conf` file from `/etc/kubernetes/` folders
```bash
sudo kubeadm token create --print-join-command
```

8. restart kubelet and check it (accessory can also restart containerd but I didn't need myselfu)
```bash
sudo systemctl restart kubelet
sudo systemctl status kubelet
```

# Check The Controller Node
9. check that `workers` are nw back to work with status `Ready`
```bash
kubectl get nodes -o wide
```

10. Test a deployment and scale it and expose it and check if you see the app in browser, then tear down
```bash
# create namesapce
kubectl create ns nginx
# create deplyment
kubectl create deployment test-nginx -ns nginx --image=nginx
# scale deplyment
kubectl scale deployment test-nginx  -ns nginx --replicas=3
# check deployment and pods
kubectl get deployments test-nginx -ns nginx
kubectl get pods -l app=test-nginx -ns nginx
# expose deplyment through service creation for it
kubectl expose deployment test-nginx -ns nginx --port=80 --type=NodePort
kubectl get svc test-nginx -ns nginx
# can curl or just url in the browser the port is the one on the `right side 80:<port_for_brower>`
curl http://localhost:31234
# tear all down
kubectl delete svc test-nginx -ns nginx
kubectl delete deployment test-nginx -ns nginx
kubectl delete ns nginx
```

________________________________________________________________________________________________________________
# Next
- [ ] update the cluster versions until we reach 1.32 (we are at 1.27)
    so we will have to do same process several times and fix any compatibility issues along the way.
    need to check supported versions ranges for each kubeadm updated version


# Take Snapshot of ETCD (for Safety)

## 1. Get the etcd Pod Name
```bash
kubectl get pods -n kube-system | grep etcd
Outputs:
etcd-controller-01   1/1   Running   0   1d
```

## 2. Create a Snapshot of etcd
```bash
sudo ETCDCTL_API=3 etcdctl snapshot save /var/lib/etcd/etcd-snapshot-$(date +%F-%T).db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key
```
This saves a snapshot with a timestamped filename in /var/lib/etcd/.
You can change the path if needed.

## 3. Verify the Snapshot Integrity
```bash
sudo ETCDCTL_API=3 etcdctl snapshot status /var/lib/etcd/etcd-snapshot-YYYY-MM-DD-HH-MM-SS.db
```

## 4. Etra: Restoring etcd from Snapshot (if Needed)
```bash
sudo ETCDCTL_API=3 etcdctl snapshot restore /var/lib/etcd/etcd-snapshot-YYYY-MM-DD-HH-MM-SS.db \
  --data-dir /var/lib/etcd-new
```
**Then, update the etcd manifest in `/etc/kubernetes/manifests/etcd.yaml` to point to `/var/lib/etcd-new`, and restart the control plane.**

# Uprgade Cluster:
- **[source: Kubernetes Documentation](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/#:~:text=During%20upgrade%20kubeadm%20writes%20the,etc%2Fkubernetes%2Ftmp)**
###  **Rules:**
- kubeadm cluster updates cn be done onl one minor version at a time 1.27 -> 1.28 -> 1.29...
- kubelet and containerd versions cn e max 3 versions older see documentation to see their corresponding versions but it is recommended to have those not more than on version older
- Need to do controller nodes first one at a time
- Need to to do the worker nodes one at a time
- Need for sure to know the architecture of taints/toleration/resource limits to make sure for seamless workload move to other nodes.
- Need also to see upgrades matching of Calico (but used the latest available the free one)
- Need to Backup etcd even if kubeadm doesn it automaicall placing backups to `/etc/kubernetes/tmp`
- Need to make sure version are `unhold` then upgrade and `hold` thse back for verions to not upgrade and have full control on stable cluster state 


**Important:** For `1` to `3` you can do the upgrade and then cordon and drain or the otherway around 
    as new state application to the cluster is done only with the command of `4` of `4'`

## 1. Cordon Node to Separa 
```bash
kubectl cordon <controller-node-name OR worker-node-name>
```

## 2. Drain Node: Make Node Unschedulable To prepare For Maintenance
```bash
kubectl drain <node-to-drain> --ignore-daemonsets --delete-emptydir-data
```

## 3. Upgrade Kubeadm to next minor version 1.27 -> 1.28
- make sure you have the repo in `apt` if not install it:
```bash
sudo apt update && sudo apt install -y curl apt-transport-https
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /usr/share/keyrings/kubernetes-archive-keyring.gpg
echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
# check the versions
c
# then run next command with the right version:
`sudo apt install -y kubeadm=1.28.10-00`
```
# get rid of deprecated kubernetes repo list
sudo rm /etc/apt/sources.list.d/kubernetes.list

# disable swap
sudo apt update
sudo apt install -y curl apt-transport-https
# get the signing key
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /usr/share/keyrings/kubernetes-archive-keyring.gpg
# add kubernetes apt repo
echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
# update package list
sudo apt update

# on all nodes disable swap and add kernel settings
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

# set up some kernel configs
sudo tee /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF

# load necessary kernel modules
sudo modprobe overlay
sudo modprobe br_netfilter
sudo tee /etc/sysctl.d/kubernetes.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF
Outputs:
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1


# reload changes
sudo sysctl --system
# check versions available to do the update for kubeadm kubelet and kubectl
sudo apt-cache madison kubeadm
sudo apt-cache madison kubelet
sudo apt-cache madison kubectl
```bash
sudo apt-mark unhold kubeadm kubelet kubectl && \
sudo apt-get update && \
sudo apt-get install -y kubeadm=1.28.15-1.1 kubelet=1.28.15-1.1 kebectl=1.28.15-1.1 && \
sudo apt-mark hold kubeadm kubelet kubectl
```

## . Install Compatible Version of Containerd (Optional but better have it updated even if it is Ok for few years...)
```bash
sudo apt-get install -y containerd.io=<required-version>
sudo apt install containerd.io=1.7.25-1
sudo systemctl restart containerd
```

## 4. Verify Upgrade Plan and Ugrade (Only in the first Control Plane: other ones are going to pick it up)
This is for the first control plane node only
```bash
# if having only one control node need to uncordon it  and restart kubelet and containerd
# to have the node `Ready` otherwise you won't be able to upgrade
kubectl uncorn controller.creditizens.net
sudo systemctl restart kubelet containerd
sudo kubeadm upgrade plan
sudo kubeadm upgrade apply v1.28.x
```

## 4'. Upgrade Other Control Planes (Optional if more than one control plane)
This is after having ugraded the first control plane and the other ones don't use `apply` but use `upgrade node` instead
Also no need to `uprgade plan` in those controller nodes.
```bash
sudo kubeadm upgrade node
```



## . Restart Kubelet
```bash
sudo systemctl daemon-reload
sudo systemctl restart kubelet
```

## . Bring Node Back Online
```bash
kubectl uncordon <node-to-uncordon>
```

## . Optionally Check that critical add-ons (CoreDNS, kube-proxy) are running the updated versions
```bash
kubectl get daemonset kube-proxy -n kube-system -o=jsonpath='{.spec.template.spec.containers[0].image}'


_________________________________________________

For worker nodes
- on controller node
kubectl cordon <controller-node-name OR worker-node-name>
```## 2. Drain Node: Make Node Unschedulable To prepare For Maintenance
```bash
kubectl drain <node-to-drain> --ignore-daemonsets --delete-emptydir-data

- on worker node
sudo apt update && sudo apt install -y curl apt-transport-https
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /usr/share/keyrings/kubernetes-archive-keyring.gpg
echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update -y


# on all nodes disable swap and add kernel settings
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

# set up some kernel configs
sudo tee /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF

# load necessary kernel modules
sudo modprobe overlay
sudo modprobe br_netfilter
sudo tee /etc/sysctl.d/kubernetes.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF
Outputs:
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1

sudo sysctl --system

kubeadm version
kubectl version
kubelet --version
containerd --version

# just check if version is there otherwise error with signing key or repo in `/etc/source.list.d/kubernetes.list`
sudo apt-cache madison kubeadm
sudo apt-cache madison kubelet
sudo apt-cache madison kubectl

sudo apt-mark unhold kubeadm kubectl kubelet
sudo apt install kubeadm=1.28.15-1.1 kubectl=1.28.15-1.1 kubelet=1.28.15-1.1 -y
sudo apt update

containerd --version

# optional trick to get the terminal show you the true version to get but from `containerd.io=<version>`
sudo apt install containerd=1.7
Outputs:
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
Package containerd is a virtual package provided by:
  containerd.io 1.7.25-1
You should explicitly select one to install.

# othersize just install it direcly, this is done having checked the version matching at: [containerd.io doc](https://containerd.io/releases/#:~:text=Kubernetes%20Version%20containerd%20Version%20CRI,36%2Bv1)
sudo apt install containerd.io=1.7.25-1

containerd --version
kubelet --version
Kubernetes v1.28.15
kubectl version
kubeadm version

# perform the upgrade
kubeadm upgrade node

sudo systemctl daemon-reload
sudo systemctl restart kubelet
sudo systemctl restart kubelet containerd
sudo systemctl status kubelet containerd
```

## . Restart Kubelet
```bash
sudo systemctl daemon-reload
sudo systemctl restart kubelet


- on controller node
```bash
kubectl uncordon <node-to-uncordon>
```

___________________________________________________________________________________________

# Kubernetes Plugins 

External utilities can be installed and used to interact with Kubernetes.
The easiest way to manage those is by installing `Krew` which will be used in combinaison of `kubectl`.

### install `krew`
source: [Krew Install](https://krew.sigs.k8s.io/docs/user-guide/setup/install/)
1. Install using this command (do not omit the parenthesis):
```bash
(
  set -x; cd "$(mktemp -d)" &&
  OS="$(uname | tr '[:upper:]' '[:lower:]')" &&
  ARCH="$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\(arm\)\(64\)\?.*/\1\2/' -e 's/aarch64$/arm64/')" &&
  KREW="krew-${OS}_${ARCH}" &&
  curl -fsSLO "https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz" &&
  tar zxvf "${KREW}.tar.gz" &&
  ./"${KREW}" install krew
)
```
2. add to `~/.bashrc` the export to add it to `PATH`:
```bash
export PATH="${KREW_ROOT:-$HOME/.krew}/bin:$PATH"
source ~/.bashrc
```
3. we can now use `krew` with `kubectl` to install plugins
```bash
# here plugin that will display the yaml/josn in a `neat` clean way when for example extracting those from cluster
kubectl krew install neat
```
4. now use the plugin to get here nice `yaml` or `json` output of resources
```bash
kubectl get deployment nginx -n nginx -o yaml | kubectl neat > nginx-deployment.yaml
```
- outputs:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "3"
  labels:
    app: nginx
  name: nginx
  namespace: nginx
spec:
  progressDeadlineSeconds: 600
  replicas: 3
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: nginx
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
```
___________________________________________________________________________________________________________
# Kubernetes Patching Deployment
Here we will use a side `.yaml` file to patch our deployment:
- if the keys names are the same as in the initial deployment the field will be updated
- if the keys names are different those will be added to the deployment
- therefore, **make sure you choose the name of the keys carfully**

- here have added a `configMap` as a mounted volume to the `nginx` deployment
```bash
cat config-map.yaml 
```
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-html
  namespace: nginx
data:
  index.html: |
    <h1 style="color:red;">Creditizens Customized Page Red</h1>
```

- here we create the side file that target the fields that will be updated
```bash
cat nginx-deployment-patching.yaml 
```
```yaml
spec:
  template:
    spec:
      containers:
      - name: nginx
        image: nginx
        volumeMounts:
        - name: nginx-html-conf
          mountPath: /usr/share/nginx/html/
      volumes:
      - name: nginx-html-conf
        configMap:
          name: nginx-html
```

- now patch deployment and you will see that nginx pages are displaying the red message `Creditizens Page Red`
```bash
# apply patch
kubectl patch deployment nginx -n nginx --type merge --patch-file nginx-deployment-patching.yaml
# update deployment (will start new pod and make the rolling update of those with default 25% surge)
kubectl rollout restart deployment/nginx -n nginx
```

_______________________________________________________________________________________

# Kubernetes `kubectl` auto-completion
source: [aliases and completion `kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#enable-shell-autocompletion)
- install bash completion
```bash
sudo apt install bash-completion
```
- install `kubectl` completion and make an short alias
```bash
echo 'source <(kubectl completion bash)' >>~/.bashrc
echo 'alias k=kubectl' >>~/.bashrc
echo 'complete -o default -F __start_kubectl k' >>~/.bashrc
source ~/.bashrc
```
_______________________________________________________________________________________

# Kubernetes RollBack ('rollout undo')
source: [rollout history and rollout undo](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-back-a-deployment)
- check the history of different revisions. The biggest number is the lastest and the `1` is the oldest
```bash
 k rollout history  deployment nginx -n nginx
```
- You can have the a description recorded btu not using flag `--record` which is deprecated but using an `annotation`
eg.: patching a deployment and adding an annotation type `kubernetes.io/change-cause:<the cause of the change>`
```yaml
metadata:
  annotations:
    kubernetes.io/change-cause: "Shibuya is not accessible today, VIOLET ALERT!"
spec:
  template:
    spec:
      containers:
      - name: nginx
        image: nginx
        volumeMounts:
        - name: nginx-violet
          mountPath: /usr/share/nginx/html/
      volumes:
      - name: nginx-violet
        configMap:
          name: nginx-html
```
- rollback is done still using the keyword `rollout` but with `undo` and `--to-revision=<nbr of revision shown in history>`
```bash
- example output of some with the command used that use `--record` deprecated flag, some the `annotation` and some nothing:
k rollout history  deployment nginx -n nginx
Outputs:
deployment.apps/nginx 
REVISION  CHANGE-CAUSE
4         <none>   # example of nothing specified
5         <none>
6         <none>
7         <none>
8         <none>
10        kubectl patch deployment nginx --namespace=nginx --type=merge --   # example of `--record` flag used (deprecated)
11        kubectl patch deployment nginx --namespace=nginx --type=merge --
12        Shibuya is not accessible today, VIOLET ALERT!    # example of the `annotations: kubernetes.io/change-cause: "<cause to put here>" 
13        Shibuya is not accessible today, VIOLET ALERT
14        kubectl patch deployment nginx --namespace=nginx --type=merge --patch-file=nginx-deployment-patching.yaml --record=true
15        kubectl patch deployment nginx --namespace=nginx --type=merge --patch-file=nginx-deployment-patching.yaml --record=true
```
```bash
# now we perform the rolback
k rollout undo deployment/nginx -n nginx --to-revision=9
Outputs:
deployment.apps/nginx rolled back
```

Now at every use of the command `k rollout undo` or `k rollout restart` we will have a new `revision` numbered line in the history. 

rollout is just for: `kind:` `Deployments` or`ReplicaSets` or `StatefulSets`

**NOTE:**
When a `node` is down you lose all and `kubernetes` only can recreate pods in other nodes if they use `replicasets` under the hoow. So only `kind:` `kind:` `Deployments` or`ReplicaSets` or `StatefulSets` are getting the change to have their pods redeployed in healthy nodes. The standalone pods, are not recreated and lost, if using `hostPath` or `local` `persistent volumes` it is lost as it leaves in the `node` so you better use `ebs`, `nfs` or remote persistant volumes so that new pod recreated in other nodes will be able to reach those.
`DaemonSet`, `CongiMap`, `Secret`, `ServiceAccount`, `Service`: all of those will survive! so for standalone `pods` better change `kind` from `kind: Pod` to `kind: Deployment` so that they will be rescheduled in another healthy `node`.
