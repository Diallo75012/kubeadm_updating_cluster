 Create Certificates For The Cluster
Go to source Kubernetes Documentation Which Has All Details: [Cert Creation Doc](https://kubernetes.io/docs/tasks/administer-cluster/certificates/)

## Kubeadm when initialized creates certificates automatically
- The certificates generated by kubeadm include:​
    - CA Certificates: These are the root certificates used to sign other certificates within the cluster.​
    - API Server Certificates: Used by the Kubernetes API server to establish secure communications.​
    - Kubelet Client Certificates: Used by the kubelet to authenticate to the API server.​
    - Etcd Certificates: Used for secure communication with the etcd key-value store.​

# KUBEADM ISSUE
- Kubelet not starting
- Certs are all expired for every components: `controller`, `api-server`, `scheduler`, `etcd`

1. Can renew certs using a simple command:
```bash
sudo kubeadm certs check-expiration
kubeadm certs renew all
# OR for specific ones
sudo kubeadm certs renew admin.conf
```

Which will renew the `admin.conf` embedded certs.

2. Embedded certs are base64 encoded
```bash
base64 -w 0 /etc/kubernetes/pki/ca.crt > ca.crt.base64
base64 -w 0 /etc/kubernetes/pki/admin.crt > admin.crt.base64
base64 -w 0 /etc/kubernetes/pki/admin.key > admin.key.base64
base64 -w 0 /etc/kubernetes/pki/apiserver-kubelet-client.crt > apiserver-kubelet-client.crt.base64
base64 -w 0 /etc/kubernetes/pki/apiserver-kubelet-client.key > apiserver-kubelet-client.key.base64
base64 -w 0 /etc/kubernetes/pki/front-proxy-client.crt > front-proxy-client.crt.base64
base64 -w 0 /etc/kubernetes/pki/front-proxy-client.key > front-proxy-client.key.base64
base64 -w 0 /etc/kubernetes/pki/apiserver-etcd-client.crt > apiserver-etcd-client.crt.base64
base64 -w 0 /etc/kubernetes/pki/apiserver-etcd-client.key > apiserver-etcd-client.key.base64
```

- can also manually convert those keys and past those in the configuration files:
eg:.
```bash
cat /etc/kubernetes/pki/apiserver-etcd-client.crt | base64
```

3. Cert can't be found for `admin.conf` in `../pki/..` folder:
Extract Keys from the `admin.conf` file for example to get the cert
- can also install `yq` which is a tool helping from the terminal to fetch `YAML` fields data, eg.:
```bash
sudo add-apt-repository ppa:rmescandon/yq
sudo apt update
sudo apt install yq -y
```
- then use it like that:
```bash
yq e '.users[0].user.client-certificate-data' /etc/kubernetes/admin.conf | base64 -d | sudo tee /etc/kubernetes/pki/admin.crt
```

## **Important**
- **Make Backups of certs and inital `YAML` files**

4. Documentation to understand certificates with some tables:
[Best Practices Kubernetes certificates](https://kubernetes.io/docs/setup/best-practices/certificates/)



# Controller Components Having a `.conf` File (present in `/etc/kubernetes/) Because Need To Authenticate To `Api-Server`

Note: Certs are base64 encoded and embedded directly within the admin.conf file. If needed to be extracted, decode necessary to get original cert back.

1. admin.conf
- Purpose: Provides administrative access to the Kubernetes API server, typically used by cluster administrators.​
- Default Location: /etc/kubernetes/admin.conf​
- Certificate Authority: /etc/kubernetes/pki/ca.crt​
- Client Certificate: Embedded within the admin.conf file under users.user.client-certificate-data​
- Client Key: Embedded within the admin.conf file under users.user.client-key-data​

2. controller-manager.conf
- Purpose: Used by the Kubernetes Controller Manager to authenticate with the API server.​
- Default Location: /etc/kubernetes/controller-manager.conf​
- Certificate Authority: /etc/kubernetes/pki/ca.crt​
- Client Certificate: Embedded within the controller-manager.conf file under users.user.client-certificate-data​
- Client Key: Embedded within the controller-manager.conf file under users.user.client-key-data​

3. scheduler.conf
- Purpose: Utilized by the Kubernetes Scheduler to authenticate with the API server.​
- Default Location: /etc/kubernetes/scheduler.conf​
- Certificate Authority: /etc/kubernetes/pki/ca.crt​
- Client Certificate: Embedded within the scheduler.conf file under users.user.client-certificate-data​
- Client Key: Embedded within the scheduler.conf file under users.user.client-key-data​
 
4. kubelet.conf
- Purpose: Allows the Kubelet agent, which runs on each node, to authenticate with the API server.​
devopscube.com
- Default Location: /etc/kubernetes/kubelet.conf​
- Certificate Authority: /etc/kubernetes/pki/ca.crt​
- Client Certificate: Typically located at /var/lib/kubelet/pki/kubelet-client-current.pem​
- Client Key: Typically located at /var/lib/kubelet/pki/kubelet-client-current.pem​



# Controller Components Without `.conf` file (present in `/etc/kubernetes/manifest/`) Because Need To Always Be Alive (Critical Components)

5. API Server (kube-apiserver):
- Purpose: The API server acts as the central management entity, validating and configuring data for API objects such as pods, services, and replication controllers.​
- Certificate Authority File: /etc/kubernetes/pki/ca.crt​
- Server Certificate and Key Files: /etc/kubernetes/pki/apiserver.crt and /etc/kubernetes/pki/apiserver.key​
- Client Certificates for etcd and kubelet:
  - etcd Client Certificate and Key Files: /etc/kubernetes/pki/apiserver-etcd-client.crt and /etc/kubernetes/pki/apiserver-etcd-client.key
    Description: Used by the API server to authenticate itself to the etcd server.
- kubelet Client Certificate and Key Files: /etc/kubernetes/pki/apiserver-kubelet-client.crt and /etc/kubernetes/pki/apiserver-kubelet-client.key
  Description: Used by the API server to authenticate itself to kubelets.

6. etcd:
- Purpose: etcd is a consistent and highly-available key-value store used as Kubernetes' backing store for all cluster data.​
- Certificate Authority Files: /etc/kubernetes/pki/etcd/ca.crt and /etc/kubernetes/pki/etcd/ca.key​
  Description: The root CA certificate and key for etcd, used to sign etcd server and peer certificates.​
- Server Certificate and Key Files: /etc/kubernetes/pki/etcd/server.crt and /etc/kubernetes/pki/etcd/server.key​
  Description: Used by etcd to serve secure (HTTPS) endpoints.​
- Peer Certificates Files: /etc/kubernetes/pki/etcd/peer.crt and /etc/kubernetes/pki/etcd/peer.key​
  Description: Used for secure communication between etcd peers in a cluster.​
- Client Certificates Files: /etc/kubernetes/pki/etcd/healthcheck-client.crt and /etc/kubernetes/pki/etcd/healthcheck-client.key​
  Description: Used by clients performing health checks on etcd.

# Folder Separation Of Concerns For Cluster-Wide Components and Node Components
## Folder For Cluster-Wide Components
- /etc/kubernetes/pki/ contains cluster-wide control plane certificates (API server, etcd, CA).
## Folder For Node Components
- /var/lib/kubelet/pki/ stores node-specific kubelet certificates.
Here for example the `Kubelet` client certificates will be stored as `Kubelet` on each node need a different key to authenticate to the `Api-Server`
**Here Unique `.pem` File As It Holds `crt` and `key`**: simplifies automatic renewal and storage (one file instead of two)

# `Admin.conf` and `Ca.crt`

## `Admin.conf`
- `Admin.conf`: is the same file that you are going to find in /home/<admin_user>/.kube/config
```bash
mkdir -p $HOME/.kube
sudo cp /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

## `Ca.crt` = **Root CA**
- `Ca.crt`: is the **authority** certificate for all the cluster controller components.
- Sign in all clients and server certificates.

`Admin.conf` is which is copied and changed permission to the `admin_user` home directory in `~/.kube/config`
is used by `kubectl` commands to authenticate to the cluster where `Api Server` will use the `Ca.crt` **Authority** to verify signature and permissions.

# Diagram of configs dependences on certs
(diagram)[https://excalidraw.com/#json=elokPxtHr6KkfK5rcWw0j,sOYYyGGT7QpDV1iBqvlfEg]


# MISSING KEYS, CERTS ETC....
## `sa.pub` file (/etc/kubernetes/pki/sa.pub) Missing
It is while checking into details the `yaml` files present in `/etc/kubernetes/manifest/` folder that I realized that
the `kube-apiserver.yaml` file is using also `sa.pub` that verifies signature service account tokens.
To recreate it I had to run this command:
```bash
openssl rsa -in /etc/kubernetes/pki/sa.key -pubout -out /etc/kubernetes/pki/sa.pub
```

## `controller-manager.crt and .key` (/etc/kubernetes/pki/..) Missing
- Need to recreate the keys and then to encode base64 and to update the `controller-manager.conf` file with those key/crt in the `user.client` section of the file

1. Generate a New Private Key
```bash
sudo openssl genrsa -out /etc/kubernetes/pki/controller-manager.key 2048
```

2. Create a Certificate Signing Request (CSR)
```bash
sudo openssl req -new -key /etc/kubernetes/pki/controller-manager.key \
-out /etc/kubernetes/pki/controller-manager.csr \
-subj "/CN=system:kube-controller-manager/O=system:kube-controller-manager"
```

3.  Sign the Certificate Using the Kubernetes CA
```bash
sudo openssl x509 -req -in /etc/kubernetes/pki/controller-manager.csr \
  -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key \
  -CAcreateserial -out /etc/kubernetes/pki/controller-manager.crt \
  -days 365 -sha256
Outputs:
Certificate request self-signature ok
subject=CN = system:kube-controller-manager, O = system:kube-controller-manager
```

4. Verify the Certificate
```bash
sudo openssl x509 -in /etc/kubernetes/pki/controller-manager.crt -noout -text | grep -E 'Subject:|Issuer:'
        Issuer: CN = kubernetes
        Subject: CN = system:kube-controller-manager, O = system:kube-controller-manager
Outputs:
Issuer: CN = kubernetes
        Subject: CN = system:kube-controller-manager, O = system:kube-controller-manager
```

## `admin.crt and .key` (/etc/kubernetes/pki/..) Missing
- need to recreate those following same steps as above but just changing names so:
  - Generate key
  - Create CSR
  - Sign using Kubernetes `ca.crt` and `ca.key`
  - Verify Certificate (just checking...)
```bash
sudo openssl genrsa -out /etc/kubernetes/pki/admin.key 2048
sudo openssl req -new -key /etc/kubernetes/pki/admin.key \
  -out /etc/kubernetes/pki/admin.csr \
  -subj "/CN=kubernetes-admin/O=system:masters"
sudo openssl req -new -key /etc/kubernetes/pki/admin.key \
  -out /etc/kubernetes/pki/admin.csr \
  -subj "/CN=kubernetes-admin/O=system:masters"
sudo openssl x509 -req -in /etc/kubernetes/pki/admin.csr \
  -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key \
  -CAcreateserial -out /etc/kubernetes/pki/admin.crt \
  -days 365 -sha256
Outputs:
Certificate request self-signature ok
subject=CN = kubernetes-admin, O = system:masters
sudo openssl x509 -in /etc/kubernetes/pki/admin.crt -noout -text | grep -E 'Subject:|Issuer:'
Outputs:
Issuer: CN = kubernetes
Subject: CN = kubernetes-admin, O = system:masters

```


# How to recreate `.pem` file that `kubelet` need and which is located at `/var/lib/kubelet/pki/` ?
Normally Kubernetes create it auromatically but you can use a simple command to recreate it:
- just use the `kubeadm` command:
```bash
sudo kubeadm certs renew all
sudo systemctl restart kubelet
```

OR manually (but i don't see why we do it manually when it should present in each node and it is a different .pem content for each node)
```bash
sudo openssl genrsa -out /var/lib/kubelet/pki/kubelet-client.key 2048
sudo openssl req -new -key /var/lib/kubelet/pki/kubelet-client.key \
  -out /var/lib/kubelet/pki/kubelet-client.csr \
  -subj "/CN=system:node:controller.creditizens.net/O=system:nodes"
openssl x509 -req -in /var/lib/kubelet/pki/kubelet-client.csr \
  -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key \
  -CAcreateserial -out /var/lib/kubelet/pki/kubelet-client-current.pem \
  -days 365 -sha256
sudo systemctl restart kubelet
```

**Important**:
- The `Kubelet` `.pem` file located at `/var/kubernetes/pki/` is unique in each nodes, therefore, can be manually created only on the `controller` node.
- If a worker node loses or have issues with that file, it needs to request for a new one as it got the file from when it had joined the cluster:
```bash
sudo kubeadm join --token <your-token> --discovery-token-ca-cert-hash sha256:<hash>
```
- If a worker node need a new `.pem` file it need to approve pending `csr`:
```bash
kubectl get csr
kubectl certificate approve <csr-name>
# if too many <csr-name> in `pending` mode and too many to do it manually just run this command
# which will" get all csr | filter pending one | run the cert approval for each of those
kubectl get csr | awk '/Pending/ {print $1}' | xargs kubectl certificate approve
```

# `/var/lib/kubelet/pki/` `.pem` file symlink is pointing to outdated `.pem` file
The `...current.pem` file is pointing to an outdated one, so we need to make a backup of the outdated one and use the sigle command to renew certs, see issue when `ls -la` folder:
```bash
sudo ls -la /var/lib/kubelet/pki
total 28
drwxr-xr-x 2 root root 4096 mars  12 20:52 .
drwx------ 8 root root 4096 juil.  7  2023 ..
-rw------- 1 root root 1062 mars  12 20:53 kubelet-client-2023-07-07-09-39-43.pem
-rw-r--r-- 1 root root  960 mars  12 20:53 kubelet-client.csr
lrwxrwxrwx 1 root root   59 juil.  7  2023 kubelet-client-current.pem -> /var/lib/kubelet/pki/kubelet-client-2023-07-07-09-39-43.pem
-rw------- 1 root root 1704 mars  12 20:52 kubelet-client.key
-rw-r--r-- 1 root root 2371 juil.  7  2023 kubelet.crt
-rw------- 1 root root 1675 juil.  7  2023 kubelet.key
```
**Fix:**
- we backup the outdated .epm file or just delete it
```bash
sudo mv kubelet-client-2023-07-07-09-39-43.pem BAK_kubelet-client-2023-07-07-09-39-43.pem
# OR just : sudo rm -rf kubelet-client-2023-07-07-09-39-43.pem
```
- if new file present: we can just create new symlink
```bash
sudo ln -sf /var/lib/kubelet/pki/kubelet-client-<new-date>.pem /var/lib/kubelet/pki/kubelet-client-current.pem
```
- if not what is my case (the best ever), we create a new one from scratch:
```bash
sudo kubeadm certs renew kubelet
sudo ln -sf /var/lib/kubelet/pki/kubelet-client.pem /var/lib/kubelet/pki/kubelet-client-current.pem
sudo systemctl restart kubelet
```
- i have finally deleted everything from thi `/var/lib/kubelet/pki/` folder to recreate all certs so that everything is renewed and good for learning
```bash
sudo rm -rf /var/lib/kubelet/pki/*
```
so we recreate all like that:
```bash
# recreate key
sudo openssl genrsa -out /var/lib/kubelet/pki/kubelet-client.key 2048
# check
sudo ls -lah /var/lib/kubelet/pki/
Outputs:
total 20K
drwxr-xr-x 2 root root 4,0K mars  12 21:23 .
drwx------ 8 root root 4,0K juil.  7  2023 ..
-rw------- 1 root root 1,7K mars  12 21:30 kubelet-client.key
-rw-r--r-- 1 root root 2,4K juil.  7  2023 kubelet.crt
-rw------- 1 root root 1,7K juil.  7  2023 kubelet.key
# create csr
sudo openssl req -new -key /var/lib/kubelet/pki/kubelet-client.key \
    -out /var/lib/kubelet/pki/kubelet-client.csr \
    -subj "/CN=system:node:controller.creditizens.net/O=system:nodes"
# verify the csr
sudo openssl req -in /var/lib/kubelet/pki/kubelet-client.csr -noout -text
Outputs:
Certificate Request:
    Data:
        Version: 1 (0x0)
        Subject: CN = system:node:controller.creditizens.net, O = system:nodes
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                Public-Key: (2048 bit)
                Modulus:
                    00:e6:d6:13:89:65:c9:61:c4:bc:7f:bd:3d:0e:99:
                    bb:50:9f:6f:74:48:31:20:ef:12:de:92:a6:b1:b6:
                    31:56:6f:fb:99:15:2f:b8:aa:4b:a1:d9:6d:ec:a3:
                    95:7f:45:11:6e:0b:8e:7f:2b:b0:3d:80:3d:7d:f7:
                    ce:04:61:c5:f7:79:06:d8:40:ea:7a:d2:b4:e2:c9:
                    cb:5d:84:2e:98:f5:f0:e2:9d:d8:89:87:3f:77:74:
                    ec:03:ec:25:1b:da:82:eb:d0:2a:57:42:77:d7:b0:
                    a1:88:2b:e5:43:b4:25:01:44:ec:4b:05:88:34:10:
                    b6:f6:58:9f:3e:f8:e5:73:e3:1b:6c:7b:04:14:a8:
                    27:14:36:74:f3:63:67:56:d9:d1:c6:05:19:98:18:
                    0b:fd:ea:b6:69:12:4c:99:79:aa:58:b1:5b:b5:3b:
                    1f:10:c8:47:7a:8e:6a:79:49:8b:52:8f:b8:b4:ef:
                    5f:1c:02:2f:97:ce:35:c1:3b:db:09:f6:9a:61:ef:
                    e0:88:d8:04:d6:cb:76:33:77:95:aa:19:3b:ff:80:
                    56:ed:c0:01:a6:f9:07:2b:78:4f:fa:89:7f:ab:10:
                    41:4e:d6:67:c8:65:b4:2c:a2:31:c4:67:a7:3f:56:
                    81:90:5f:e2:d4:e4:84:18:44:63:43:70:23:02:b5:
                    58:9b
                Exponent: 65537 (0x10001)
        Attributes:
            (none)
            Requested Extensions:
    Signature Algorithm: sha256WithRSAEncryption
    Signature Value:
        78:b3:b9:9d:10:a3:da:c5:c4:b5:87:e1:5f:fd:83:d7:21:27:
        f0:d7:fd:48:c7:f8:b7:e0:70:b7:de:66:00:71:2d:72:bb:e3:
        6f:25:0f:b7:f8:04:47:94:5f:5b:7a:4a:00:12:ea:b8:b7:54:
        5a:87:76:5a:07:79:68:8b:8e:d5:f8:2c:50:f9:cf:c9:97:6f:
        71:20:12:48:4a:c9:66:0d:a0:ec:41:fe:67:46:5b:9d:63:a7:
        6b:85:c1:80:92:a6:82:43:f3:0f:67:c2:08:97:05:5b:7d:f2:
        95:d1:93:1b:3f:d8:60:9f:da:3f:50:32:b3:46:3a:dd:31:58:
        c0:26:7f:f9:1f:77:ea:f5:f7:94:cb:bb:b2:4a:d8:17:02:a1:
        13:b1:0a:8c:84:5c:dd:af:df:d6:f5:e5:81:91:4f:f7:00:d6:
        58:21:33:8e:70:ce:4b:cc:2a:5a:83:ed:ea:cb:30:b3:8f:31:
        85:f1:f8:b6:8c:fb:66:0a:9b:b2:18:29:b6:7d:57:9d:52:b4:
        9e:66:f2:eb:37:8b:91:49:15:17:60:df:84:bd:5e:dd:1e:1b:
        38:4a:ec:ec:eb:5c:cf:a2:c3:b5:0c:8b:7a:a7:03:68:7e:6d:
        7e:d9:c4:ab:26:e8:a7:40:ea:67:0d:5d:7f:30:15:8e:a9:91:
        e5:e3:7f:6b
# sign the csr with Kubernetes CA
sudo openssl x509 -req -in /var/lib/kubelet/pki/kubelet-client.csr \
    -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key \
    -CAcreateserial -out /var/lib/kubelet/pki/kubelet-client-current.pem \
    -days 365
Outputs:
Certificate request self-signature ok
subject=CN = system:node:controller.creditizens.net, O = system:nodes
# change permissions
sudo chown root:root /var/lib/kubelet/pki/kubelet-client-current.pem
sudo chmod 600 /var/lib/kubelet/pki/kubelet-client-current.pem
# check cert validity
sudo openssl x509 -in /var/lib/kubelet/pki/kubelet-client-current.pem -noout -text | grep "validity"
# fix symlink
sudo ln -sfn /var/lib/kubelet/pki/kubelet-client-current.pem /var/lib/kubelet/pki/kubelet-client.pem
# check symlink
sudo ls -la /var/lib/kubelet/pki/
Outpus:
total 28
drwxr-xr-x 2 root root 4096 mars  12 21:35 .
drwx------ 8 root root 4096 juil.  7  2023 ..
-rw-r--r-- 1 root root  960 mars  12 21:31 kubelet-client.csr
-rw-r--r-- 1 root root 1062 mars  12 21:33 kubelet-client-current.pem
-rw------- 1 root root 1704 mars  12 21:30 kubelet-client.key
lrwxrwxrwx 1 root root   47 mars  12 21:35 kubelet-client.pem -> /var/lib/kubelet/pki/kubelet-client-current.pem
-rw-r--r-- 1 root root 2371 juil.  7  2023 kubelet.crt
-rw------- 1 root root 1675 juil.  7  2023 kubelet.key
# create missing `kubelet.crt` and `kubelet.key`
sudo openssl genrsa -out /var/lib/kubelet/pki/kubelet.key 2048
sudo openssl req -new -key /var/lib/kubelet/pki/kubelet.key -out /var/lib/kubelet/pki/kubelet.csr \
    -subj "/CN=kubelet"
sudo openssl x509 -req -in /var/lib/kubelet/pki/kubelet.csr \
    -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key \
    -CAcreateserial -out /var/lib/kubelet/pki/kubelet.crt -days 365
sudo chmod 600 /var/lib/kubelet/pki/kubelet.key
sudo chmod 644 /var/lib/kubelet/pki/kubelet.crt
# check
sudo openssl x509 -in /var/lib/kubelet/pki/kubelet.crt -noout -text
# restart and check kubelet service
sudo systemctl restart kubelet
sudo systemctl status kubelet
```

# SOLUTION FOR KUBELET .PEM CERT/KEY RENEWAL AND PRESENCE
- the first issue was that certificates were outdated, therefore, i have changed them all
- next issue was that the `.pem` file of kubelet wasn't holding `crt` and `key` but only the `key`: 
    - therefore, i have delete those files from the `/var/lib/kubelet/pki/` and recreated everything from scratch
      but changed the process by manually adding the two keys in the `.pem` files 
- lesson need a good understanding of all the files and permissions and where those are located, their config files. 
    That is why I have made the detailed diagram that shows where are the leys, certs and configs and used arrows to show where they pointing to.
- After when `kubelet` was back i had to wait a bit before getting the `kubectl` command back and the visibility on `nodes`, `pods` etc...
- terminal output review of what has been done for the last step to fix it:
```bash
# stop kubelet (not need it wouldn't start LOL)
sudo systemctl stop kubelet
# Remove Incorrect Files
sudo rm -rf /var/lib/kubelet/pki/kubelet-client*.pem
sudo rm -rf /var/lib/kubelet/pki/kubelet.crt /var/lib/kubelet/pki/kubelet.key
sudo rm -rf /var/lib/kubelet/pki/kubelet-client.csr
'''
- Only this left in the folder but can empty it fully and recreate all keys:
sudo ls -la /var/lib/kubelet/pki/
total 20
drwxr-xr-x 2 root root 4096 mars  12 23:24 .
drwx------ 8 root root 4096 juil.  7  2023 ..
-rw-r--r-- 1 root root  960 mars  12 23:24 kubelet-client.csr
-rw------- 1 root root 1704 mars  12 23:14 kubelet-client.key
-rw-r--r-- 1 root root  887 mars  12 21:37 kubelet.csr
'''

'''
If wanted to recreate all kubelet.key, kubelet.csr, kubelet.crt can run:
sudo openssl genrsa -out /var/lib/kubelet/pki/kubelet.key 2048
sudo openssl req -new -key /var/lib/kubelet/pki/kubelet.key -out /var/lib/kubelet/pki/kubelet.csr \
    -subj "/CN=kubelet"
sudo openssl x509 -req -in /var/lib/kubelet/pki/kubelet.csr \
    -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key \
    -CAcreateserial -out /var/lib/kubelet/pki/kubelet.crt -days 365
sudo chmod 600 /var/lib/kubelet/pki/kubelet.key
sudo chmod 644 /var/lib/kubelet/pki/kubelet.crt
'''

# Generate a New CSR (Certificate Signing Request)
sudo openssl genrsa -out /var/lib/kubelet/pki/kubelet-client.key 2048
sudo openssl req -new -key /var/lib/kubelet/pki/kubelet-client.key \
-out /var/lib/kubelet/pki/kubelet-client.csr \
-subj "/CN=system:node:controller.creditizens.net/O=system:nodes"
# Sign the CSR to Generate the Correct Certificate
sudo openssl x509 -req -in /var/lib/kubelet/pki/kubelet-client.csr \
-CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key \
-CAcreateserial -out /var/lib/kubelet/pki/kubelet-client.crt \
-days 365
# Create the Correct kubelet-client-current.pem
# This file must contain both the private key and the certificate.
# that is the super command that manually added `crt` and `key` in the `.pem` file
sudo cat /var/lib/kubelet/pki/kubelet-client.key /var/lib/kubelet/pki/kubelet-client.crt | sudo tee /var/lib/kubelet/pki/kubelet-client-current.pem
# Fix Permissions
sudo chown root:root /var/lib/kubelet/pki/kubelet-client-current.pem
sudo chmod 600 /var/lib/kubelet/pki/kubelet-client-current.pem
# restart kubelet
sudo systemctl restart kubelet
# check status (should be fine)
sudo systemctl status kubelet
# check journal eventually
sudo journalctl -u kubelet --no-pager --lines=50
# wait a bit and then start running kubectl commands
kubectl get pods
kubectl get nodes
...etc...
```

# For worker nodes how to fix the issue of certificate of kubelet expired
- make sure you have an ssh connection between worker node and controller node as we are going to more files to be signed there as CA authority `.key` file is only there
```bash
# on worker and controller
sudo apt install ssh
sudo systemctl status ssh
ssh-keygen
# then from controller to worker copy the key so that controller is part of the known hosts
ssh-copy-id -i ~/.ssh/id_rsa.pub creditizens@node1.creditizens.net
```
- on the worker node you can delete everything from the folder `/var/lib/kubelet/pki/` then restart `kubelet service`: `sudo systemctl restart kubelet`.
  Then this will automatically populate the folder `var/lib/kubelet/pki/` with new `kubelet.crt` and `kubelet.key` that you are going to use for next command:
```bash
sudo openssl req -new -key /var/lib/kubelet/pki/kubelet.key \
    -out /var/lib/kubelet/pki/kubelet-client.csr \
    -subj "/CN=system:node:node1-creditizens.net/O=system:nodes"
```

- Copy the CSR to the Controller Node
Since the worker node doesn’t have /etc/kubernetes/pki/ca.key, we must sign the CSR on the controller.
Run this command on the worker node to transfer the CSR file:
```bash
scp /var/lib/kubelet/pki/kubelet-client.csr creditizens@controller.creditizens.net>:/tmp/
```

- Sign the CSR on the Controller Node
Now, on the controller node, sign the CSR using the CA:
```bash
sudo openssl x509 -req -in /tmp/kubelet-client.csr \
    -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key \
    -CAcreateserial -out /tmp/kubelet-client.crt \
    -days 365
```
This signs the CSR, producing kubelet-client.crt, which must be combined with the key.

- Copy the Signed Certificate Back to the Worker Node
On the controller, transfer the signed certificate back to the worker:
```bash
scp /tmp/kubelet-client.crt creditizens@node1.creditizens.net:/tmp/
```

- Now, on the worker node, move it into place:
```bash
sudo mv /tmp/kubelet-client.crt /var/lib/kubelet/pki/kubelet-client.crt
```

- Create the Correct kubelet-client-current.pem
The kubelet-client-current.pem file must contain both the certificate and the private key.
Run this command on the worker node to combine them into a proper PEM file:
```bash
sudo cat /var/lib/kubelet/pki/kubelet-client.crt /var/lib/kubelet/pki/kubelet.key | sudo tee /var/lib/kubelet/pki/kubelet-client-current.pem
```

-  Set the Correct Permissions
```bash
sudo chown root:root /var/lib/kubelet/pki/kubelet-client-current.pem
sudo chmod 600 /var/lib/kubelet/pki/kubelet-client-current.pem
```

- Restart Kubelet
```bash
sudo systemctl restart kubelet
sudo systemctl status kubelet
```


# ISSUE WITH SCHEDULER CRASHBACKLOOP BECAUSE OF CERTIFICATE EXPIRED
- state of Scheduler in Cluster before fix
```bash
# kubectl get pods -n kube-system
kube-scheduler-controller.creditizens.net            0/1     CrashLoopBackOff   57 (2m27s ago)   615d
# logs
kubectl logs kube-scheduler-controller.creditizens.net -n kube-system
I0313 17:04:47.970993       1 serving.go:348] Generated self-signed cert in-memory
E0313 17:04:47.972942       1 run.go:74] "command failed" err="error loading config file \"/etc/kubernetes/scheduler.conf\": illegal base64 data at input byte 25"
```

- Check authority certificate expiry date
```bash
sudo openssl x509 -in /etc/kubernetes/pki/front-proxy-ca.crt -noout -dates
Outputs:
notBefore=Jul  7 07:39:41 2023 GMT
notAfter=Jul  4 07:39:41 2033 GMT  STILL VALID!
```

- Check client certiifcate expiry date
```bash
sudo openssl x509 -in /etc/kubernetes/pki/front-proxy-client.crt -noout -dates
Outputs:
notBefore=Jul  7 07:39:41 2023 GMT
notAfter=Mar 10 18:21:00 2026 GMT EXPIRED NEED CREATE A NEW ONE!
```

### HOW TO CREATE A NEW ONE
```bash
# Delete scheduler client keys
sudo rm -rf /etc/kubernetes/pki/front-proxy-client.crt /etc/kubernetes/pki/front-proxy-client.key
```
```bash
# generate a new key
sudo openssl genrsa -out /etc/kubernetes/pki/front-proxy-client.key 2048
```
```bash
# create certificate signing request
sudo openssl req -new -key /etc/kubernetes/pki/front-proxy-client.key -subj "/CN=front-proxy-client" -out /etc/kubernetes/pki/front-proxy-client.csr
```
```bash
# sign the key with the authority certificate
sudo openssl x509 -req -in /etc/kubernetes/pki/front-proxy-client.csr -CA /etc/kubernetes/pki/front-proxy-ca.crt -CAkey /etc/kubernetes/pki/front-proxy-ca.key -CAcreateserial -out /etc/kubernetes/pki/front-proxy-client.crt -days 365
```

# SCHEDULER IS RUNNING FINE NOW BUT LOGS SHOW TLS ISSUES

- Check Authority CA expiry date:
```bash
openssl x509 -in /etc/kubernetes/pki/ca.crt -noout -dates
Outputs:
notBefore=Jul  7 07:39:41 2023 GMT
notAfter=Jul  4 07:39:41 2033 GMT
```

- Copy the Authority CA cert to `/usr/local/share...` and update certificates:
```bash
sudo cp -f /etc/kubernetes/pki/ca.crt /usr/:local/share/ca-certificates/kubernetes.crt
# for cert to be trusted server wide this need to be done on each `controller` nodes
sudo update-ca-certificates
```

# TO DELETE ALL PENDING OR RUNNING POD WITH ONE COMMAND
- Pending
```bash
kubectl delete pods -n kube-system --field-selector=status.phase=Pending
```
- Running
```bash
kubectl delete pods -n kube-system --field-selector=status.phase=Running
```

____________________________________________________________________________

After all of those struggles finally found the right way to do it:

# Utilities command to check cert expiry and more
- check certificates expiry dates
```bash
(sudo) openssl x509 -in <crt_file_path> -noout -dates
```
- check journal of a specify service
```bash
journalctl -u <service_name> -n <number_of_line_(end of file)_logs> --no-pager
```

# On All Worker Nodes:
Those files will be recreated at the end when we re-join worker to the cluster

1. delete all keys and config files
```bash
# maybe need to do those one by one manually and make sure to do the symlinked file first before it linked file
# otherwise you will need to create a dummy one until you delete that file and then delete your dummy file
sudo rm -rf /var/lib/kubelet/pki/*
# delete ca.crt and kubelet config
sudo rm -rf /etc/kubernetes/pki/ca/crt
sudo rm -rf /etc/kubernetes/kubelet.conf
```

# On the Controller Node
We used `kubeadm` to create the cluster so let's use it's utilities commands to renew all certs and config files
We will also delete all files that are not needed but needs to be regenreated

1. Remove expired kubelet certificates/config
```bash
# do it manually for all files in the folder and delete symlinked file (the file with `->`) before the other
sudo rm -f /var/lib/kubelet/pki/*
sudo rm -f /etc/kubernetes/kubelet.conf
```

2. Recreate kubelet.conf for control-plane
```bash
sudo kubeadm init phase kubeconfig all
# if `kubelet.conf` is not recreated use the following command to target it alone (it will recreate it)
sudo kubeadm init phase kubeconfig kubelet
```

3. Ensure rotation enabled (should already be done)
```
# ensure rotateCertificates: true
sudo nano /var/lib/kubelet/config.yaml
```

4. Restart kubelet
```
sudo systemctl restart kubelet
```

5. Check kubelet and cluster status
```bash
sudo systemctl status kubelet
kubectl get nodes
```

Now `kubectl` command should work and controller node should appear fine
but the other `worker` nodes should be `NotReady`.

It is time now to regenerate a token to join the cluster again
which will recreate certs and config files in `worker` nodes.


6. create the token that will be applied on worker node to join the cluster
```bash
kubeadm token create --print-join-command
```

# On Each Worker Node
7. past the command to join the cluster
    after having made sure that you have deleted all `.crt`, `.key` and `.pem`
    from `/var/lib/kubelet/pki/` and `/etc/kubernetes/pki/`
    and `.conf` file from `/etc/kubernetes/` folders
```bash
sudo kubeadm token create --print-join-command
```

8. restart kubelet and check it (accessory can also restart containerd but I didn't need myselfu)
```bash
sudo systemctl restart kubelet
sudo systemctl status kubelet
```

# Check The Controller Node
9. check that `workers` are nw back to work with status `Ready`
```bash
kubectl get nodes -o wide
```

10. Test a deployment and scale it and expose it and check if you see the app in browser, then tear down
```bash
# create namesapce
kubectl create ns nginx
# create deplyment
kubectl create deployment test-nginx -ns nginx --image=nginx
# scale deplyment
kubectl scale deployment test-nginx  -ns nginx --replicas=3
# check deployment and pods
kubectl get deployments test-nginx -ns nginx
kubectl get pods -l app=test-nginx -ns nginx
# expose deplyment through service creation for it
kubectl expose deployment test-nginx -ns nginx --port=80 --type=NodePort
kubectl get svc test-nginx -ns nginx
# can curl or just url in the browser the port is the one on the `right side 80:<port_for_brower>`
curl http://localhost:31234
# tear all down
kubectl delete svc test-nginx -ns nginx
kubectl delete deployment test-nginx -ns nginx
kubectl delete ns nginx
```

________________________________________________________________________________________________________________
# cNext
- [ ] update the cluster versions until we reach 1.32 (we are at 1.27)
    so we will have to do same process several times and fix any compatibility issues along the way.
    need to check supported versions ranges for each kubeadm updated version


# Take Snapshot of ETCD (for Safety)

## 1. Get the etcd Pod Name
```bash
kubectl get pods -n kube-system | grep etcd
Outputs:
etcd-controller-01   1/1   Running   0   1d
```

## 2. Create a Snapshot of etcd
```bash
sudo ETCDCTL_API=3 etcdctl snapshot save /var/lib/etcd/etcd-snapshot-$(date +%F-%T).db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key
```
This saves a snapshot with a timestamped filename in /var/lib/etcd/.
You can change the path if needed.

## 3. Verify the Snapshot Integrity
```bash
sudo ETCDCTL_API=3 etcdctl snapshot status /var/lib/etcd/etcd-snapshot-YYYY-MM-DD-HH-MM-SS.db
```

## 4. Etra: Restoring etcd from Snapshot (if Needed)
```bash
sudo ETCDCTL_API=3 etcdctl snapshot restore /var/lib/etcd/etcd-snapshot-YYYY-MM-DD-HH-MM-SS.db \
  --data-dir /var/lib/etcd-new
```
**Then, update the etcd manifest in `/etc/kubernetes/manifests/etcd.yaml` to point to `/var/lib/etcd-new`, and restart the control plane.**

# Uprgade Cluster:
- **[source: Kubernetes Documentation](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/#:~:text=During%20upgrade%20kubeadm%20writes%20the,etc%2Fkubernetes%2Ftmp)**
###  **Rules:**
- kubeadm cluster updates cn be done onl one minor version at a time 1.27 -> 1.28 -> 1.29...
- kubelet and containerd versions cn e max 3 versions older see documentation to see their corresponding versions but it is recommended to have those not more than on version older
- Need to do controller nodes first one at a time
- Need to to do the worker nodes one at a time
- Need for sure to know the architecture of taints/toleration/resource limits to make sure for seamless workload move to other nodes.
- Need also to see upgrades matching of Calico (but used the latest available the free one)
- Need to Backup etcd even if kubeadm doesn it automaicall placing backups to `/etc/kubernetes/tmp`
- Need to make sure version are `unhold` then upgrade and `hold` thse back for verions to not upgrade and have full control on stable cluster state 


**Important:** For `1` to `3` you can do the upgrade and then cordon and drain or the otherway around 
    as new state application to the cluster is done only with the command of `4` of `4'`

## 1. Cordon Node to Separa 
```bash
kubectl cordon <controller-node-name OR worker-node-name>
```

## 2. Drain Node: Make Node Unschedulable To prepare For Maintenance
```bash
kubectl drain <node-to-drain> --ignore-daemonsets --delete-emptydir-data
```

## 3. Upgrade Kubeadm to next minor version 1.27 -> 1.28
- make sure you have the repo in `apt` if not install it:
```bash
sudo apt update && sudo apt install -y curl apt-transport-https
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /usr/share/keyrings/kubernetes-archive-keyring.gpg
echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
# check the versions
kubeadm version
kubectl version
kubelet --version
containerd --version
# then run next command with the right version:
`sudo apt install -y kubeadm=1.28.10-00`
```
# get rid of deprecated kubernetes repo list
sudo rm /etc/apt/sources.list.d/kubernetes.list

# disable swap
sudo apt update
sudo apt install -y curl apt-transport-https
# get the signing key
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /usr/share/keyrings/kubernetes-archive-keyring.gpg
# add kubernetes apt repo
echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
# update package list
sudo apt update

# on all nodes disable swap and add kernel settings
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

# set up some kernel configs
sudo tee /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF

# load necessary kernel modules
sudo modprobe overlay
sudo modprobe br_netfilter
sudo tee /etc/sysctl.d/kubernetes.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF
Outputs:
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1


# reload changes
sudo sysctl --system
# check versions available to do the update for kubeadm kubelet and kubectl
sudo apt-cache madison kubeadm
sudo apt-cache madison kubelet
sudo apt-cache madison kubectl
```bash
sudo apt-mark unhold kubeadm kubelet kubectl && \
sudo apt-get update && \
sudo apt-get install -y kubeadm=1.28.15-1.1 kubelet=1.28.15-1.1 kebectl=1.28.15-1.1 && \
sudo apt-mark hold kubeadm kubelet kubectl
```

## . Install Compatible Version of Containerd (Optional but better have it updated even if it is Ok for few years...)
```bash
sudo apt-get install -y containerd.io=<required-version>
sudo apt install containerd.io=1.7.25-1
sudo systemctl restart containerd
```

## 4. Verify Upgrade Plan and Ugrade (Only in the first Control Plane: other ones are going to pick it up)
This is for the first control plane node only
```bash
# if having only one control node need to uncordon it  and restart kubelet and containerd
# to have the node `Ready` otherwise you won't be able to upgrade
kubectl uncorn controller.creditizens.net
sudo systemctl restart kubelet containerd
sudo kubeadm upgrade plan
sudo kubeadm upgrade apply v1.28.x
```

## 4'. Upgrade Other Control Planes (Optional if more than one control plane)
This is after having ugraded the first control plane and the other ones don't use `apply` but use `upgrade node` instead
Also no need to `uprgade plan` in those controller nodes.
```bash
sudo kubeadm upgrade node
```



## . Restart Kubelet
```bash
sudo systemctl daemon-reload
sudo systemctl restart kubelet
```

## . Bring Node Back Online
```bash
kubectl uncordon <node-to-uncordon>
```

## . Optionally Check that critical add-ons (CoreDNS, kube-proxy) are running the updated versions
```bash
kubectl get daemonset kube-proxy -n kube-system -o=jsonpath='{.spec.template.spec.containers[0].image}'


_________________________________________________

For worker nodes
- on controller node
kubectl cordon <controller-node-name OR worker-node-name>
```## 2. Drain Node: Make Node Unschedulable To prepare For Maintenance
```bash
kubectl drain <node-to-drain> --ignore-daemonsets --delete-emptydir-data

- on worker node
sudo apt update && sudo apt install -y curl apt-transport-https
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /usr/share/keyrings/kubernetes-archive-keyring.gpg
echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update -y


# on all nodes disable swap and add kernel settings
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

# set up some kernel configs
sudo tee /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF

# load necessary kernel modules
sudo modprobe overlay
sudo modprobe br_netfilter
sudo tee /etc/sysctl.d/kubernetes.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF
Outputs:
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1

sudo sysctl --system

kubeadm version
kubectl version
kubelet --version
containerd --version

# just check if version is there otherwise error with signing key or repo in `/etc/source.list.d/kubernetes.list`
sudo apt-cache madison kubeadm
sudo apt-cache madison kubelet
sudo apt-cache madison kubectl

sudo apt-mark unhold kubeadm kubectl kubelet
sudo apt install kubeadm=1.28.15-1.1 kubectl=1.28.15-1.1 kubelet=1.28.15-1.1 -y
sudo apt update

containerd --version

# optional trick to get the terminal show you the true version to get but from `containerd.io=<version>`
sudo apt install containerd=1.7
Outputs:
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
Package containerd is a virtual package provided by:
  containerd.io 1.7.25-1
You should explicitly select one to install.

# othersize just install it direcly, this is done having checked the version matching at: [containerd.io doc](https://containerd.io/releases/#:~:text=Kubernetes%20Version%20containerd%20Version%20CRI,36%2Bv1)
sudo apt install containerd.io=1.7.25-1

containerd --version
kubelet --version
Kubernetes v1.28.15
kubectl version
kubeadm version

# perform the upgrade
kubeadm upgrade node

sudo systemctl daemon-reload
sudo systemctl restart kubelet
sudo systemctl restart kubelet containerd
sudo systemctl status kubelet containerd
```

## . Restart Kubelet
```bash
sudo systemctl daemon-reload
sudo systemctl restart kubelet


- on controller node
```bash
kubectl uncordon <node-to-uncordon>
```

___________________________________________________________________________________________

# Kubernetes Plugins 

External utilities can be installed and used to interact with Kubernetes.
The easiest way to manage those is by installing `Krew` which will be used in combinaison of `kubectl`.

### install `krew`
source: [Krew Install](https://krew.sigs.k8s.io/docs/user-guide/setup/install/)
1. Install using this command (do not omit the parenthesis):
```bash
(
  set -x; cd "$(mktemp -d)" &&
  OS="$(uname | tr '[:upper:]' '[:lower:]')" &&
  ARCH="$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\(arm\)\(64\)\?.*/\1\2/' -e 's/aarch64$/arm64/')" &&
  KREW="krew-${OS}_${ARCH}" &&
  curl -fsSLO "https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz" &&
  tar zxvf "${KREW}.tar.gz" &&
  ./"${KREW}" install krew
)
```
2. add to `~/.bashrc` the export to add it to `PATH`:
```bash
export PATH="${KREW_ROOT:-$HOME/.krew}/bin:$PATH"
source ~/.bashrc
```
3. we can now use `krew` with `kubectl` to install plugins
```bash
# here plugin that will display the yaml/josn in a `neat` clean way when for example extracting those from cluster
kubectl krew install neat
```
4. now use the plugin to get here nice `yaml` or `json` output of resources
```bash
kubectl get deployment nginx -n nginx -o yaml | kubectl neat > nginx-deployment.yaml
```
- outputs:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "3"
  labels:
    app: nginx
  name: nginx
  namespace: nginx
spec:
  progressDeadlineSeconds: 600
  replicas: 3
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: nginx
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
```
___________________________________________________________________________________________________________
# Kubernetes Patching Deployment
Here we will use a side `.yaml` file to patch our deployment:
- if the keys names are the same as in the initial deployment the field will be updated
- if the keys names are different those will be added to the deployment
- therefore, **make sure you choose the name of the keys carfully**

- here have added a `configMap` as a mounted volume to the `nginx` deployment
```bash
cat config-map.yaml 
```
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-html
  namespace: nginx
data:
  index.html: |
    <h1 style="color:red;">Creditizens Customized Page Red</h1>
```

- here we create the side file that target the fields that will be updated
```bash
cat nginx-deployment-patching.yaml 
```
```yaml
spec:
  template:
    spec:
      containers:
      - name: nginx
        image: nginx
        volumeMounts:
        - name: nginx-html-conf
          mountPath: /usr/share/nginx/html/
      volumes:
      - name: nginx-html-conf
        configMap:
          name: nginx-html
```

- now patch deployment and you will see that nginx pages are displaying the red message `Creditizens Page Red`
```bash
# apply patch
kubectl patch deployment nginx -n nginx --type merge --patch-file nginx-deployment-patching.yaml
# update deployment (will start new pod and make the rolling update of those with default 25% surge)
kubectl rollout restart deployment/nginx -n nginx
```

_______________________________________________________________________________________

# Kubernetes `kubectl` auto-completion
source: [aliases and completion `kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#enable-shell-autocompletion)
- install bash completion
```bash
sudo apt install bash-completion
```
- install `kubectl` completion and make an short alias
```bash
echo 'source <(kubectl completion bash)' >>~/.bashrc
echo 'alias k=kubectl' >>~/.bashrc
echo 'complete -o default -F __start_kubectl k' >>~/.bashrc
source ~/.bashrc
```
_______________________________________________________________________________________

# Kubernetes RollBack ('rollout undo')
source: [rollout history and rollout undo](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-back-a-deployment)
- check the history of different revisions. The biggest number is the lastest and the `1` is the oldest
```bash
 k rollout history  deployment nginx -n nginx
```
- You can have the a description recorded btu not using flag `--record` which is deprecated but using an `annotation`
eg.: patching a deployment and adding an annotation type `kubernetes.io/change-cause:<the cause of the change>`
```yaml
metadata:
  annotations:
    kubernetes.io/change-cause: "Shibuya is not accessible today, VIOLET ALERT!"
spec:
  template:
    spec:
      containers:
      - name: nginx
        image: nginx
        volumeMounts:
        - name: nginx-violet
          mountPath: /usr/share/nginx/html/
      volumes:
      - name: nginx-violet
        configMap:
          name: nginx-html
```
- rollback is done still using the keyword `rollout` but with `undo` and `--to-revision=<nbr of revision shown in history>`
```bash
- example output of some with the command used that use `--record` deprecated flag, some the `annotation` and some nothing:
k rollout history  deployment nginx -n nginx
Outputs:
deployment.apps/nginx 
REVISION  CHANGE-CAUSE
4         <none>   # example of nothing specified
5         <none>
6         <none>
7         <none>
8         <none>
10        kubectl patch deployment nginx --namespace=nginx --type=merge --   # example of `--record` flag used (deprecated)
11        kubectl patch deployment nginx --namespace=nginx --type=merge --
12        Shibuya is not accessible today, VIOLET ALERT!    # example of the `annotations: kubernetes.io/change-cause: "<cause to put here>" 
13        Shibuya is not accessible today, VIOLET ALERT
14        kubectl patch deployment nginx --namespace=nginx --type=merge --patch-file=nginx-deployment-patching.yaml --record=true
15        kubectl patch deployment nginx --namespace=nginx --type=merge --patch-file=nginx-deployment-patching.yaml --record=true
```
```bash
# now we perform the rolback
k rollout undo deployment/nginx -n nginx --to-revision=9
Outputs:
deployment.apps/nginx rolled back
```

Now at every use of the command `k rollout undo` or `k rollout restart` we will have a new `revision` numbered line in the history. 

rollout is just for: `kind:` `Deployments` or`ReplicaSets` or `StatefulSets`

**NOTE:**
When a `node` is down you lose all and `kubernetes` only can recreate pods in other nodes if they use `replicasets` under the how.
So only `kind:` `kind:` `Deployments` or`ReplicaSets` or `StatefulSets` are getting the change to have their pods redeployed in healthy nodes.
The standalone pods, are not recreated and lost, if using `hostPath` or
`local` `persistent volumes` it is lost as it leaves in the `node` so you better use `ebs`,
`nfs` or remote persistant volumes so that new pod recreated in other nodes will be able to reach those.
`DaemonSet`, `CongiMap`, `Secret`, `ServiceAccount`, `Service`: 
  all of those will survive! so for standalone `pods` better change `kind` from `kind: Pod` to `kind: Deployment`
  so that they will be rescheduled in another healthy `node`.


_________________________________________________________________________________________________________________

# Kubernetes Commands `CLI`

- Use `kubeconfig` file to authenticate to cluster
```bash
kubectl get pods --kubeconfig ~/.kube/config -A
```

- `--raw` flag to see sensitive data and raw bytes data
source: [doc about --raw](https://kubernetes.io/docs/reference/kubectl/generated/kubectl_config/kubectl_config_view/)
source: [doc about --raw](https://kubernetes.io/docs/reference/kubectl/generated/kubectl_get/)
```
--raw string
Raw URI to request from the server. Uses the transport specified by the kubeconfig file.
```

Eg.:
```bash
# this shows sensitive data of the kubeconfig certs/keys (base64 encoded so as those are displayed in the file)
kubectl config view --raw
# can use raw URL to have resources info
kubectl get --raw /api/v1/namespaces/default/pods
{"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"104637"},"items":[]}
```

- Authentication RBAC
```bash
kubectl auth whoami
ATTRIBUTE   VALUE
Username    kubernetes-admin
Groups      [system:masters system:authenticated]
```

- Create User And Provide Access Using Cert Creation Way
source: [User Permission Access: Cert Creation Way](https://kubernetes.io/docs/reference/access-authn-authz/authentication/)
source: [Authorisation](https://kubernetes.io/docs/reference/access-authn-authz/authorization/)
```bash
# create key
openssl genrsa -out key_creditizens.pem
```
```bash
# create certificate signing request with username and organizations (different groups)
openssl req -new -key key_creditizens.pem -out creditizens.csr -subj "/CN=creditizens/O=devops/O=sre/O=genaiops"
```
```bash
# extract the base64 encode key `csr` in a one liner for the next .yaml file that we are going to embedded into
cat creditizens.csr | base64 | tr -d '\n'
```
source: (but version 1.27 not available so will try with my hold notes) [doc for v1.28](https://v1-28.docs.kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/)
source where `yaml` file is found: [Managing TLS in a Cluster](https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/)
```yaml
# create `yaml` file `kind: CertificateSigningRequest`
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  # this will be username
  name: creditizens
spec:
  groups:
  - system:authenticated
  request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ2pUQ0NBWFVDQVFBd1NERVVNQklHQTFVRUF3d0xZM0psWkdsMGFYcGxibk14RHpBTkJnTlZCQW9NQm1SbApkbTl3Y3pFTU1Bb0dBMVVFQ2d3RGMzSmxNUkV3RHdZRFZRUUtEQWhuWlc1aGFXOXdjekNDQVNJd0RRWUpLb1pJCmh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBS09DQWVCR3h4WVQ5U1VHNGtWZVRNNlJmVlhEdlVMTTBuM3cKYlNKa1AyYW5XRHozSm5QQ003MFBLNmo3cWNpakxyL0czcVkrMzVvL0pYeXJVL0hJOVRxaXNxeTRvT1kxa1ZEawo2amNKVGhhd1hJNExtT0dDNFVkSENVVGhQYTVBZkZMMjZ1RmNNWDZ0ZGh2MWtTa0NSNXJUSmlPbFg5MWtsWm83ClE5S3lhdDVNQ1hNTm1iZXFkVTRVY1NWZ0NaOTlJNHdLSy9KQytVMHR4amNPUzdNd04wekJJL2VSM29XOFhZS3AKMEFXWUw1MVRhRHNpM1hLSW9WMTFrVXFqeERtNGI4Qy9pMFRJbDRDMVJIUmtCNmVJbnQwbFcxSjk0aDZ1UmEvNAphYUlnS1lxcEQ0WjBpaE9CRGl5dkNVcTVRS1FvTVVXWXlsZ2IwUzVhcHVEbkRiNUxkVTBDQXdFQUFhQUFNQTBHCkNTcUdTSWIzRFFFQkN3VUFBNElCQVFCTUs1ZGJUekZWOXBIM3g3Mk9SMm56TzhoQnQzSGdySGplZ1ZkVlZ4dEUKR3N6bHZXemw2S3gxb25zQlhmdTcvWWdIbUlMZG5EdmdTaCtzSUhKUkNpNDR3TmRhQmZtdHl1L0pSTFFUbmVEZQozb0JSY0tnZHZrclRQVWZieittMGRXUGVRemVWU3BUS1F6dytBK0ZGcURtbTYwano2ZWc4ZnNjVkpZSCtBTlhjCm1zcGZFeTBhdDNEbjJpelNZZmRaVlNrWTc2cjFUclpNNXdaWVA5WXo1U1Y5NHVMR2ZhYWlrblErMEw0Zm80OUoKM0JESFllRXMzZHZFbFJ3Zk9MSVpBaUYwOEowNmdYMmtHajFiTk1taHNDdGY4T2RTVzZwYktBUVhxMDVZeDJxNwp1ODR0UU9zZXlLNDFwNVpCUDdEYVBsU3ZocVFDMFZKbHVtNDE0TzZxR0JodwotLS0tLUVORCBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0K
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - digital signature
  - key encipherment
  - client auth
  # this is just full name
  username: "creditizens metaverse"
```
```bash
# apply yaml file
kubectl apply -f auth_creditizens_to_api_server_component_create.yaml 
```
source: [approve csr](https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/)
```bash
# get csr using `kubectl` command and approve user
kubectl get csr
# approve the name of the csr which the the username choosen int he `yaml` file
kubectl certificate approve creditizens
```
```bash
# check the yaml version of the kuebrnetes `csr`
kubectl get csr creditizens -o yaml
# then extract from it the certificate and save it in a file for the user to be able to use that `crt` (situated at `.status.certificate`)
kubectl get csr creditizens -o jsonpath='{.status.certificate}' | base64 -d > creditizens.crt
```
source: [imperative command](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/imperative-command/)
source: [imperative all kubetcl command doc](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands)
```bash
# create namespace (Optional as you can leave it to default namespace)
kubectl create ns aishibuya
# create role
kubectl create role aidevops --verb=create,get,update,delete,list --resource=pods -n aishibuya
```
```bash
# create a role binding
kubectl create rolebinding deployment-ai-apps-creditizens --user=creditizens --role=aidevops -n aishibuya
```
```bash
# now set credentials for the user using key(`.pem` or `.key` if used `-in <....>.key` when using the openssl command but choose `.pem` way) and crt
# so now those credential are added to kubeconfig
kubectl config set-credentials creditizens --client-key=key_creditizens.pem --client-certificate=creditizens.crt --embed-certs=true
```
```bash
# set the context for this user that will use previously added credentials in the kubeconfig file (~/.kue/config)
kubectl config set-context creditizens --user=creditizens --cluster=kubernetes
```
```bash
# switch to the new user context and it will be limited to the namespace `aishibuya` and only `pod` resource (delete, get, list, create..)
kubectl config use-context creditizens
```
```bash
# swith context
kubectl config use-context kubernetes-admin@kubernetes
# delete context created and it will disappear from the '~/.kube/config' file
kubectl config delete-context creditizens
# check user is not anymore in `~/.kube/config`
cat ~/.kube/config
```


_______________________________________________________________________________________________________________

# Labels & Selectors
source: (Kubernetes Documentation)[https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/]

## Labels & Selectors Using `ReplicaSets` as example

(label names standards depending on DNS type name possibility or not)[https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-label-names]

**Important:**
- make sure that the `Selectors` do not match the labels of some resources implementing `ReplicaSets` for example.
  otherwise you risk to have that `pod` being `acquired` by the resource implementing `Replicasets`

We will provide examples of `kind: Deployment` or `kind: ReplicaSet` that will have a certain number of `replicas` count. And we will see what happen when you create a pods before that with a `selector` that have same label as the `replicaSet` implemented resource.
Answer is that:
  - if the `pod` is created after the `ReplicaSet`, the `pod` will be kiled instantly as the `ReplicaSet` have already reach maximum of `replicas` counts deployed in the cluster.
  - if the `pod` is created before the `ReplicaSet`, when the `ReplicaSet` will be deployed to the cluster, it will acquire those pods having the `Selector` matching its `labels`, therefore it will only add pods on top of what it has in `replicas` count OR will aquire pods until it fulfills its scaling `replicas` count and would kill all other pods having `selector` matching it `labels`
**So Be Very Carefull Here!**

### `ReplicaSets` restart policy
Source: (restart policy doc)[https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy]
- `Always`: default, always restarts container on terminaison
- `OnFailure`: restarts container on error
- `Never`: Never restarts container, dead/terminaison/error/other just bye bye!

### `ReplicaSets` `Selector` of `pod` `Labels` to acquire in yaml
- the yaml file has more field but this is extraction of what we are focusing on this example
```yaml
spec:
  selector:
    matchLabels:
      tier: junko
  # `template` part need also to therefore have a `label` and MUST match the `selector` `matchlabels` to not be rejected by the API Server
  template:
    metadata:
      labels:
        tier: junko
```

### Using REST API to delete `ReplicaSet`
There is different ways to interact with the custer and here we are not passing by the API Server but using REST API
to perform action on the Cluster. It is passing through the `Front Proxy`
```bash
kubectl proxy --port=8080
curl -X DELETE  'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend' \
  # option `propagationPolicy have to be `Foreground` or `Background`, if using instead `orphan` it will delete only the replicaset and not affect pods.
  # which will be still there but not manage anymore by `ReplicaSet` so not recreated if they die(error) or terminate
  -d '{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Foreground"}' \
  -H "Content-Type: application/json"
```
- Here `propagationPolicy` can be `orphan`, but why use this behavior of getting rid of the `replicaset` but keeping the `pods` in the Cluster as standalone `pods`?
  - for example, if we want to debug or test a new version in a specific pod and don't want to replicaset to restart the pods,
    we can't just scale down it to `0`. We can use this strategy to get rid of the `replicaset` temporaly and then recreate a new one that would `acquire` those pods back based on `labels` for example.
  - imperative command:
    - `kubectl delete replicaset <rs-name> --cascade=orphan`
    - `kubectl delete replicaset my-rs --grace-period=0 --cascade=orphan --force`

### 'acquire` rules
`Pods` -> `ReplicaSet` -> `Deployment`
- `Deployment` does manage the `ReplicaSet` and not the `Pod` so tell it: `Yo! Make sure that the pods are running with this container and replicas`
- `ReplicaSet` does manage the `Pod` and his boss the `Deployment` manages him OR `ReplicaSet` can be in cluster just to manage `Pod` without being part of `Deployment`
- `Pod` is managed by `ReplicaSet` or just running as a standalone `Pod`
- Therefore:
  - `Deployment` will adopt `ReplicaSet` if:
    - `ReplicaSet` has **no `ownerReferences`**
    - `ReplicaSet`’s `spec.selector` **matches** **`Deployment`'s selector**
    - `ReplicaSet`’s template **matches** **`Deployment`'s `Pod` template**
  - `ReplicaSet` can adopt a `Pod` if:
     - `Pod` **matches** the `ReplicaSet`’s **label selector**
     - `Pod` has **no `ownerReference`** (i.e., it’s "orphaned"): `kubectl get pod <pod_name> -o jsonpath='{.metadata.ownerReferences}'`
     - `Pod` `template spec` is an **exact match** with the **`ReplicaSet`'s template** (containers, volumes, etc.)


### When Scaling Down `ReplicaSets` How Pods Deletion Are Prioritized:
- 1: first will be deleted any pods in `pending` state or `unschedulable`
- 2: then,  will come any pods with the `annotation`: `controller.kubernetes.io/pod-deletion-cost`. The lower number one is delete first and so on.
- 3: then, `pods` on `nodes` with more `replicas` are delete first compared to pod on nodes having less replicas.
- 4: then comes, `pods` created more recently comes first if their creation time differs
- 5: randomly delete `pods` if all above matches

Note about the `annotation: controller.kubernetes.io/pod-deletion-cost`: between `-2147483648 and 2147483647` and it is `best-effort` so doesn't guarantees any deletion in the order (info about pod deleition cost annotation)[https://kubernetes.io/docs/reference/labels-annotations-taints/#pod-deletion-cost]

## `Labels` and `NodeSelectors` Using `Pods` as Example

### `pod` yaml example
taken from kubernetes documentation, here the pod will be selecting node with label `accelerator:nvidia-tesla-p100`
```yaml
spec:
  containers:
    - name: cuda-test
      image: "registry.k8s.io/cuda-vector-add:v0.1"
      resources:
        limits:
          nvidia.com/gpu: 1
  nodeSelector:
    accelerator: nvidia-tesla-p100
```

### `Set-Based` Requirement meaning filtering on the label keys and using keywords for values
- keywords for values:
  - `!`: means NOT and placed in front of key to excluse those keys
- keywords for values:
  - `in`: values are `EQUAL` to in the set of values supplied
  - `notin`: values are `NOT EQUAL` to in the set of values supplied
  - `exists`: a label that does exist (only check key not value)
  - `DoesNotExist`: a label that does not exist (only check key not value)
  - `,`: comma means `AND`

eg. of meanings :
```yaml
:
# key=`citynode`, values accepted=`tokyo` AND (because of comma: `,`) hokkaido
citynode in (tokyo, hokkaido)
# key=`nevereurope`, values=france AND emgland
nevereurope notin (france, england)
# key exclusive without value indicated. key=`shibuya`
shibuya
# key=NOT EQUAL to (because exclamation mark: `!`) shinjuku
!shinjuku
### more complexe requirements
# (key=`mangakissa`, with values=`ueno` AND `omotesando`), AND (key=`appdeploymentgroup`, values: NOT EQUAL to `production`)
mangakissa in (ueno, omotesando), appdeploymentgroup!=production
# checks if key `shomikitazawa` exist (not value)
exists: shimokitazawa
```

`Set-Based` requirements uses keywords while the other `Equality-Based` requirements are using `=`, `==` `!=`
We can use any of those two in our imperative commands:
```bash
kubectl get pods -l appdeploymentgroup!=production,magakissa=shibuya
kubectl get pods -l 'appdeploymentgroup in (staging),mangakissa in (shibuya)'
```

- in this `yaml` example from the `Kubernetes` documentation, satisfaction in when `ALL` `matchExpressions` **MUST** be satisfied
```yaml
selector:
  matchLabels:
    component: redis
  matchExpressions:
    - { key: tier, operator: In, values: [cache] }
    - { key: environment, operator: NotIn, values: [dev] }
```

## What is `ANDed` (`AND`) and What is `ORed` (`OR`)

### `matchExpressions`
- Here all conditions must be true (AND logic)
```yaml
matchExpressions:
  - key: env
    operator: In
    values: [prod, dev]
  - key: app
    operator: Exists
```
This matches only Pods that: have env=prod or env=dev `AND` have the label app (any value)

### `nodeSelector`
- Here `nodeSelector`: Only `AND`, no `OR`
```yaml
spec:
  nodeSelector:
    env: prod
    region: us-west
```
strictly: env == prod `AND` region == us-west, **No** support for `OR` here.

### `nodeAffinity` (same for: `podAffinity`)
- `AND`: `requiredDuringSchedulingIgnoredDuringExecution` (hard constraints) only supports `AND` across matchExpressions.
  So here inside one `matchExpressions` if there more than one option it is `AND` so all of those have to be true
```yaml
requiredDuringSchedulingIgnoredDuringExecution:
  nodeSelectorTerms:
    - matchExpressions:
        - key: env
          operator: In
          values: [prod]
        - key: region
          operator: In
          values: [us-west]
```
That means: (env == prod) `AND` (region == us-west)

- `OR`: `requiredDuringSchedulingIgnoredDuringExecution` also support `OR` but this time across several `match#xpressions` each `matchExpressions` being sets of `ORs`

You can provide multiple `nodeSelectorTerms` `matchExpressions`, and those terms are `ORed`.
Here match nodes that are either: env=prod `OR` region=us-west
```yaml
requiredDuringSchedulingIgnoredDuringExecution:
  nodeSelectorTerms:
    - matchExpressions:
        - key: env
          operator: In
          values: [prod]
    - matchExpressions:
        - key: region
          operator: In
          values: [us-west]
```
(env == prod) `OR` (region == us-west): inside each individual `matchExpressions` terms are `ANDed`, the `nodeSelectorTerms` several `matchExpressions` between themselves are `ORed`

## Label Usefulness To Have View in Cluster With Different Parts Grouped in Custom Columns

- We get the cluster pods and show the labels to see what is present and be able to create columns like pivoting the output
```bash
k get pod --all-namespaces --show-labels -o wide
NAMESPACE     NAME                                                 READY   STATUS    RESTARTS        AGE     IP                NODE                         NOMINATED NODE   READINESS GATES   LABELS
kube-system   calico-kube-controllers-85578c44bf-xqlmz             1/1     Running   4 (133m ago)    4d15h   172.16.247.185    controller.creditizens.net   <none>           <none>            k8s-app=calico-kube-contr
ollers,pod-template-hash=85578c44bf
kube-system   calico-node-8v4pl                                    1/1     Running   19 (133m ago)   623d    192.168.186.146   controller.creditizens.net   <none>           <none>            controller-revision-hash=
754b4dfc9f,k8s-app=calico-node,pod-template-generation=1
kube-system   calico-node-wh76j                                    1/1     Running   6 (133m ago)    623d    192.168.186.147   node1.creditizens.net        <none>           <none>            controller-revision-hash=
754b4dfc9f,k8s-app=calico-node,pod-template-generation=1
kube-system   calico-node-zk8zl                                    1/1     Running   7 (133m ago)    623d    192.168.186.148   node2.creditizens.net        <none>           <none>            controller-revision-hash=
754b4dfc9f,k8s-app=calico-node,pod-template-generation=1
kube-system   coredns-5d78c9869d-kpbtn                             1/1     Running   5 (133m ago)    5d17h   172.16.247.183    controller.creditizens.net   <none>           <none>            k8s-app=kube-dns,pod-temp
late-hash=5d78c9869d
kube-system   coredns-5d78c9869d-l47pn                             1/1     Running   4 (133m ago)    4d15h   172.16.247.184    controller.creditizens.net   <none>           <none>            k8s-app=kube-dns,pod-temp
late-hash=5d78c9869d
kube-system   etcd-controller.creditizens.net                      1/1     Running   5 (133m ago)    5d18h   192.168.186.146   controller.creditizens.net   <none>           <none>            component=etcd,tier=contr
ol-plane
kube-system   kube-apiserver-controller.creditizens.net            1/1     Running   6 (133m ago)    5d18h   192.168.186.146   controller.creditizens.net   <none>           <none>            component=kube-apiserver,
tier=control-plane
kube-system   kube-controller-manager-controller.creditizens.net   1/1     Running   14 (133m ago)   5d18h   192.168.186.146   controller.creditizens.net   <none>           <none>            component=kube-controller
-manager,tier=control-plane
kube-system   kube-proxy-88h87                                     1/1     Running   4 (133m ago)    5d17h   192.168.186.147   node1.creditizens.net        <none>           <none>            controller-revision-hash=
747c75b954,k8s-app=kube-proxy,pod-template-generation=2
kube-system   kube-proxy-bs58j                                     1/1     Running   5 (133m ago)    5d17h   192.168.186.148   node2.creditizens.net        <none>           <none>            controller-revision-hash=
747c75b954,k8s-app=kube-proxy,pod-template-generation=2
kube-system   kube-proxy-mwnmk                                     1/1     Running   5 (133m ago)    5d17h   192.168.186.146   controller.creditizens.net   <none>           <none>            controller-revision-hash=
747c75b954,k8s-app=kube-proxy,pod-template-generation=2
kube-system   kube-scheduler-controller.creditizens.net            1/1     Running   14 (133m ago)   5d18h   192.168.186.146   controller.creditizens.net   <none>           <none>            component=kube-scheduler,
tier=control-plane
```

- We are going to make groups of `label` `keys` to have columns and group of resources
 
```bash
k get pod --all-namespaces -Lcontroller-revision-hash -Lk8s-app -Lcomponent 
NAMESPACE     NAME                                                 READY   STATUS    RESTARTS         AGE     CONTROLLER-REVISION-HASH   K8S-APP                   COMPONENT
kube-system   calico-kube-controllers-85578c44bf-xqlmz             1/1     Running   4 (3h21m ago)    4d16h                              calico-kube-controllers   
kube-system   calico-node-8v4pl                                    1/1     Running   19 (3h21m ago)   623d    754b4dfc9f                 calico-node               
kube-system   calico-node-wh76j                                    1/1     Running   6 (3h21m ago)    623d    754b4dfc9f                 calico-node               
kube-system   calico-node-zk8zl                                    1/1     Running   7 (3h21m ago)    623d    754b4dfc9f                 calico-node               
kube-system   coredns-5d78c9869d-kpbtn                             1/1     Running   5 (3h21m ago)    5d18h                              kube-dns                  
kube-system   coredns-5d78c9869d-l47pn                             1/1     Running   4 (3h21m ago)    4d16h                              kube-dns                  
kube-system   etcd-controller.creditizens.net                      1/1     Running   5 (3h21m ago)    5d19h                                                        etcd
kube-system   kube-apiserver-controller.creditizens.net            1/1     Running   6 (3h21m ago)    5d19h                                                        kube-apiserver
kube-system   kube-controller-manager-controller.creditizens.net   1/1     Running   14 (3h21m ago)   5d19h                                                        kube-controller-manager
kube-system   kube-proxy-88h87                                     1/1     Running   4 (3h21m ago)    5d19h   747c75b954                 kube-proxy                
kube-system   kube-proxy-bs58j                                     1/1     Running   5 (3h21m ago)    5d19h   747c75b954                 kube-proxy                
kube-system   kube-proxy-mwnmk                                     1/1     Running   5 (3h21m ago)    5d19h   747c75b954                 kube-proxy                
kube-system   kube-scheduler-controller.creditizens.net            1/1     Running   14 (3h21m ago)   5d19h                                                        kube-scheduler 
```

## `-l` to label pods or get pods based on labels, `-L` to create colum based on those labels keys

- from `Kubernetes` documentation
```bash
kubectl label pods -l app=nginx tier=fe

kubectl get pods -l app=nginx -L tier
Output:
NAME                        READY     STATUS    RESTARTS   AGE       TIER
my-nginx-2035384211-j5fhi   1/1       Running   0          23m       fe
my-nginx-2035384211-u2c7e   1/1       Running   0          23m       fe
my-nginx-2035384211-u3t6x   1/1       Running   0          23m       fe
```

# Node Affinity and Anti-Affinity

- here we can use `nodeSelector` which only select nodes with specific labels
- vs Node `affinity`/ `anti-affinity` adds some more option and control over the node selection:
  - `soft` and `preferred` keyword can be used to tell `scheduler` to schedule `pod` event no `nodes` are matching
  - or create rules based on other `pods` `labels`

From kubernetes Documentation. (source)[https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/]:
- `Node affinity` functions like the `nodeSelector` field but is more expressive and allows you to specify soft rules.
- `Inter-pod` `affinity/anti-affinity` allows you to constrain `Pods` against `labels` on other `Pods`.

## Node Affinity
- two keywords, Like `nodeSelector` but with more detailed specification:
  - `requiredDuringSchedulingIgnoredDuringExecution`: `required` so MUST be equal to those specifications.
  - `preferredDuringSchedulingIgnoredDuringExecution`: `preferred` so NOT STRICT RULE but preferred rule.

Eg. from kubernetes documentation (just spec of pod part targeting our subject):
(`operator` can be any of those values: `In`, `NotIn`, `Exists`, `DoesNotExist`, `Gt` or `Lt`)
```yaml
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution: # MUST: for the rules coming after that
        nodeSelectorTerms:
        - matchExpressions: # if many `matchExpressions` it is `AND` so all have to match for `pod` to be scheduled
          - key: topology.kubernetes.io/zone
            # can be: `In`, `NotIn`, `Exists`, `DoesNotExist`, `Gt` or `Lt`
            operator: In
            values: # OR `antarctica-east1` OR `antarctica-west1`
            - antarctica-east1
            - antarctica-west1
      preferredDuringSchedulingIgnoredDuringExecution: # PREFERRED but not obliged to: for the rules coming after that
      - weight: 1 # can be from 1 to 100. If many different `preferences` `weight` is calculated. `Nodes` satisfying highest score are prioritized
        preference:
          matchExpressions:
          - key: another-node-label-key
            # can be: `In`, `NotIn`, `Exists`, `DoesNotExist`, `Gt` or `Lt`
            operator: In
            values:
            - another-node-label-value
```

## ** Notes from documentation **
Note:
- specify both `nodeSelector` and `nodeAffinity`: both must be satisfied for the Pod to be scheduled onto a node.
- specify multiple terms in `nodeSelectorTerms` with `nodeAffinity` types:  `Pod` can be scheduled to node if one specified terms can be satisfied (`OR`).
- specify multiple expressions in a single `matchExpressions` associated with a term in `nodeSelectorTerms`: `Pod` can be scheduled on node only if all expressions satisfied (`AND`)

## Node Anti-Affinity
Is done using keyword in `operator`:
- `NotIn`
- `DoesNotExist`


# Taints & Tolerations
source: (doc taints and tolerations)[https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/]
**NOTE:**
if `.spec.nodeName` is specified in a `pod` then it will bypass the scheduler and not `taint` will be checked. It will be put in the `node` specified even if that one `pod` had a `toleration` not matching that `node` `taint` but it the node `taint` has a `NoExecute` `effect` in the `taint`, th epod will be ejected.

## Taints
- Taints are applied to `nodes` to tell which `pod` label is accepted to be scheduled in the `node`
In this example, `node1` is `tainted` to not accept any `pod` sheduled having `label` key `special` with value `mangakissa`
and the effect `:` `NoSchedule` to 'Not Schedule'.
If this `taint` already exists it can be deleted by adding same line but with a minus at the end `-` like `NoSchedule-` 
```bash
kubectl taint nodes node1 special=mangakissa:NoSchedule
```

## Toleration
- Tolerations are applied to `pods` to tell to scheduler where the `pod` can be scheduled
In this example of the `PodSpec` part of `pod` definition, we have two different ways of doing it: one with key/value and another with only key

```yaml
# key/value
tolerations:
# node taint need to match those fields when `Equal` `operator:` is used
- key: "special" # node tiant need to match key
  operator: "Equal"
  value: "mangakissa" # node taint need to match value
  effect: "NoSchedule" # node taint need to match effect

# key only
tolerations:
# node taint need to match those fields when `Exists` `operator:` is used
- key: "special" # node taint need to match key
  operator: "Exists"
  effect: "NoSchedule" # node taint need to match effect
```

### rules for `operator` in `toleration` `pod.spec`:
- `operator:` default value is `Equal` then a `value:` field should be specified
- if `operator:` is `Exists` then no `value:` field should be specified, it will match on `key:` and `effect:`

### rules for empty field
- if `key:` is empty then `operator:` have to be `Exists` and it will match all key/values. AND, `effect:` still need to match at the same time
- if `effect:` is empty then it will match all `effect:` with `key: <your_key>``

### values taken by `effect`
- `NoExecute`:
  - `taint` not tolerated: pods are evicted from the node
  - `taint` tolerated:
    - `tolerationSeconds` indicated in `pod`: after that timelapse the `pod` will be evicted from `node`
    - `tolerationSeconds` not indicated in `pod`: No eviction, `pod` will remain forever in `node`
- `NoSchedule`: existing `pod` on `node` not evicted, only `pod` with matching `toleration` can be schedule in `node` otherwise not
- `PreferredNoSchedule`: it is a `soft NoSchedule`, so scheduler will try to not put `pod` on `node` tainted
  or `pod` spec without `toleration` indicated, but it is not guaranteed.

### rules for mutiple `toleration:` indicated in same pod
Here the scheduler will work fine but use those as filters, checking `nodes` `taints`, the `pod` `effects` and `operators`

## list of `kubernetes` automatic `taints`
- `node.kubernetes.io/not-ready`: Node is not ready. This corresponds to the `NodeCondition` `Ready="False"`.
- `node.kubernetes.io/unreachable`: Node is unreachable from the node controller. This corresponds to the `NodeCondition` `Ready="Unknown"`.
- `node.kubernetes.io/memory-pressure`: Node has memory pressure.
- `node.kubernetes.io/disk-pressure`: Node has disk pressure.
- `node.kubernetes.io/pid-pressure`: Node has PID pressure.
- `node.kubernetes.io/network-unavailable`: Node's network is unavailable.
- `node.kubernetes.io/unschedulable`: Node is unschedulable.
- `node.cloudprovider.kubernetes.io/uninitialized`: For cloud to have time to make setup. After a controller from the `cloud-controller-manager` initializes this `node`, the `kubelet` removes this `taint`.

```yaml
tolerations:
- key: "node.kubernetes.io/unreachable"
  operator: "Exists"
  effect: "NoExecute"
  tolerationSeconds: 6000 # unless you set it like that it is automatically set to 300 (5mn) by the the controller
```

## example `node drain` of `taints` automatically applied to the `node`
When a `node` is `drained`, the `node controller` or `kubelet` adds `taints` with `NoExecute effect`. 
It also adds `node.kubernetes.io/not-ready` and `node.kubernetes.io/unreachable`


scenarios:
- 1) We can here justify that scenario with a troubleshooting in staging environment, so temporaly delete replicaset and then re-acquire pods: 
create deployment nginx with 1 replica only and use labels
create a pod with a label selector same as the deployment one to show that it dies
create a pod a label selector different from the deployment to show that it stays alive
delete the deployment tearing it down
keep the pod and deploy again with 3 replicas and show that that pod will be acquired by the replicaset (so replicaset will be only deploying 2 new pods and acquiring the existing one)
show labels grouping using -L which will create a column int he output of `get pods` using the key of the label

- 2) Pod deletion priority setup using annotation
- 1: first will be deleted any pods in `pending` state or `unschedulable`
- 2: then,  will come any pods with the `annotation`: `controller.kubernetes.io/pod-deletion-cost`. The lower number one is delete first and so on.
So here we could do a scenario to have pods being annotated and see how the deployment replicas are scaled down meaning in which order
We could create an error first having the full replicaspulling an image that doesn't exist
then patch some replicas to fix those pods and get those running
and then have the scaling down showing in which order those pods would be deleted
then do the deployment again with healthy pods 
then annotate some of those pods with the `annotation`: `controller.kubernetes.io/pod-deletion-cost` and then scale down to see in which order those are scale down


- 3) Scenario in which we would use `nodeselector` to affect pods in specific nodes
```
selector:
  matchLabels:
    component: redis
  matchExpressions:
    - { key: tier, operator: In, values: [cache] }
    - { key: environment, operator: NotIn, values: [dev] }
```
this would need us to label nodes and then have selectors in deploymed pods
this also could be used with `selector` and use `matchExpression` in order to show IN/NOTIN/EXIST/DOESNOTEXIST 
little explanation of those:
```
# key=`citynode`, values accepted=`tokyo` AND (because of comma: `,`) hokkaido
citynode in (tokyo, hokkaido)
# key=`nevereurope`, values=france AND emgland
nevereurope notin (france, england)
# key exclusive without value indicated. key=`shibuya`
shibuya
# key=NOT EQUAL to (because exclamation mark: `!`) shinjuku
!shinjuku
### more complexe requirements
# (key=`mangakissa`, with values=`ueno` AND `omotesando`), AND (key=`appdeploymentgroup`, values: NOT EQUAL to `production`)
mangakissa in (ueno, omotesando), appdeploymentgroup!=production

```

```
selector:
  matchLabels:
    component: redis
  matchExpressions:
    - { key: tier, operator: In, values: [cache] }
    - { key: environment, operator: NotIn, values: [dev] }
```

- 4) Scenario in which we would show how labelling helps groups resources and have nice Cluster resources view by grouping those in representative custom columns
```
k get pod --all-namespaces --show-labels -o wide
```
```
k get pod --all-namespaces -Lcontroller-revision-hash -Lk8s-app -Lcomponent
```

- 5) Scenario in which we would use a `nodeSelector` and then a node `affinity` to show that we can set more specific rules using `affinity`
- `requiredDuringSchedulingIgnoredDuringExecution`: `required` so MUST be equal to those specifications.
- `preferredDuringSchedulingIgnoredDuringExecution`: `preferred` so NOT STRICT RULE but preferred rule.
here show emphasis in explaning the ORed and ANDed of matchExpressions
```
# ANDed
requiredDuringSchedulingIgnoredDuringExecution:
  nodeSelectorTerms:
    - matchExpressions:
        - key: env
          operator: In
          values: [prod]
        - key: region
          operator: In
          values: [us-west]
```
```
# ORed
requiredDuringSchedulingIgnoredDuringExecution:
  nodeSelectorTerms:
    - matchExpressions:
        - key: env
          operator: In
          values: [prod]
    - matchExpressions:
        - key: region
          operator: In
          values: [us-west]
```

- 6) Scenario in which we will use again Japanese locations to show how `node` `taint` and `effect` work with `pod` `toleration`
  - `NoExecute`: first create a deployment with 3 replicasets, then taint one node with effect `NoExecute` to `evict` the pod and show that it is recreated in another node, then taint another node with the same way to show that pods are again evited and will recreated somewhere else. then untaint one node and delete one of the deplyed pods which will be redeployed in the untainted node. Then do the same with the other tainted node by untainting it and deleting the one of the two pods which are on the same node and it will we rescheduled in another node.
    - now use yaml file only:
      ```yaml
      tolerations:
      - key: "special"
        operator: "Equal"
        value: "mangakissa"
        effect: "NoSchedule"
        tolerationSeconds: 30 # how long before being evicted
      ```

  - `NoSchedule`: use from here only `yaml` file
      ```yaml
      tolerations:
      - key: "special"
        operator: "Equal"
        value: "mangakissa"
        effect: "NoSchedule"
      ```
     Then taint the node with matching `taint` and `effect` and you should see that existing pods are not evicted
     then create a standalone pod with a nodeSelector or Affinity to that node, to show that it won't be schedule there as it doesn't have the toleration for that node taint.
     then add the toleration to the pod yaml file and show that now it can be scheduled there
     then get rid of the toleration of the pod keeping the affinity or nodeselector and taint the node with `preferredNoSchedule` the soft version and see that pod will be scheduled there (maybe need to try...)

  - `NoExecute`: create two pods with affinity of node selector just to maake sure those two are scheduled in the same node.
    both will have a toleration with effect `NoExecute` but one has the `tolerationSeconds` and the other not
    then taint the node where those two pods are located with same taint matching their toleration therefore effect `NoExecute`
    You will see that the pod not having the `tolerationSeconds` will stay in the node while the other one will be evicted after those `tolerarionSeconds`
_______________________________________________________________________________
(From documentation for: `ownerReferences`)[https://kubernetes.io/docs/concepts/overview/working-with-objects/owners-dependents/]
# Pod being acquired
cat pod.yaml
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-standalone
  labels:
    app: nginx
spec:
  containers:
    - name: nginx
      image: nginx
```

`kubectl get pod <standalone_pod_name> -o jsonpath='{.metadata.ownerReferences}'`

cat replicaset.yaml
```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-rs
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx
```

`kubectl get pod <standalone_pod_name> -o jsonpath='{.metadata.ownerReferences}'`

# Replicset being acquired

cat replicaset.yaml
```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-orphan-rs
  # make sure to not forget to add here the label matching the one of the deployment otherwise this replicaset won't be acquired by the deployment
  labels: 
    app: nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
```

`kubectl get rs <replicaset_name> -o jsonpath='{.metadata.ownerReferences}'`


cat deployment.yaml
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-adopter
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
```
`kubectl get rs <replicaset_name> -o jsonpath='{.metadata.ownerReferences}'`

____________________________________________________________________

# DNS ACCESSING PODS THROUGH SERVICES USING NAMES

## PODS IN SAME NAMESPACE
As IPs can change there is in Kubernetes native `DNS` resolution:
- services need to be created first (before pod for the pod to be able to capture the env var set automaticcally by Kubernetes) if using IPs instead
  but in general using DNS name of the POD and SVC and NAMESPACE and CLUSTER would reolve to accessing the pod.
- `resolv.conf` file is where the DNS of the service will be indicated

```bash
k exec -it -n nginx nginx-pod -- sh
# cat /etc/resolv.conf
search nginx.svc.cluster.local svc.cluster.local cluster.local localdomain
nameserver 10.96.0.10
options ndots:5
```
So here the service name `nginx` is indicated in the `resolv.conf` file for the pod to be reached at `<pod_name>.nginx.svc.cluster.local`
So another pod in another namespace could pass throught the `ClusterIP` service named `nginx` in the namespace `nginx` and find the pod

**Experiement DNS Using Imperative Commands**
We will make a service that has a `spec.selector` pointing to a pod `label` and will create the pod with the same label , all in same namespaces
As we are using `names` we will be able to use `DNS` names and `not unstable` `IPs`, therefore, there is no rule in the order in which we will create the service, it can be after or before the pod creation

. 1) Run pod with label
```bash
k run nginx --image=nginx \
  --restart=Never \
  --namespace=nginx \
  --labels=app=nginx
```

. 2) Expose the pod to enable `DNS` resolution through a service `nginx-service` with a `selector` pointing to the `pod` `label`
After that we need to expose the service to get `DNS` `nginx-service.nginx.svc.cluster.local` ready and callable from anyu other pod in the cluster:
Here we do something easy to understand but policies can be created to limit access to only pods inside the same namespace for example as namespaces are made for that to separate concerns:
```bash
k expose pod nginx \
  --port=80 \
  --target-port=80 \
  --name=nginx-service \
  --namespace=nginx \
  --selector=app=nginx
```

. 3) Create another temparary pod to test that `nginx` pod is reachable through `DNS` resolution `nginx-service.dev.svc.cluster.local`
```bash
k run debug --rm -it --image=busybox --restart=Never \
  --namespace=nginx -- /bin/sh

# curl not available so used wget which pooled the `index.html` page meaning that the pod is accessible through the service
wget http://nginx-service
Outputs:
Connecting to nginx-service (10.110.245.196:80)
saving to 'index.html'
index.html           100% |************************************************************************************************************************|   615  0:00:00 ETA
'index.html' saved

# then we cat the file pulled
cat index.html 
Outputs:
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
outputs

# nslookup
# nslookup nginx-service.nginx.svc.cluster.local
Server:         10.96.0.10
Address:        10.96.0.10:53


Name:   nginx-service.nginx.svc.cluster.local
Address: 10.110.245.196
```
so here we get confirmation


## DNS RESOLUTION FROM POD IN ANOTHER NAMESPACE

. 4) Check if other pods in other namespaces can use the `DNS` to reach the pod
Here pod created in `default` namespace and will access the pod in `nginx` namespace using the `DNS` which maps the `pod` through the `service` name
```bash
k run debug --rm -it --image=busybox --restart=Never -- /bin/sh
If you don't see a command prompt, try pressing enter.

# nlookup way
/ # nslookup nginx-service.nginx.svc.cluster.local
Outputs:
Server:         10.96.0.10
Address:        10.96.0.10:53


Name:   nginx-service.nginx.svc.cluster.local
Address: 10.110.245.196

# wget way but this this time as we are not in same namesapce we need to provide full `DNS` with namespace
/ # wget nginx-service.nginx.svc.cluster.local
Outputs
Connecting to nginx-service.nginx.svc.cluster.local (10.110.245.196:80)
saving to 'index.html'
index.html           100% |************************************************************************************************************************|   615  0:00:00 ETA
'index.html' saved

# then we check that the file is nginx index.html
/ # cat index.html 
Outputs:
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```

## SETTING POD DNS RESOLV.CONF CONTENT FROM YAML CREATION
eg: source: (form doc kubernetes)[https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/]
```yaml
apiVersion: v1
kind: Pod
metadata:
  namespace: default
  name: dns-example
spec:
  containers:
    - name: test
      image: nginx
  # can be `Default`, `Clusterfirst`, `ClusterFirstWithHostNet` or `None`
  # when `None` we can custom config  `resolv.comf` of the pod
  dnsPolicy: "None"
  # all fields are not required, you can just choose to use one
  # or `nameservers`, or `searches` or `options`
  # but if you want more control you probably want to use them all
  dnsConfig:
    # get the `nameserver ip of the cluster by running this command:
    # kubectl get svc -n kube-system kube-dns
    :nameservers:
      - 10.96.0.10
    # here we put what services we want to reach using Kubernetes DNS way
    searches:
      # <service_anme.name_space.cluster.local>
      # or <namespace.cluster.local>
      - ns1.svc.cluster.local
      - my.dns.search.suffix
    # example of options that you find in `/etc/resolv.conf`
    options:
      # `ndots` is if lower thant `2` dots here
      # will try append the above `searches` to the `curl <domain>`
      # so will try `<domain>.ns1.svc.cluster.local`
      # then will try `<domain>.my.dns.search.suffix` if the first doesn't work and so on...
      - name: ndots
        # higer value more checks and latency, lower value better performance but could skip some
        value: "2"
      - name: edns0
      - name: timeout
        value: "2"
```

- `dnsPolicy` can take different values:
dnsPolicy	| Description	|Use When
------------+---------------+-----------
ClusterFirst (default)	| Uses cluster DNS (CoreDNS). Can resolve Kubernetes service names across namespaces.	| ✅ Most common for standard Pods|
------------------------+---------------------------------------------------------------------------------------+----------------------------------+
ClusterFirstWithHostNet	| Same as above but used when Pod uses hostNetwork: true	| ✅ Use when Pod shares host network|
------------------------+-----------------------------------------------------------+-------------------------------------+
Default	Uses the node’s /etc/resolv.conf. | Can’t resolve Kubernetes service DNS.	| ❌ Avoid unless you want to use external DNS only|
------------------------------------------+-----------------------------------------+--------------------------------------------------+
None	| Lets you manually define DNS config via dnsConfig in the Pod spec	| ✅ Use if you want full custom DNS entries, e.g., override search domains or stub resolvers|
--------+-------------------------------------------------------------------+--------------------------------------------------------------------------------------------+

**Summary**
- `ClusterFirst`: is the default one using CorDNS of Kubernetes
- `ClusterfirstWithHostNet`: Only when `hostNetwork: true`, when pod shares host network
- `Default`: not the 'default' as the name says this only for external DNS
- `None`: when setting custom DNS in pods via `dnsConfig` (/etc/resolv.conf)
  - `nameservers` ip of the cluster is given by CoreDNS `kube-dns` in `kube-system` namespace:
```bash
kubectl get svc -n kube-system kube-dns
Outputs
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   630d
```


## SETUP NETWORK POLICY AS DNS IS RESOLVING ANY SERVICE MAPPED TO POD INSIDE THE CLUSTER
source: (Network Policies Doc)[https://kubernetes.io/docs/concepts/services-networking/network-policies/]

**Important:** By default the Kubernetes allow all traffic and only when you set a rules it will deny all other rules:
`kind: NetworkPolicy` is deny all when set and in the policy you are going to allow `ingress`/`egress`
therefore, you just set the policy and put what is allowed in the rest will be denied.
And this denial is activated because you have set a rule `ingress` or `egress`. without rule all is allowed.


here we have an example in how to setup a policy.
`ingress` and `egress` can be setup, refer to documentation for `egress` as here we are going to do only `ingress`
`egress` is the same anyway but just you replace `ingress` examples with `egress` or just refer to doc as it might change


### INGRESS:
- (Ingress rules) allows connections to all pods in the default namespace with the label role=db on TCP port 6379 from:
      - any pod in the default namespace with the label role=frontend
      - any pod in a namespace with the label project=myproject
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  ingress:
  - from:
            # we just see `namespaceSelector` and `podSelector` not the third one `ipBlock`
    - namespaceSelector:
        matchLabels:
          project: myproject
    - podSelector:
        matchLabels:
          role: frontend
    ports:
    - protocol: TCP
      port: 6379
```

### DENY ALL

eg: deny all traffic
```yaml
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: {}
  policyTypes:
  - Ingress
```

### ANDed VS ORed RULES

eg: ANDed vs ORed
- ANDed
```yaml
  ingress:
  # here `single` element in the `- from` (`- namespaceSelector`) which make it `ANDed`
  - from:
    # note here that here only `namespaceSelector` have the `-` which enabled the `AND`for the next rule `podSelector`
    - namespaceSelector:
        matchLabels:
          user: alice
      podSelector:
        matchLabels:
          role: client
```
- ORed
```yaml
  ingress:
  # here two elements in the `- from` (`- namespaceSelector`, `- podSelector`) which makes it ORed
  - from:
    - namespaceSelector:
        matchLabels:
          user: alice
    # note that here we have `-` which is enabling the `OR` instead of `AND`
    - podSelector:
        matchLabels:
          role: client
```


1) scenario that shows how dns works, very simple, within same namespace creating pod and exposing with a service and creating a third pod that would use dns to access the pod using DNS (curl/wget/nslookup whatever works)
then do same scenario but this time the third pod is created outside of the namespace and curl/wget/nslookup again to show that DNS means that pods from other namespaces can resolve to the pod using DNS call

2) do another scenario showing now how to setup a pods and determining the reolv.conf of the pod content so that pod can call that other pod through another service dedicated to that other pod. maybe customize nginx in that namespace with different one and different messages index.html pages and having each different services attached to those. and go inside pod to show that it can resolve that pod
eg: source: (form doc kubernetes)[https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/]
apiVersion: v1
kind: Pod
metadata:
  namespace: default
  name: dns-example
spec:
  containers:
    - name: test
      image: nginx
  dnsPolicy: "None"
  dnsConfig:
    nameservers:
      - 192.0.2.1 # this is an example
    searches:
      - ns1.svc.cluster-domain.example
      - my.dns.search.suffix
    options:
      - name: ndots
        value: "2"
      - name: edns0

3) now make it more complexe by making this example more interesting by adding a network policy as before even if not in resolv conf the pod could use the kubernetes native dns call to reach the other pod anyway, but this policy would say no ingress accepted from other namespaces for that pod specifically but the other pod would be still reachable
source services can be created with name on ports which would be included in the DNS to target that port so the service behind it, can be nice to use to target the different nginx behind it with different html pages: (doc)[https://kubernetes.io/docs/concepts/services-networking/service/]
o here say that network plugins are required and that we are using `Calico` already installed in the cluster and it is a prerequisite
  - example from doc:
    - (Ingress rules) allows connections to all pods in the default namespace with the label role=db on TCP port 6379 from:
      - any pod in the default namespace with the label role=frontend
      - any pod in a namespace with the label project=myproject
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  ingress:
  - from:
            # we just see `namespaceSelector` and `podSelector` not the third one `ipBlock`
    - namespaceSelector:
        matchLabels:
          project: myproject
    - podSelector:
        matchLabels:
          role: frontend
    ports:
    - protocol: TCP
      port: 6379
```

eg: deny all traffic
```yaml
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: {}
  policyTypes:
  - Ingress
```

eg: ANDed vs ORed
- ANDed
```yaml
  ingress:
  # here `single` element in the `- from` (`- namespaceSelector`) which make it `ANDed`
  - from:
    # note here that here only `namespaceSelector` have the `-` which enabled the `AND`for the next rule `podSelector`
    - namespaceSelector:
        matchLabels:
          user: alice
      podSelector:
        matchLabels:
          role: client
```
- ORed
```yaml
  ingress:
  # here two elements in the `- from` (`- namespaceSelector`, `- podSelector`) which makes it ORed
  - from:
    - namespaceSelector:
        matchLabels:
          user: alice
    # note that here we have `-` which is enabling the `OR` instead of `AND`
    - podSelector:
        matchLabels:
          role: client
```

### ALLOW ALL INGRESS
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-ingress
spec:
  podSelector: {}
  ingress:
  - {}
  policyTypes:
  - Ingress
```

k run nginx-pod --port=80  --labels=location=shibuya --image=nginx
k run debug --rm -it --image=busybox --restart=Never   --namespace=nginx -- /bin/sh
k expose pod nginx   --port=80   --target-port=80  --namespace=nginx
k expose pod nginx-pod --port=80 --target=port=80 --name=nginx-service --selector=location=shibuya --type=NodePort

# networkpolicy to prevent all ingress traffic to hachiko pod and permit only the right labelled namespace and the right labelled pod
cat network-policy-aishibuya-hachiko.yaml 
# network-policy-aishibuya-hachiko.yaml NetworkPolicy
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: aishibuya-network-policy
  namespace: aishibuya
spec:
  podSelector:
    matchLabels:
      shibuya-location: hachiko
  policyTypes:
  - Ingress
  ingress:
  - from:
    # ANDed as no `-` at `podSelector`
    - namespaceSelector:
        matchLabels:
          # need to label the default namespace with that `zone=tokyo`
          zone: tokyo
      podSelector:
        matchLabels:
          shibuya-fan: access
    ports:
    - protocol: TCP
      port: 80
#spec:
#  podSelector: {}
#  policyTypes:
#  - Ingress


# pods running in defaultnamespace which will be used in `exec -it` mode to use dns lookups, busybox for nslookup and nginx pods with different labels for curl commands
cat pod_resolv_conf_services_without_netpolicy_busybox.yaml 
# pod_resolv_conf_services_having_netpolicy_busybox.yaml Pod
apiVersion: v1
kind: Pod
metadata:
  namespace: default
  name: busy-tokyo
  #name: nginx2-tokyo
spec:
  containers:
    #- name: nginx2-tokyo-connect-to-aishibuya
    #  image: nginx
    - name: busy-tokyo-connect-to-aishibuya
      image: busybox:1.35
      # this necessary as busybox exits and pod will never stay alive so we need to sleep it for a while so we can do our work
      command: ["sleep", "36000"]
  # can be `Default`, `Clusterfirst`, `ClusterFirstWithHostNet` or `None`
  # when `None` we can custom config  `resolv.comf` of the pod
  dnsPolicy: "None"
  dnsConfig:
    # obtained ip by running: `kubectl get svc -n kube-system kube-dns`
    nameservers:
      - 10.96.0.10
    # here we put what services we want to reach using Kubernetes DNS way
    searches:
      # <name_space.cluster.local>
      # OR also can add: <service_name.namespace.svc.cluster.local>
      - aishibuya.svc.cluster.local
    options:
      - name: ndots
        value: "5"

cat pod_resolv_conf_services_without_netpolicy_nginx2.yaml
# pod_resolv_conf_services_having_netpolicy_nginx2.yaml Pod
apiVersion: v1
kind: Pod
metadata:
  namespace: default
  #name: busy-tokyo
  name: nginx2-tokyo
spec:
  containers:
    - name: nginx2-tokyo-connect-to-aishibuya
      image: nginx
    #- name: busy-tokyo-connect-to-aishibuya
      #image: busybox:1.35
      # this necessary as busybox exits and pod will never stay alive so we need to sleep it for a while so we can do our work
      #command: ["sleep", "36000"]
  # can be `Default`, `Clusterfirst`, `ClusterFirstWithHostNet` or `None`
  # when `None` we can custom config  `resolv.comf` of the pod
  dnsPolicy: "None"
  dnsConfig:
    # obtained ip by running: `kubectl get svc -n kube-system kube-dns`
    nameservers:
      - 10.96.0.10
    # here we put what services we want to reach using Kubernetes DNS way
    searches:
      # <name_space.cluster.local>
      # OR also can add: <service_name.namespace.svc.cluster.local>
      - aishibuya.svc.cluster.local
    options:
      - name: ndots
        value: "5"

cat pod_resolv_conf_services_having_netpolicy.yaml 
# pod_resolv_conf_services_having_netpolicy.yaml Pod
apiVersion: v1
kind: Pod
metadata:
  namespace: default
  name: nginx-tokyo
  labels:
    shibuya-fan: access
spec:
  containers:
    - name: nginx-tokyo-connect-to-aishibuya
      image: nginx
  # can be `Default`, `Clusterfirst`, `ClusterFirstWithHostNet` or `None`
  # when `None` we can custom config  `resolv.comf` of the pod
  dnsPolicy: "None"
  dnsConfig:
    # obtained ip by running: `kubectl get svc -n kube-system kube-dns`
    nameservers:
      - 10.96.0.10
    # here we put what services we want to reach using Kubernetes DNS way
    searches:
      # <name_space.cluster.local>
      # OR also can add: <service_name.namespace.svc.cluster.local>
      - aishibuya.svc.cluster.local
    options:
      - name: ndots
        value: "5"


k get pods
NAME           READY   STATUS    RESTARTS   AGE
busy-tokyo     1/1     Running   0          93m
nginx-tokyo    1/1     Running   0          50m
nginx2-tokyo   1/1     Running   0          5s

# pods applied to the cluster with different config maps for different messages display and configmaps in one unique file
cat nginx_aishibuya_tsutaya_pod.yaml 
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: nginx
    shibuya-location: tsutaya
  name: nginx-tsutaya
  namespace: aishibuya
spec:
  containers:
  - name: nginx
    image: nginx
    volumeMounts:
    - name: creditizens-aishibuya-custom-message
      mountPath: /usr/share/nginx/html/
  volumes:
  - name: creditizens-aishibuya-custom-message
    configMap:
      name: aishibuya-tsutaya-message


cat nginx_aishibuya_hachiko_pod.yaml 
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: nginx
    shibuya-location: hachiko
  name: nginx-hachiko
  namespace: aishibuya
spec:
  containers:
  - name: nginx
    image: nginx
    volumeMounts:
    - name: creditizens-aishibuya-custom-message
      mountPath: /usr/share/nginx/html/
  volumes:
  - name: creditizens-aishibuya-custom-message
    configMap:
      name: aishibuya-hachiko-message

cat config_maps_for_aishibuya.yaml 
# config_map_for_aishibuya.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: aishibuya-hachiko-message
  namespace: aishibuya
data:
  index.html: |
    <html>
    <h1 style="color:green;">Hachiko Statute Will Be Renovated</h1>
    </br>
    <h1 style="color:green;">The Refreshed Hachiko Will Be Available From December 2025.</h1>
    </html>

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: aishibuya-tsutaya-message
  namespace: aishibuya
data:
  index.html: |
    <html>
    <h1 style="color:orange;">Starbuck Will Be Closed Till November 2025</h1>
    </br>
    <h1 style="color:red;">But Tsutaya Upper Stairs Will Still Be Open.</h1>
    </html>


# the services applied to cluster
cat hachiko-service.yaml 
apiVersion: v1
kind: Service
metadata:
  labels:
    app: nginx
  name: hachiko-service
  namespace: aishibuya
spec:
  ports:
  - name: access-hachiko
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    shibuya-location: hachiko
  type: ClusterIP

cat tsutaya-service.yaml 
apiVersion: v1
kind: Service
metadata:
  labels:
    app: nginx
  name: tsutaya-service
  namespace: aishibuya
spec:
  ports:
  - name: access-tsutaya
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    shibuya-location: tsutaya
  type: ClusterIP

k get svc -n aishibuya 
NAME              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
hachiko-service   ClusterIP   10.101.194.141   <none>        80/TCP    13m
tsutaya-service   ClusterIP   10.111.135.216   <none>        80/TCP    13m


# getting th epods to see if matching service backend (just after this step)
k get pods -o wide -n aishibuya 
NAME            READY   STATUS    RESTARTS   AGE   IP              NODE                    NOMINATED NODE   READINESS GATES
nginx-hachiko   1/1     Running   0          71m   172.16.210.82   node2.creditizens.net   <none>           <none>
nginx-tsutaya   1/1     Running   0          70m   172.16.206.71   node1.creditizens.net   <none>           <none>

# can't reach the pod that has an ingress policy because the label of the pod not matching even if the namespace label matches (we used ANDed rule)
kubectl get endpoints hachiko-service -n aishibuya
NAME              ENDPOINTS          AGE
hachiko-service   172.16.210.82:80   10s  # correctly pointing to only one pod behind the service and it is the right one (right ip, see above get pods)
k exec -it nginx2-tokyo -- bash
root@nginx2-tokyo:/# curl tsutaya-service.aishibuya.svc.cluster.local
<html>
<h1 style="color:orange;">Starbuck Will Be Closed Till November 2025</h1>
</br>
<h1 style="color:red;">But Tsutaya Upper Stairs Will Still Be Open.</h1>
</html>
root@nginx2-tokyo:/# curl hachiko-service.aishibuya.svc.cluster.local
^C
root@nginx2-tokyo:/# curl hachiko-service.aishibuya.svc.cluster.local
^C



# can access to the right pod as it is matching policy labels for namespace and pod origin
k get pods
NAME           READY   STATUS    RESTARTS   AGE
busy-tokyo     1/1     Running   0          114m
nginx-tokyo    1/1     Running   0          71m
nginx2-tokyo   1/1     Running   0          21m
k exec -it nginx-tokyo -- bash
root@nginx-tokyo:/# curl tsutaya-service.aishibuya.svc.cluster.local
<html>
<h1 style="color:orange;">Starbuck Will Be Closed Till November 2025</h1>
</br>
<h1 style="color:red;">But Tsutaya Upper Stairs Will Still Be Open.</h1>
</html>
root@nginx-tokyo:/# curl hachiko-service.aishibuya.svc.cluster.local
<html>
<h1 style="color:green;">Hachiko Statute Will Be Renovated</h1>
</br>
<h1 style="color:green;">The Refreshed Hachiko Will Be Available From December 2025.</h1>
</html>
root@nginx-tokyo:/# 

# can still nslookup but this stops at the service and gets the ips of backend and ports
k exec -it busy-tokyo -- sh
/ # nslookup _access-tsutaya._tcp.tsutaya-service.aishibuya.svc.cluster.local
Server:		10.96.0.10
Address:	10.96.0.10:53


Name:	_access-tsutaya._tcp.tsutaya-service.aishibuya.svc.cluster.local
Address: 10.103.229.167

/ # nslookup _access-hachiko._tcp.hachiko-service.aishibuya.svc.cluster.local
Server:		10.96.0.10
Address:	10.96.0.10:53

Name:	_access-hachiko._tcp.hachiko-service.aishibuya.svc.cluster.local
Address: 10.101.83.147


_____________________________________________________________________

# Init Containers & Sidecar Containers

- `Init Containers`:
  source: (init container doc)[https://kubernetes.io/docs/concepts/workloads/pods/init-containers/]
  - first `kublet` makes `network` and `storage` ready then `init container` can start
  - will always start, run, and finish running before any other container starts
  - if many defined: will start sequentially and one starts when the other is done completely
  - defined in the pod at the container level
  - it is NOT possible to define any king of `..probes` (readiness, liveliness, lifecycle...etc...)
  - BUt `resource limits` and `security` can be defined
  - can be used to set conditions to be met before `pod` starts main container app.
  - can be used to have access to some secrets, or install tools (awk, dig, python...etc...) making the app container lighter
  - can be used to wait for a `service` to be ready or even use pod ip to inject in jinja template for pod configuration necessity
  - can become a `side container` and live as long as pod is alive if a `sidecar container` is defined inside of it
  - shares same `namespace`, `network` and `volumes` (use of `emptyDir`) as the main container app. but not `probes can be defined`
  - changing the `container image` wouldn't restart `pod` but just the `container`

- `Sidecar Containers`:
  source: (sidecar doc)[https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/]
  - unlike `Init Containers` it will be started alongside normal containers
  - it is possible to define `..probe` (readiness, liveliness...etc.., lifecycle), then this probe would be used for the `pod` `probe` like if `readinessProbe` would be used for `pod` `readinessProbe`
  - shares resources with other containers of the same pod application
  - used to set logging, monitoring, data synchronization, security
  - can be an `init container` with `restatPolicy` set to `always` which will make it run during the full lifetime of the pod. OR just a normal pod which is the way to do it normally but make sure it is just to do some side stuff.
  - would be terminated after the main pod container app. and would be created just after `init containers`
  - can be defined inside an `init container` and would live as long as the `pod` is alive. so here here it is like an `init container` that would not exit and next `init container` would start without waiting for this one to exit when the `sidecar` defined inside the `init container` would start
  - lives alongside main container pod and have its own `restarPolicy` and can be `scaled` separately but shares `namespace`, `networking` and `resources` (limits, volumes...) with the `pod`. = `independent lifecycle`

## Scenario 1: Creating a Pod With Init Container Writing Custom Nginx HTML Page

So here instead of using `kind: ConfigMap` as we did in the past, we are going to use an `init container` to customize
the `nginx` `html` page.
It is like making a copy of from the `init container` pod path location of the volume having that cutom `index.html` to the `nginx` `volumeMount` `mountPath` location.
Doing a `port-forward` of the pod would provide access to the browser to show the custom page!

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: tower-109-promo-message
  namespace: department-stores
spec:
  volumes:
    - name: html-message
      emptyDir: {}

  initContainers:
    - name: init-html
      # can use custom image if want to have full control
      # meaning that here you would create a container which have copied the necessary files and just retrieve it from it
      image: busybox:1.35
      # command can be `wget` pulling file from the repo git or copy `cp` command to copy from custom image registry or other like copy from filesytem local, can be a jinja file which will be populated by the app container later...etc..
      # here we just `echo` command a sentence simply
      # best practice is to make this command `idempotent` so make sure that file doesn't already exist as we might get an error (not done in this example)
      command: ['sh', '-c', 'echo "<h1 style="color:#800f71">Shibuya 109 is Running a Sakura Promo During April: ALL 30% OFF!!</h1>" > /department-store/109/messages/index.html']
      volumeMounts:
        - name: html-109-message
          mountPath: /department-store/109/messages/

  containers:
    - name: nginx
      image: nginx
      volumeMounts:
        - name: html-109-message
          mountPath: /usr/share/nginx/html
```

- another example: cat tower-109-promo-message.yaml 
```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: tower-109-promo-message
  name: tower-109-promo-message
  namespace: department-stores
spec:
  # main container app
  containers:
    - name: nginx
      image: nginx
      volumeMounts:
        - name: html-109-message
          mountPath: /usr/share/nginx/html/

  # init container
  initContainers:
      # can use custom image if want to have full control
      # meaning that here you would create a container which have copied the necessary files and just retrieve it from it
    - name: init-109-html
      image: busybox:1.35
      # command can be `wget` pulling file from the repo git or copy `cp` command to copy from custom image registry or other like copy from filesytem local, can be a  jinja file which will be populated by the app container later...etc..
      # here we just `echo` command a sentence simply
      # best practice is to make this command `idempotent` so make sure that file doesn't already exist as we might get an error (not done in this example)
      command: ['sh', '-c', 'echo "<h1 style="color:#800f71">Shibuya 109 is running a Spring Sakura Promo: ALL -30%!!!!</h1>" > /depratment-stores/109/message/html/index.html']
      volumeMounts:
        - name: html-109-message
          mountPath: /depratment-stores/109/message/html/

  # now we need the shared volumes between both
  volumes:
    - name: html-109-message
      emptyDir: {}
```

```bash
# make it available for the browser using exposition type `port-fowrward
kubectl apply -f nginx-with-init.yaml
kubectl port-forward pod/nginx-with-init 8080:80
curl http://localhost:8080
```

### Order Of Execution
The order of `Init Containers`, `Sidecar Containers`, even `pods` is determined by the `resource/limits`
The more resources is asked the first it would be ran.
So always `Init Container` first runs, then it is checked which one has highest `resource/limits` to determine which one starts first, therefore, not rquesting resource and limits, means it is the highest.
Then `Sidecars`, and then `pods` would run and also here `resource/limits` would deternmine in which orderi
Otherwise, for each group it would be done sequentially, in the order of how those are defines in the `yaml` file's `.spec.initContainers`.

### Order of Deletion
When termination pod, first the main application container would stop, then the `sidecars` so it following the inverse order of the execution when starting the pod.

## Scenario 2: Nginx logs are being capture by a sidecar container (2 ways)
- way 1: `sidecar container` is an `init container` with `restartPolicy` equal to `always`
- way 2: `sidecar container` is a normal pod capturing nginx logs
- Both ways use the technique of setting `emptyDir: {}` for shared volumes between the two (like I do in my `Python` apps when i create a dynamic `.env` file to share data between processes)

### Way 1:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-init-sidecar
spec:
  initContainers:
    - name: log-agent
      image: busybox
      command: ["sh", "-c", "echo '###### running from sidecar Init Container Way ########' > /share/index.html && tail -f /shared/index.html"]
      volumeMounts:
        - name: shared-content
          mountPath: /shared
      restartPolicy: Always  # ✅ THIS IS CRUCIAL

  containers:
    - name: nginx
      image: nginx
      volumeMounts:
        - name: shared-content
          mountPath: /usr/share/nginx/html

  volumes:
    - name: shared-content
      emptyDir: {}
```


**VERY IMPORTANT**
- **Feature SidecarContainers need to be activated on the cluster as of the v1.28.15 of kubeadm it is not activated, after from next version it will be** 
  - **first get the `kubeadm` config file boilerplate and update it for all component activating the feature `SidecarContainers` also adding the controller ip address and the controller hostname as it is ran on the controller node**:
```bash
cat kubeadm_config_to_patch_sidecar_feature_enablement_boilerplate.yaml
```

```yaml
# get this boilerplate file that you need to update manually using: `kubeadm config print init-defaults > kubeadm-config.yaml`
apiVersion: kubeadm.k8s.io/v1beta3
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  ### need to update this to `advertiseAddress:` to the control plane api address running `k get nodes -o wide` or `hostname -I | awk '{print $1}'`
  ### advertiseAddress: 1.2.3.4
  advertiseAddress: 192.168.186.146
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/containerd/containerd.sock
  imagePullPolicy: IfNotPresent
  ### make sure the hsotname matched the controller node one `run: `hostname`` and get the result of that field here
  name: controller.creditizens.net
  taints: null
  ### this activate the sidecar feature
  kubeletExtraArgs:
    feature-gates: "SidecarContainers=true"
---
apiServer:
  timeoutForControlPlane: 4m0s
  ### this to add the feature `SidecarContainers` to the API server
  extraArgs:
    feature-gates: "SidecarContainers=true"
apiVersion: kubeadm.k8s.io/v1beta3
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
### updated controller manager from default `controllerManager: {}` to add the feature `SidecarContainers`
#controllerManager: {}
controllerManager:
  extraArgs:
    feature-gates: "SidecarContainers=true"
dns: {}
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.k8s.io
kind: ClusterConfiguration
kubernetesVersion: 1.28.0
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
### updated scheduler fromd efault `scheduler: {}` to add the feature `SidecarContainers`
#scheduler: {}
scheduler:
  extraArgs:
    feature-gates: "SidecarContainers=true"

# then run : sudo kubeadm upgrade apply <your_kubeadm_actual_version> --config=<name_of_this_pathcer_yaml_file>
```


- After that that worked fine showing that it is activated as otherwise it wouldn't accept the `restartPolicy: Always` inside the `initContainers`:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: manga-kissa-abunai
  labels:
    app: manga-kissa-abunai
spec:

  containers:
    - name: proxy-mangakissa
      image: nginx
      volumeMounts:
        - name: mangakissa-shared-volume
          mountPath: /usr/share/nginx/html/

  initContainers:
    - name: log-mangakissa-events
      image: busybox:1.35
      command: ['sh', '-c']
      args:
        - |
          mkdir -p /mangakissa/events/
          echo "*****Running From Init Container: Log-MangaKissa-Events****** \n <h1 style=\"color:red;\">Manga Kissa Abunai!</h1>" > /mangakissa/events/index.html
          sleep 3600
      volumeMounts:
        - name: mangakissa-shared-volume
          mountPath: /mangakissa/events/
      restartPolicy: Always

  volumes:
    - name:  mangakissa-shared-volume
      emptyDir: {}

```

- you can check the `real cluster config and see that `SidecarContainers` feature activation is there and that is why the pod could run without any issues
```bash
kubectl get configmap kubeadm-config -n kube-system -o yaml > real_cluster_config.yaml
cat real_cluster_config.yaml
```
```yaml
apiVersion: v1
data:
  ClusterConfiguration: |
    apiServer:
      extraArgs:
        authorization-mode: Node,RBAC
        feature-gates: SidecarContainers=true
      timeoutForControlPlane: 4m0s
    apiVersion: kubeadm.k8s.io/v1beta3
    certificatesDir: /etc/kubernetes/pki
    clusterName: kubernetes
    controllerManager:
      extraArgs:
        feature-gates: SidecarContainers=true
    dns: {}
    etcd:
      local:
        dataDir: /var/lib/etcd
    imageRepository: registry.k8s.io
    kind: ClusterConfiguration
    kubernetesVersion: v1.28.15
    networking:
      dnsDomain: cluster.local
      serviceSubnet: 10.96.0.0/12
    scheduler:
      extraArgs:
        feature-gates: SidecarContainers=true
kind: ConfigMap
metadata:
  creationTimestamp: "2023-07-07T07:39:53Z"
  name: kubeadm-config
  namespace: kube-system
  resourceVersion: "413627"
  uid: 744a0490-3f7a-4d87-bedd-4b225edd0758
```

### Way 2:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-normal-sidecar
spec:
  # so here we have two containers running and defined, one being a sidecar container
  containers:
    - name: log-agent
      image: busybox
      command: ["sh", "-c", "echo '###### running from sidecar Container Normal Way ########' > /share/index.html && tail -f /shared/index.html"]
      volumeMounts:
        - name: shared-content
          mountPath: /shared

    - name: nginx
      image: nginx
      volumeMounts:
        - name: shared-content
          mountPath: /usr/share/nginx/html

  volumes:
    - name: shared-content
      emptyDir: {}
```

- another example that works fine and contextualized:
```bash
cat sidecar_container_as_sidecar_container_the_normal_one.yaml 
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: tokyo-tower-sidecar-info
spec:
  # so here we have two containers running and defined, one being a sidecar container
  containers:
    - name: info-of-the-day
      image: busybox
      command: ["sh", "-c", "echo '###### Tokyo Tower Will Be Opened On The 1st April and This is Not a Joke! ########' > /tokyo-towaaa/info/index.html && tail -f /dev/
null"]
      volumeMounts:
        - name: info-message
          mountPath: /tokyo-towaaa/info/

    - name: nginx
      image: nginx
      volumeMounts:
        - name: info-message
          mountPath: /usr/share/nginx/html

  volumes:
    - name: info-message
      emptyDir: {}
```

### Yaml command to container different ways to do it
. 1) using `args` to put all commands to run `sh -c` accepts command in one line with `&&` as well we will see it after:
```yaml
command: ["sh", "-c"]
args:
  - |
    echo "First command"
    echo "Second command"
    echo "Third command"
```

. 2) as a single line:
```yaml
command: ["sh", "-c", "echo one && echo two && echo three"]
sh -c lets you run multiple shell commands in a single string.
```

. 3) Use a shell script
```yaml
command: ["sh", "/scripts/startup.sh"]
```

### Debug command
- if issues: `kubectl debug pod/manga-kissa-abunai -it --image=busybox --target=log-mangakissa-events`
- make sure you have enabled the feature in your cluster to run sidecars:
```bash
sudo nano /var/lib/kubelet/config.yaml
# then add this:
featureGates:
  SidecarContainers: true
# then restart kubelet
sudo systemctl restart kubelet
```

## REMINDER ON PODS RESTART POLICIES VALUE EFFECTS (ChatGPT)
1. Always (default for Pods)
- Container restarts automatically on failure or exit.
- Used for long-running containers, like web servers.
- Required for Deployments, ReplicaSets, etc.

2. OnFailure
- Restart only if container exits with non-zero code (error).
- Does not restart if the container exits cleanly (exit 0).
- Used in Jobs (batch processing).

3. Never
- Never restarts, no matter how the container exits.
- Used when you want one-shot containers (manual runs, debugging).

_________________________________________________________________

# STORAGE

## Persistant Volumes (PVs)
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0003
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: slow
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    path: /tmp
    server: 172.17.0.2
```


Like when a pod is created it request cpu and memory from the node,
here the `PVs` are the resources configured by th ecluster admin and `PVCs` (persistant volume claims) are like the pods requestiong for resources to be mounted.
Different access depending on what it is wanted to be done, but not only,
also depends on the resource that is used for storage,
like some accept multiple entries while others have some different specifics:
**Important those are NOT GUARANTEED as no constraint is put by Kubernetes on those volumes**
- ReadWriteOnce (RWO): `single node` mount, can be `red and written` by all pods living on that node.
- ReadOnlyMany (ROX): this volume can be mounted as `read only` by `many modes`
- ReadWriteMany (RWX): this volume can be mounted as `read and write` by `many modes`
- ReadWriteOncePod (RWOP): this volume ensures that accros the whole cluster `only one pod` can `read and write` on the volume
source: (Check doc to see table of volumes and what access mode they support or not)[https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes]

eg: `hostPath` is supporting only `ReadWriteOnce`
and this is what we are probably going to use as we are running `kubeadm` locally
and not in the cloud (no `ebs` volumes for eg.) 

### Storage Class of PVs
`PVs` with annotation (might become deprecated) `volume.beta.kubernetes.io/storage-class`
or `storageClassName` (more actual way of doing it) would only be mounted by claims matching those
otherwise `PVs` without it would be bound to `PVCs` that do not specify storage class.

### Reclaim Policy
- Retain -- manual reclamation
- Recycle -- basic scrub (rm -rf /thevolume/*) and this only for `nfs` and `hostPath` types of volumes
- Delete -- delete the volume

### Affinity
Can be set only when using `local`(which can only be a static PV and not Dynamic one) `PVs`.

### Example `YAML` file showing those previous concepts with field defined
source: (PVs type: local)[https://kubernetes.io/docs/concepts/storage/volumes/#local]
source: (STorageClass Creation)[https://kubernetes.io/docs/concepts/storage/storage-classes/#local]

so when using `local` types of volumes we **MUST** set node affinity!
here we do use example from documentation using `local` volumes to set `volumeBindingMode` set to `WaitForFirstConsumer` in the first `yaml` part.
Other than that we could use are `hostPath` and `emptyDir`. Those could also be used for SSD/USB/FIlePath and depends on underlying system access to those.

```yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner # indicates that this StorageClass does not support automatic provisioning
volumeBindingMode: WaitForFirstConsumer

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: example-pv
# probably goes here
spec:
# here we set the capacity of this volume made available in the cluster to pods (in nodes following the defined affinity here)
  capacity:
    storage: 100Gi
# `FileStystem` if using file system otherwise use `Block` as well for hard drives probably
  volumeMode: Filesystem
# the access mode RWO
  accessModes:
  - ReadWriteOnce
# the reclaim policy type set to `Delete`
  persistentVolumeReclaimPolicy: Delete
# the `storageClassName` defined in the above `yaml`
  storageClassName: local-storage
# using here `local` which makes us then obliged to use node affinity
  local:
    path: /mnt/disks/ssd1
# here creating the node affinity constraint
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - example-node
```

### Little Schema To understand
-> StorageClass
  -> PV uses that storage class and defines volume size, access mode, reclaim policy, node affinity (if local type of volume)
    -> pods request the PV with the right storage class

**Note:** Delaying volume binding ensures that the PersistentVolumeClaim binding decision will also be evaluated with any other node constraints the Pod may have,
such as node resource requirements, node selectors, Pod affinity, and Pod anti-affinity.

### IN THE CLOUD
In the cloud it is different than locally running a Kubernetes cluster as for the `CSI` (Container Storage Interface) different drivers would be used
so need to check on the documentation and also what is possible to do and not.
It works like that:
- `CSI` driver is deployed in the Kubernetes cluster
- then from that moment the Cloud volumes would be available to be mounted and used. After depends on which ones are available
- different Cloud Providers have different settings in how many volumes MAX could be attached to a single node.
- Need also to check on that. eg: x36 `EBS` volumes for `AWS` on each node and there is an env var that can be modified to have control on that...check docs!.

### PVs Phases
those are the different states that PVs can have:
- `Available`: a free resource that is not yet bound to a claim
- `Bound`: the volume is bound to a claim
- `Released`: the claim has been deleted, but the associated storage resource is not yet reclaimed by the cluster
- `Failed`: the volume has failed its (automated) reclamation



## Persistant Volume Cliam PVCs
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 8Gi
  storageClassName: slow
  selector:
      release: "stable"
    matchExpressions:
      - {key: environment, operator: In, values: [dev]}
```

### Access modes and Volumes Types
Same as `PVs` ones

### Resources
Here is the difference as this is a `claim` the pod would request for a certain amount of resources (like a pod would do to request cpu and memory).

### Selectors
To match a set of specific volumes, volumes can be labelled and the `PVC` would use a selector like we do with pods.
It is ANDed:
- matchLabels - the volume must have a label with this value
- matchExpressions with operators: In, NotIn, Exists, and DoesNotExist

### Class 'storageClass'
a `PVC` can actually specify a storage class by name using: `storageClassName`
if `storageClassName: ""`:
- it will be set for `PV` taht do not have any storage class name
  - if `DefaultStorageClass` have been enabled in the kubernetes cluster.
    done by adding annotation: `storageclass.kubernetes.io/is-default-class:true` (will be deprecated, better to use `storageClassName`):
    - the `storageClassName: ""` request would be bounded to the default `storageClass` set by the kubernetes admin
  - if `DefaultStorageClass` is not enabled: the `PVC` would be bound to the latest `PV` created. Order is from the newest to the oldest if many.
    and those `PVs` need to also have `storageClassName: ""`.

Some other rules:
- can create `PVC` without `storageClassName` only when the `DefaultStorageClass` is not enabled.
- if no `StorageClassName` defined when creating a `PVC` and then you enable `DefaultStorageClass`, kubernetes would set `StorageClassName: ""` to those `PVCs`
- if `StorageClassName: ""` defined in `PVC` and then you enable `DefaultStorageClass`, kubernetes won't update those `PVCs` as those are fine with the right `:""`

## Namespaced or not?
- `PVs` are NOT namespaced
- `StorageClasses` are NOT namespaces
- `PVCs` are YES namespaces
```bash
# k api-resources | grep "storageclasses"
storageclasses                    sc           storage.k8s.io/v1                      false        StorageClass

# k api-resources | grep "pv"
persistentvolumeclaims            pvc          v1                                     true         PersistentVolumeClaim
persistentvolumes                 pv           v1                                     false        PersistentVolume
```

## Claim as Volume
source: (claims as volume)[https://kubernetes.io/docs/concepts/storage/persistent-volumes/#claims-as-volumes]
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim
```

## Raw Block as Volume
**Note:** Here instead of using `mountPath` on the pod `volume` we use `devicePath`
- `PV`
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: block-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  volumeMode: Block
  persistentVolumeReclaimPolicy: Retain
  fc:
    targetWWNs: ["50060e801049cfd1"]
    lun: 0
    readOnly: false

```
- `PVC`
```yaml apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: block-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Block
  resources:
    requests:
      storage: 10Gi
```
- `Pod`
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-block-volume
spec:
  containers:
    - name: fc-container
      image: fedora:26
      command: ["/bin/sh", "-c"]
      args: [ "tail -f /dev/null" ]
      volumeDevices:
        - name: data
# here we use `devicePath` instead of `mountPath`
          devicePath: /dev/xvda
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: block-pvc
```

## Enabling `--feature-gates` to make Cross-Namespace Volumes Possible [`Alpha`]
source: (cross namespace volumes)[https://kubernetes.io/docs/concepts/storage/persistent-volumes/#cross-namespace-data-sources]
Kubernetes supports cross namespace volume data sources.
To use cross namespace volume data sources, 
you must enable the AnyVolumeDataSource and CrossNamespaceVolumeDataSource feature gates for the kube-apiserver and kube-controller-manager. 
Also, you must enable the CrossNamespaceVolumeDataSource feature gate for the csi-provisioner.

Enabling the CrossNamespaceVolumeDataSource feature gate allows you to specify a namespace in the dataSourceRef field.


## Strictly binding a PVC with a PV
**Good if `PV` set `persistentVolumeReclaimPolicy: Retain`** 
source: (doc)[https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims]

- this will not strictly bind it:
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: foo-pvc
  namespace: foo
spec:
  storageClassName: "" # Empty string must be explicitly set otherwise default StorageClass will be set
  volumeName: foo-pv
  ...
```
- this would strictly bind it by reserving that `PV` to that `PVC` using `claimRef`:
Therefore, here it has to be set also on the `PV` side the `claimRef` referencing the corresponding `PVC`
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: foo-pv
spec:
  storageClassName: ""
# so here we use `clainRef` to bind the `PV` to a claim
  claimRef:
    name: foo-pvc
    namespace: foo
  ...
```
- Recap comnditions for this to work:
`PVC` -> referencing the `PV` using `volumeName`
`PV` -> referencing `PVC` using `claimRef`

## RECAP (ChatGPT)
### Dynamic Provisioning.
Here kubernetes creates the `PV` **automatically**

**Define:**
- a StorageClass
- a PVC (that refers to that StorageClass)
- and a Pod that uses the PVC
- Then Kubernetes automatically creates the PersistentVolume (PV)
- no need to pre-create a PV.


**Why create PVs manually (Static Provisioning)?**
Only create a PV manually when:
- Static Provisioning: have pre-existing storage (e.g., a mounted NFS share, disk partition, etc.) and want to bind it manually.
- Want to control which pod gets which exact volume (e.g., binding a specific disk to a specific application).
- For air-gapped clusters or restricted environments where can't use dynamic storage backends.
- For on-premise storage where the admin provisions and maintains volumes manually.


**So two options:**
|Option	|What You Define	|Who Creates the PV	|Use Case |
+-------+-----------------------+-----------------------+---------+
|Dynamic Provisioning	|StorageClass + PVC	|Kubernetes (automatically)	|Most common, easy, scalable|
+-----------------------+-----------------------+-------------------------------+---------------------------+
|Static Provisioning	|PV + PVC	|You (admin)	|Pre-provisioned disks, special use cases|
+-----------------------+---------------+---------------+----------------------------------------+

**Can a Pod bind to a specific PV?**
Indirectly, yes — by:
- Creating a PVC with:
  - the same:
    - storage size
    - accessModes
  - and optionally matching the volumeName or selector labels used by the PV

Then the PVC will bind to that specific PV.

# When using `WaitForFirstConsumer` in `StorageClass`, you need to:
**Important:**
Let's say we are in the example of some `scheduling` rules for pods that need to be in certain zones or nodes in the world.
Now the default behavior is `immediate` binding of the `PVC` to available volumes `PVs`. But this would by-pass the `scheduling` requirements.
Therefore, an issue as you won't get your workload following the rule set for the `scheduler` in the `pod`. 
This is when we use `WaitForFirstConsumer` for the `scheduler` to be taken into consideration by delaying the volume binding.
Now it is given the time to understand the zones and nodes, the resouces limits (taint/toleration, affinity and more ...) in each for each pods and the full environemnt for a good scheduling of the volumes.
So here to resume we are not by-passing topology rules and scheduler contraints.

- `StorageClass`: use of `WaitForFirstConsumer` with `allowedTopologies`
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
provisioner:  example.com/example
parameters:
  type: pd-standard
# can be `Delete` to delete `PV` automatically created when `PVC` is deleted, or `Retain` which is the default one keeping the volume created intact 
ReclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
allowedTopologies:
- matchLabelExpressions:
  - key: topology.kubernetes.io/zone
    values:
    - us-central-1a
    - us-central-1b
```
**Notes About `RetainPolicy`**: there is another `RetainPolicy` also that can be set when setting `Static` provisioning meaning that you are the one who
create the `PV`, the `PV` itself have a `PersistentVolumeReclaimPolicy` which takes precedence on the one set in the `StorageClass` so no need to set one in the `StorageClass` when creating a `Static` volume. Also it can be patched so changed. In the case of `Dynamic` so creation by `StorageClass` matching `PV` it will be taking the `StorageClass` defined `ReclaimPolicy`

- Pod: not using `nodeName` but `hostName` in selector when not using affinity
Here we can see the way to use `nodeSelector` as well
```yaml
spec:
  nodeSelector:
    kubernetes.io/hostname: kube-01
```

# some extra to know
- `StorageClass`(storageclass.yaml) is not indicating the `capacity`, but the `PVC`(pvc.yaml) would indicate `capacity` (`resources` -> `request` -> `storage`)
  and then the pod would reference that `PVC`.
  This is how the `Pod` would get request resources satisfied and volume mounted ('persistentVolumeClaim' -> `claimName`). 
  `PV` would be created automatically (`Dynamic`) by kubernetes.
- You need to install `CSI` drivers if you want to extend storage to external ones (like they do in the cloud,
  see next the example with local `S3` like volume using `Minio` which can listen to a directory path...)



# example `S3` like volume using `Minio`
What is interesting with `Minio` is that it can listen to a folder path if it is used for it's bucket path
So here the solution would  be to install a `CSI` criver for the `s3` like volume.
and then have a local `Minio` OR `external` to the cluster listening on a node folder path or internal to the cluster sharing volume with the host node and being
used as `sidecar` container.
- **Deploy the `CSI` driver**: follow installation (instructions)[https://github.com/ctrox/csi-s3] of this `csi-s3` available on `github`
- **Create StorageClass**: this `StorageClass` would be using it:
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: s3-storage
provisioner: s3.csi.k8s.io
parameters:
  bucket: my-bucket
  region: us-east-1
  mounter: rclone
  options: --s3-endpoint=https://s3.amazonaws.com
```
- **Create `PVC`**: this `PVC` would reference the `StorageClass`
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: s3-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
  storageClassName: s3-storage
```
- And after just mount the volume to your `Pod`.
- But actually this is where it is interesting as you don't even need to do that if is an `object store` like `S3` or `Minio`.
  All you need here is to use the trick of shared volumes and use local `Minio` path or `S3 SDK` or `sidecar` using `mc` (Minio Client)
Minimal eg:. might need another `sidecar` for `sync data` with `Minio` server.
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: app-with-minio-sidecar
spec:
  containers:
    - name: my-app
      image: my-app-image
      volumeMounts:
        - name: shared-data
          mountPath: /app/data
    - name: minio-sidecar
      image: minio/mc
      command: ["/bin/sh", "-c"]
      args:
        - |
          mc alias set myminio http://minio.default.svc.cluster.local:9000 minio minio123
          mc cp --recursive myminio/my-bucket /shared-data
          tail -f /dev/null
      volumeMounts:
        - name: shared-data
          mountPath: /shared-data
  volumes:
    - name: shared-data
      emptyDir: {}
```

- summary by `ChatGpt`:
Can I use it locally in kubeadm with MinIO? Yes! You can:
- Deploy MinIO as a pod.
- Use an S3 CSI driver that works with any S3-compatible storage (like MinIO).
- Or use a sidecar container that downloads/upload files to MinIO.



# Full example using locally `allowedTopologies` 

## Static Way:

- label nodes
```bash
kubectl label node node1 topology.kubernetes.io/zone=us-central-1a
kubectl label node node2 topology.kubernetes.io/zone=us-central-1b
```
- create `StorageClass`:
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard-local
provisioner: kubernetes.io/no-provisioner  # no dynamic provisioning
volumeBindingMode: WaitForFirstConsumer    # bind PV only when pod is scheduled
allowedTopologies:
  - matchLabelExpressions:
      - key: topology.kubernetes.io/zone
        values:
          - us-central-1a
          - us-central-1b
```
- create a `PV` (manually not automatic which gives more control to admin, called `static` way):
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-1
spec:
  capacity:
    storage: 2Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: standard-local
  local:
    path: /mnt/disks/ssd1  # you can `mkdir -p` this on the host manually
  nodeAffinity:
    required:
      nodeSelectorTerms:
        - matchExpressions:
            - key: topology.kubernetes.io/zone
              operator: In
              values:
                - us-central-1a
```
- create a `PVC`:
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-local-claim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
  storageClassName: standard-local
```
- create a `Pod`:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: local-storage-pod
spec:
  containers:
    - name: app
      image: nginx
      volumeMounts:
        - mountPath: /usr/share/nginx/html
          name: data
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: my-local-claim
```

## Dynamic Way

- install `CSI` driver for local provisioner from github (`rancher`)[https://github.com/rancher/local-path-provisioner]
It registers a StorageClass named local-path which can be used for dynamic provisioning
```bash
kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
```
- Change the `StorageClass` with the `CSI` driver deployed to the cluster
```
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: dynamic-local
provisioner: rancher.io/local-path  # Or any CSI driver available in your cluster
volumeBindingMode: WaitForFirstConsumer
```
- get rid of the previously created `PV` and use the `PVC` and `Pod` (can change name if wanted to):
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-claim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: dynamic-local
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-using-dynamic
spec:
  containers:
    - name: app
      image: nginx
      volumeMounts:
        - mountPath: /usr/share/nginx/html
          name: web-storage
  volumes:
    - name: web-storage
      persistentVolumeClaim:
        claimName: my-claim
```

# Storage Scenarios
- 1: create volumes `static way` with `storageClassName: ""`
     show also `claimRef` for strictly binded `PV` to `PVC`
explain:
-> StorageClass
  -> PV uses that storage class and defines volume size, access mode, reclaim policy, node affinity (if local type of volume)
    -> pods request the PV with the right storage class


- 2: create default `StorageClasss` in cluster and show behavior of `storageClassName: ""`
explain:
a `PVC` can actually specify a storage class by name using: `storageClassName`
if `storageClassName: ""`:
- it will be set for `PV` that do not have any storage class name
  - if `DefaultStorageClass` have been enabled in the kubernetes cluster.
    done by adding annotation: `storageclass.kubernetes.io/is-default-class:true` (will be deprecated, better to use `storageClassName`):
    - the `storageClassName: ""` request would be bounded to the default `storageClass` set by the kubernetes admin
  - if `DefaultStorageClass` is not enabled: the `PVC` would be bound to the latest `PV` created. Order is from the newest to the oldest if many.
    and those `PVs` need to also have `storageClassName: ""`.

`WaitForFirstConsumer` and `affinity` to show how the pods are not respecting affinity when it is not set and how it by-passes the `scheduler`
can create `PVC` without `storageClassName` only when the `DefaultStorageClass` is not enabled.
if no `StorageClassName` defined when creating a `PVC` and then you enable `DefaultStorageClass`, kubernetes would set `StorageClassName: ""` to those `PVCs`
if `StorageClassName: ""` defined in `PVC` and then you enable `DefaultStorageClass`, kubernetes won't update those `PVCs` as those are fine with the right `:""`


- 3: create `StorageClass` static with `allowedTopologies`
- 4: create `dynamic` way `StorageClass`

explain: `StorageClass`(storageclass.yaml) is not indicating the `capacity`, but the `PVC`(pvc.yaml) would indicate `capacity` (`resources` -> `request` -> `storage`)
  and then the pod would reference that `PVC`.
  This is how the `Pod` would get request resources satisfied and volume mounted ('persistentVolumeClaim' -> `claimName`).
  `PV` would be created automatically (`Dynamic`) by kubernetes.

- 5: do maybe something different to show when we use or not `volumeBindingMode: WaitForFirstConsumer` on `StorageClass` just to focus on:
    `Selector`(on `PVCs`),
    `afinity`(on `PVs` to restrict on which nodes can use this `PV`),
    `allowedTopologies`(on `StorageClass` which are not namespaced but could be used to have restriction in which nodes those classes can be used using `allowedTopologies`)
    `nodeSelector`, `affinity`, or `topologySpreadConstraints` (on `Pod` in combinaison with the other ones to have fine grained control on where scheduler lands `Pods`)
    ``
+---------------+-------+----+
| Field/Concept	| PVC	| PV |
+---------------+-------+----+
| Namespaced	| ✅ Yes| ❌ No |
+---------------+-------+-------+
| selector:	| ✅ Yes (to match PV labels)	| ❌ No |
+---------------+-------------------------------+-------+
| labels:	| ✅ Yes	| ✅ Yes |
+---------------+---------------+--------+
| nodeAffinity	| ❌ No	| ✅ Yes |
+---------------+-------+--------+
| storageClassName:	| ✅ Yes | ✅ Yes |
+-----------------------+--------+--------+

- static: here create my `PV` and introduce the use of `StorageClass` and just use the strictly binded rule and can introduce the idea of `default storage class`
- dynamic: here don't create `PV` and `useStorageClass` only and use the affinity, topology examples and show how scheduler is skipped

static > StorageClass > Dynamic > Topology > Affinity + Topology > CSI Driver (Rancher)

# capacity 
PersistentVolume and PersistentVolumeClaim definitions:

Unit	Meaning	Notes
Ki	Kibibyte (2¹⁰ bytes)	1 Ki = 1024 bytes
Mi	Mebibyte (2²⁰ bytes)	1 Mi = 1024 Ki = 1,048,576 bytes
Gi	Gibibyte (2³⁰ bytes)	1 Gi = 1024 Mi = 1,073,741,824 bytes
Ti	Tebibyte (2⁴⁰ bytes)	Rarely used in small clusters
Pi	Pebibyte (2⁵⁰ bytes)	Very large storage
Only binary SI units (with i) like Gi, Mi, Ki are supported and recommended in kubernetes `yaml`.

# scenario 1 (Static)
here we have affinity set on the `PV` and at the same time in the `StorageClass` we will set the `volumeBindingMode: WaitForFirstConsumer` 
so that scheduler will have the responsibility to check on nodes available and not be bypassed

- `cat storage-class-waitforfirstconsumer.yaml`
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
# indicates that this StorageClass does not support automatic provisioning
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
```

- `cat pv-local.yaml` 
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-with-mandatory-affinity-set
spec:
  capacity:
    storage: 512Ki
  # use `Block` for SSDs for eg:.
  volumeMode: Filesystem
  accessModes:
    # RWO: `single node` mount, can be `red and written` by all pods living on that node
    - ReadWriteOnce
  # `Delete/Retain also Recycle but be carefull as it uses `rm -rf` and is only for `nfs` and `hostPath``
  persistentVolumeReclaimPolicy: Delete
  # play with this field to show behavior of by-passing scheduler and also another of `DefaultStorageClass`
  # storageClassName: ""
  storageClassName: local-storage
  # using here `local` which makes us then obliged to use node affinity
  local:
    # this path need to be created manually on node
    path: /tmp/local-pv
  # here creating the node affinity constraint
  nodeAffinity:
    # required OR preferred
    required:
      nodeSelectorTerms:
      - matchExpressions:
        # kubernetes ones
        #- key: kubernetes.io/hostname
        # custom
        - key: location
          operator: In
          values:
          - shizuoka
```

- `cat pvc-without-selector.yaml`
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-scheduled-in-node-affinity-defined-by-pv-affinity
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 512Ki
  storageClassName: local-storage
```

- `cat pod-requesting-storage.yaml` 
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-needing-storage
spec:
  containers:
    - name: i-need-storage-pod
      image: nginx
      volumeMounts:
      # here using mountpath
      - mountPath: "/tmp/local-pv"
        name: my-local-storage
  volumes:
    - name: my-local-storage
      persistentVolumeClaim:
        claimName: pvc-scheduled-in-node-affinity-defined-by-pv-affinity
```

- one `node2`: 
```bash
mkdir /tmpt/local-pv
````
- label `node2`:
```bash
k label node node2.creditzens.net lcoation=shizuoka
```
- make sure the node is labelled acording to the affinity of the pod and then apply `StorageClass`, `PV`, `PVC` and `Pod` to the cluster
```bash
k apply -f <filename>
... # do the same will all of those files
```
- here normally the pod will be scheduled in node2 which has the label corresponding to the affinity of the `pv`
```bash
 k get pods -o wide
NAME                  READY   STATUS    RESTARTS   AGE    IP               NODE                    NOMINATED NODE   READINESS GATES
pod-needing-storage   1/1     Running   0          153m   172.16.210.124   node2.creditizens.net   <none>           <none>
```
- then `exec` in `pod` to create a file with content at the volume location `/tmp/local-pv`
```bash
k exec -it pod-needing-storage -- bash
root@pod-needing-storage:/# echo "junko shibuya" > /tmp/local-pv/junko-location-now.txt
```
- now go in `node2` and you will see the file and its content in the `pv` volume located at `/tmp/local-pv`
```bash
creditizens@node2:~$ cat /tmp/local-pv/junko-location-now.txt 
junko shibuya
```
**Note:**:
- the `PV` and `PVC` would still reference to each others and be bound even if pod is deleted, so they need to be delete separately and manually
-  the volume on the `node2` is not deleted by kubernetes as it is made to persist if there is `pod` failure. so need also to be deleted manually.


# Scenario 2 (From Static to Dynamic):

Here will be showing:
- how pv bind to storageclasses: using `storageClassName: ""` and `# storageClassName: ` not set
- how we need a provisioner and install rancher or show how to install it, then use dynamic provisioning showing no need to create `PV`
- after can maybe show more Topology and how to use it
- after move to `Dynamic` fully and now no more `affinity` to control where pod would be deployed, no more control but use `Topology` on `StorageClass`
  to show that this is a way to control where those pods would be deployed.
always show where is the volume created as with `rancher` `local-path` provisioner we don't get it at the path of `PV` as here it is dynamic and 
we don't create `PVs` so the provisioner will be putting the volumes at `/opt/local-path-provisioner/pvc-<numbers>...`.
So need here to describe the `PV` created automatically by the provisioner.

### install `local-path` provisioner (Rancher)
```bash
kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
```

### patch it as `default`
```bash
kubectl patch storageclass local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
```

### use `storageClassName: local-path`
in your created `PVC` use `storageClassName: local-path`

- `PVC`s
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-following-node-affinity-defined-in-pv-with-or-without-storageclass
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 512Ki
  # if no storageClassName defined kubernetes assumes `Dynamic` type of provisioning so need to change the provisioner on StorageClass
  # storageClassName: ""
  # this is the rancher Dynamic provisioner
  storageClassName: local-path

  # `selector` can be defined with `matchLabels` and `matchExpressions`
```

- `PV`s
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-with-mandatory-affinity-set
spec:
  capacity:
    storage: 512Ki
  # use `Block` for SSDs for eg:.
  volumeMode: Filesystem
  accessModes:
    # RWO: `single node` mount, can be `red and written` by all pods living on that node
    - ReadWriteOnce
  # `Delete/Retain also Recycle but be carefull as it uses `rm -rf` and is only for `nfs` and `hostPath``
  persistentVolumeReclaimPolicy: Delete
  # play with this field to show behavior of by-passing scheduler and also another of `DefaultStorageClass`
  #storageClassName: ""
  storageClassName: local-storage
  # using here `local` which makes us then obliged to use node affinity
  local:
    # this path need to be created manually on node
    path: /tmp/local-pv
  # here creating the node affinity constraint
  nodeAffinity:
    # required OR preferred
    required:
      nodeSelectorTerms:
      - matchExpressions:
        # custom
        - key: location
          operator: Exists
```
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-2-with-mandatory-affinity-set
spec:
  capacity:
    storage: 512Ki
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  #storageClassName: ""
  # storageClassName: local-storage
  local:
    path: /tmp/local-pv
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        # custom
        - key: location
          operator: Exists
```
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-3-with-mandatory-affinity-set
spec:
  capacity:
    storage: 512Ki
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: ""
  # storageClassName: local-storage
  local:
    path: /tmp/local-pv
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        # custom
        - key: location
          operator: Exists
```

- `StorageClass`es
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
# indicates that this StorageClass does not support automatic provisioning
provisioner: kubernetes.io/no-provisioner
#volumeBindingMode: WaitForFirstConsumer
```
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage2
provisioner: kubernetes.io/no-provisioner
#volumeBindingMode: WaitForFirstConsumer
```
A way to create a default `StorageClass` using the annotation but will not work on creating volume for the `pod` to consume
will just bind to `pvc` as it is expecting `dynamic` provisioning so need to install a dynamic provisioner
so here could use the `rancher` provisioner `local-path`
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage3-with-annotation-default
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
# indicates that this StorageClass does not support automatic provisioning
provisioner: kubernetes.io/no-provisioner
#volumeBindingMode: WaitForFirstConsumer
```
 This is how to install the `StorageClass` from `Rancher` `local-path` and then patch it to become `default` but can create more than one after that
so manually create `StorageClasses` using `yaml` file and with different names but same `provisioner: local-path`
```yaml
# Rancher Local Path Provisioner (storageclass, deployment, configmap)
# kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
# kubectl patch storageclass local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
```

___________________________________________________________________________________________

# Backup & Restore
source: (kubernetes doc on etcd backup and restore)[https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/]
- **Little resume:**
When we talk about backup and restore is all about ETCD. 
We need to go inside the pod which is present in the namespace `kube-system`.
Then do the work of creating snapshots but it stays inside the pod. Which is actually sharing same path with the underlying node filesystem.
Therefore, can be accessed at `/var/lib/etcd/` which has been defined as volume in `/etc/kubernetes/manifests/etcd.yaml`.

**Why no StorageClass, PVC and PV?**
Because:
- Needs to start before Kubernetes APIs and controllers are even online
- Must not depend on the Kubernetes scheduler or provisioners
- Is managed as a static pod (host-managed, not API-managed)

So here we can see that `etcd` is not using the same process and not passing by the API server.

- ckech `etcd` version
```yaml
kubectl exec -n kube-system etcd-controller.creditizens.net -- etcdctl version
etcdctl version: 3.5.15
API version: 3.5
```

## ETCD Backup

- go inside the `etcd pod` and make a snapshot. No matter how many controller in the cluster all `etcd` present in each `controller` nodes would be saved
  in that snapshot.
  So here in the node file path the backup snapshot would be in the folder `/var/lib/etcd/`.

**Command running it from the node terminal, running it inside the `etcd` pod**
```yaml
kubectl exec -n kube-system etcd-<your-controller-node-name> -- \
  etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt \
          --cert=/etc/kubernetes/pki/etcd/server.crt \
          --key=/etc/kubernetes/pki/etcd/server.key \
          snapshot save /var/lib/etcd/backup.db
```

**Command running it from the node terminal, outside this time of the `etcd` pod so just terminal to snapshop directly**
Here `ETCDCTL_API=3` is telling which version of `etcd` API we are running on the cluster (here since vercion etcd 3.5)
need to install the binary `etcdctl` so have it available to run the command:
```bash
# get your version of `etcd` or check online which `etcd` version matches your `kubernetes` version: https://kubernetes.io/releases/version-skew-policy/#etcd
kubectl exec -n kube-system etcd-controller.creditizens.net -- etcdctl version
etcdctl version: 3.5.15
API version: 3.5
# then use that version number to get the release, if not available go to their `github and check versions available in the releases`: https://github.com/etcd-io/etcd/releases
curl -LO https://github.com/etcd-io/etcd/releases/download/v3.5.15/etcd-v3.5.15-linux-amd64.tar.gz
tar -xzvf etcd-v3.5.15-linux-amd64.tar.gz
sudo mv etcd-v3.5.15-linux-amd64/etcdctl /usr/local/bin/
```
```bash
sudo ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /var/lib/etcd/backup.db
```
- accessory command if can't ssh to one of the controller nodes using `kubectl cp` command to copy the snapshot to another node file system path.
```yaml
# Copy from pod to controller node
kubectl cp kube-system/etcd-<your-node-name>:/var/lib/etcd/backup.db ./backup.db
```

- can inspect the metadata of the backup
The backup is not human readable as it is a binary file (type LevelDB)
```bash
ETCDCTL_API=3 etcdctl snapshot status backup.db
```

- Options to encrypt the `etcd` snapshot
It can't be encrypted but ....
  - can encrypt the data saved to it at rest meaning inside the cluster when saving to `etcd` all would be saved but in an encrypted form
    Kuberneted permits to save at rest using an encryption file: `/etc/kubernetes/encryption-config.yaml`
    Used for encrypting Secrets, ConfigMaps, etc., before writing to etcd.
  - encrypt after having made the snapshot using: 
    - `OpenSSL`:
      ```bash
      # encrypt
      openssl enc -aes-256-cbc -salt -in backup.db -out backup.db.enc
      # decrypt
      openssl enc -d -aes-256-cbc -in backup.db.enc -out backup.db
      ```
    - `GPG`:
      ```bash
      # encrypt
      gpg -c backup.db
      # decrypt:
      ```bash
      gpg -d backup.db.gpg > backup.db
      ```
    - `s3` or `Vault`:
      so here moving the snapshot to s3 and enabling AES or KMS encryption
      or using Hashicorp Vault and saving it there.

`OR`
use `etcdutl`:
(it is installed with the binary when you install the `etcdctl` binary)
- Encrypt snapshot: The encryption key must be a symmetric key (usually 32 chars)
```bash
etcdutl snapshot encrypt --input backup.db --output backup.encrypted.db --key myencryptionkey_very_long_string
```
- Decrypt snapshot (key must be a symmetric key (usually 32 chars))
```bash
etcdutl snapshot decrypt --input backup.encrypted.db --output backup.db --key myencryptionkey_very_long_string
```
- Inspect metadata
```bash
etcdutl snapshot status backup.db
```

### Can we get a snapshot and use it in a completely new cluster different from the one where the snapshot has been made?
Yes but... Headacke as, all thsoe components must match:
- Kubernetes version: strongly recommended
- etcd version: Match snapshot format
- Cluster name: Specified in kubeadm config
- API Server certs (SANs): Needed for consistent identity
- CNI plugin (e.g., Calico, Cilium): Recommended Some network state is in etcd


### What won’t be restored from the snapshot?
- Kubelet certificates
- Kubeconfig files
- Filesystem volumes
- Kubeadm configuration (stored outside etcd)


# Restore
**Important Node:** 
Restoration of `etcd` will create downtime in the cluster as the `controller` is not working if `etcd` is not there. 
But this will afftect only `controller` nodes, the `worker` ndoes and their workload wouldn't be affected.
So pods would still run normally and `calico` as well and networking as well as `ingress`.
Just can't :
  - use any `kubectl` commands
  - `kubeadm` commands
  - schedule new pods
  - `HPA` (autoscaling), `Jobs` as relying on `APIserver` wont works
so still a risk of issues with pods failing. but it is fine.i


- make sure that the APIserver is not running and that the `volumes`, `hostpath` and `--data-dir` folders match where the snapshot is
  so here can mv all the manifest like `/etc/kubernetes/manifests/etcd.yaml` to another folder to restart the api server or just change one config in it and it will restart, then bring it back to the `/etc/kubernetes/manifests/` folder or change the config that you have changed before back to it normal state.
  all this are tricks to restart APIserver
- stop `kubelet` service
- **Note:** The documentation is suggesting that some features of `etcdctl` might become deprecated like the one to check `status` which will be deprecated
  so just change commands by replacing `etcdctl` by `etcdutl` for `status` check and for restoration of `.db` files.
  eg.:`etcdutl --write-out=table snapshot status snapshot.db `
```bash
etcdutl --data-dir <data-dir-location> snapshot restore snapshot.db
```
so here `--data-dir` will be the directory that will be created during the restore process (from doc info)
if no validation `hash` present as snapshot created from a specific directory different from the `snapshot save` one, can run the command with option `--skip-hash-check` has normal `snapshot restore` only would check for integrity using that `hash` but sometimes it is not present (like for restore to new cluster for example). See doc of `etcd` for that: (doc etcd)[https://etcd.io/docs/v3.6/op-guide/recovery/#restoring-a-cluster]
(eg. with more options:`etcdutl snapshot restore snapshot.db --bump-revision 1000000000 --mark-compacted --data-dir output-dir`)
- restarts APIserver

**Important:**
The folder `/var/lib/etcd/` is having a `member` folder and that is where `etcd` is having it's logs (transaction history) in the form of `.wal` files and their conterpart `.snap` files which is `etcd`'s internal snapshot and where the `.db` file is the actual `snapshot` like your `backup.db`.
But, when we restore, we need to delete everything from the `var/lib/etcd/` folder to not get an error like `member` already exist. so get rid of the full `member` folder or back it up.
Your snapshot can live anywhere in the file system (AWS S3 as well as Hashicorp Vault) but the `--data-dir` should be same as in the `/etc/kubernetes/manifests/etcd.yaml` file so here `/avr/lib/etcd/` where a new member will be created when restoring.
so let's review the command:
```bash
sudo etcdutl snapshot restore /my/custom/path/location/my_backup.db --data-dir /var/lib/etcd 
```

### Full restore example
- prerequisite: have `etcdctl` or `etcdutl` installed
- your `backup.db` binary file somewhere in your `Filesystem`, `Hashicorp Vault` or `AWS s3`
- `/etc/kubernetes/manifests/etcd.yaml` file present


.1 Stop Kubelet
# stop kubelet because otherwise it will keep runnign and recreate the `member` folder that we want to get rid of to create a new one on snapshot restoration 
```bash
sudo systemctl stop kubelet
```

.2 Stop all component of controller by moving the `.yaml` files from `/etc/kubernetes/manifests/` folder:
`Kubelet` will see that `etcd.yaml` stopped and the `controller` node will stop
```bash
# example with `etcd.yaml` but do it for all other components as well `sheduler`, `apiserver`, `controller manager`
sudo mv /etc/kubernetes/manifests/etcd.yaml /etc/kubernetes/manifests/etcd.yaml.bak
```

.3 Restore
```bash
# or sudo mv /var/lib/etcd/member /var/lib/etcd/BAK_member
sudo rm -rf /var/lib/etcd/*
sudo ETCDCTL_API=3 etcdutl snapshot restore /var/lib/etcd/backup.db --data-dir /var/lib/etcd-restore  --bump-revision=1000000000 --mark-compacted
```
- `/var/lib/etcd/backup.db`: full path to the existing snapshot file
- `--data-dir` `/var/lib/etcd-restore`: folder where etcd will reconstruct the DB.
  The folder you indicate here will be created if it doesn't exist, no need manual creation beforehands.
  But it is not the default folder so need to update `/etc/kubernetes/manifests/etcd.yaml` file `volumes/hostPath, --data-dir`
  Otherwise just use the default folder but delete all from `/var/lib/etcd/` or save it somewhere else. as the restore will create a `member` folder
  and you will get an error saying that `a member already exist`

.4 Update the `etcd.yaml` file's dir path to look at (only if needed)
```bash
sudo nano /etc/kubernetes/manifests/etcd.yaml.back
```
.4' **This is optional, do it just if you want to use another folder for `--data-dir` other than the default one `/var/lib/etcd/`**
Look for the --data-dir flag and change:
```yaml
--data-dir=/var/lib/etcd
```
to:
```yaml
--data-dir=/var/lib/etcd-restore
```
Also update the hostPath volume mount if needed:
```yaml
- name: etcd-data
  hostPath:
    path: /var/lib/etcd-restore
    type: DirectoryOrCreate # this would create the folder automatically by `kubelet` so no need human to go and have it created beforehands, but need to be deleted manually as it offers a way of having a persistent volume as when the pod dies the volume and data stays on the node. so becareful as well to not have node space storage taken by forgotten test volumes or other (bash script for clean up or ansible can be good and use standardisation of which path is used so that that path can be discovered and content wipped up without any issues)
```

.5 now change the backup name back to its original for the `etcd.yaml` and all other `.yaml` files in `/etc/kubernetes/manifests/` that when restarteing kubelet it would pick it up with new config and restart
```bash
sudo mv /etc/kubernetes/manifests/etcd.yaml.back /etc/kubernetes/manifests/etcd.yaml
```

-6 restart kubelet
# restart kubelet
```bash
sudo systemctl restart kubelet
```

-7 Enjoy the cluster with restored backup
```bash
kubectl get pods -n kube-system
```        


## Backup & Restore Scenarios:
- before...
Create an `nginx` deployment with config map that show a message and updates the `index.html` file and use it for each example with a new message.
- 1) where we make backup from going inside the pod and showing where in the `/etc/kubernetes/manifests/etcd.yaml` file we find the volume path.
     say that it is a binary and that it is not encrypted. use `opensssl` way to encrypt it. OR say that there is a more complicated way to do it at `Rest` so store data to `etcd` but encrypt it before storage. Make sure the cluster state have an nginx showing a certain message
```bash
kubectl exec -n kube-system etcd-controller.creditizens.net -- \
  etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt \
          --cert=/etc/kubernetes/pki/etcd/server.crt \
          --key=/etc/kubernetes/pki/etcd/server.key \
          snapshot save /var/lib/etcd/backup.db
```
- 2) scenario in which we will do backup from outside telling that documentation is suggesting us to use `etcdctl`.
     we install it and make a snapshot from outside the cluster. We also show that we can encrypt it using `etcdctl`.
     make sure thet the state of the cluster has changed and nginx showing another message.
     Note: `etcd` and `etcdutl` binaries would be also installed during the process of installation of `etcdctl`. they are shipped together.
source: (github etcdctl)[https://github.com/etcd-io/etcd/tree/main]
```bash
# installing `etcdctl` will make `etcdutl` available as binary as well
curl -LO https://github.com/etcd-io/etcd/releases/download/v3.5.15/etcd-v3.5.15-linux-amd64.tar.gz
tar xzvf etcd-v3.5.15-linux-amd64.tar.gz
sudo mv etcd-v3.5.15-linux-amd64/etcdctl /usr/local/bin/
```
```bash
sudo ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /var/lib/etcd/backup.db
```
```bash
# encrypt
openssl enc -aes-256-cbc -salt -in backup.db -out backup.db.enc
# decrypt
openssl enc -d -aes-256-cbc -in backup.db.enc -out backup.db
```

- 3) now make restore using one snapshot and the other. so need to decrypt  and then make the snapshot.
     Say that storage for the snapshot could be AWS S3 with encryption AES or KMS. OR use Hashicorp Vault.
```bash
# this would stop kubelet so the cluster workload would be fine but not the controller node anymore as it detects the manifest presence or not
# not `etcd.yaml` would stop the `apiserver` and `kubelet`
sudo mv /etc/kubernetes/manifests/etcd.yaml /root/etcd.yaml.bak  # or just temporarily rename it to stop the static pod
# stop kubelet because otherwise it will keep runnign and recreate the `member` folder that we want to get rid of to create a new one on snapshot restoration 
sudo systemctl stop kubelet
sudo rm -rf /var/lib/etcd/*  # Clean the old data
# Now restore
sudo etcdutl snapshot restore /path/to/backup.db --data-dir /var/lib/etcd  --bump-revision=1000000000 --mark-compacted
# Put the etcd manifest back
sudo mv /root/etcd.yaml.bak /etc/kubernetes/manifests/etcd.yaml
# restart kubelet
sudo systemctl restart kubelet
```

# fixing restoration issues
The restoration went fine but when trying to exec in pod i got an error: 
```bash
k exec -it nginx-busan-574d67fb55-f427c -- bash cat /usr/share/nginx/index.html
Outputs:
error: unable to upgrade connection: pod does not exist
```
use `crictl` to check on the container runtime and saw that there were an error:
```bash
sudo crictl ps -a | grep nginx-busan
Outputs:
WARN[0000] runtime connect using default endpoints: [unix:///var/run/dockershim.sock unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/c
ri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead. 
ERRO[0000] validate service connection: validate CRI v1 runtime API for endpoint "unix:///var/run/dockershim.sock": rpc error: code = Unavailable desc = connection erro
r: desc = "transport: Error while dialing: dial unix /var/run/dockershim.sock: connect: no such file or directory" 
WARN[0000] image connect using default endpoints: [unix:///var/run/dockershim.sock unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri
-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead. 
ERRO[0000] validate service connection: validate CRI v1 image API for endpoint "unix:///var/run/dockershim.sock": rpc error: code = Unavailable desc = connection error:
 desc = "transport: Error while dialing: dial unix /var/run/dockershim.sock: connect: no such file or directory"
```

So here the issue might be caused because have restarted kubelet before mv back the manifest `etcd.yaml` file, which might have overwrittern the `member` so altered the configs...
so will restart a new restoration from scratch again following the right steps..

- so after search have seen that there could be an issue when the `/var/lib/etcd/member/snap/` number is older than what the cluster is expecting so having it always increasing , so when restoring need to use some flags for that to be done artificially: `--bump-revision and --mark-compacted`
source: (etcd doc about incrementation of numebrs)[https://etcd.io/docs/v3.5/op-guide/recovery/#:~:text=In%20the%20context%20of%20Kubernetes,effectively%20invalidating%20its%20informer%20caches]
- also if several controllers all need to be stopped when doing a `etcd` restoration, see (doc)[https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#:~:text=If%20any%20API%20servers%20are,these%20steps%20to%20restore%20etcd]

more general docon restoration from `etcd`: (kubernetes restoration doc)[https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#:~:text=]

- so need to use command to restore: 
get revision number of your snapshot so that you know what incremental number you can use in your restoration command:
eg.:
```bash
sudo ETCDCTL_API=3 etcdutl snapshot status snapshots/busan-creditizens-home.db
Outputs:
80ae79f1, 615876, 1618, 12 MB
sudo ETCDCTL_API=3 etcdutl snapshot status snapshots/hawaii-creditizens-house.db 
Outputs:
27402943, 620774, 1151, 12 MB
```
`--mark-compacted` ensures that any attempt to read older revision data (like what old controllers cached) is rejected, forcing them to re-list everything.
```bash
sudo etcdutl snapshot restore snapshot.db --data-dir=/var/lib/etcd \ 
    --bump-revision=1000000000 --mark-compacted
```
fter this fix so adding those flags `--bump-revision` bigger than last snapshot and `--mark-compacted` for history to restart form there and not look at state data cached or other from any components of the controller (scheduler, apiserver, controlle manager)

**Important Point to Consider to Not Get Errors In Restoration**
**- make sure all control plane are down to avoid state issues with one control plane having still to reference to older journal logs of state**
**- so stop `kubelet` and mv `etcd.yaml` manifest on all of those**
**- use the `--bump-revision` number flag and the other flag `--mark-compacted` to compact whatever is lower so that the cluster initializes state fromt he snapshot point**
**- if other resources have been created since the snapshot has been done, need to reuse those yaml files to get it back so you get the desire state**
**- if having issues with `tokens`, get rid of `secrets` which will be recreated**
**- make sure to copy the restored /var/lib/etcd/member/ in all other `controller` nodes using `ssh` or other means.**
**- after on all nodes mv back the `etcd.yaml` manifest file and restart `kubelet`
**- the issue is not from using `etcdutl` or `etcdctl` for restoration, you can use `etcdctl` for snapshot creation and the other `etcdutl` for restoration. The issue is more due to other components of the cluster having state of the older cluster. so mistmatch between snapshot state and some controller plane components ones. = `stale watches` issues.**

_________________________________________________________________

# Resource Limits

soource: (doc resources limites)[https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/]

`cpu` and `memory` limits can be set at the `container` level, the `pod` level (not in the version I am using now 1.28 but in the latest 1.32 yes and will be updating the cluster later to it using a rust coded custom app that upgrades kubernetes cluster just by providing kubelet and kubeadm and containerd desired versions). Also it can be set at `namespace` level.


## Limits, Requests, Hugepages, EmptyDir, Ephemeral-Storage

### Limits
- `limits`: is the maximum that the resource can consume and it is a:
  - `cpu`: Hard limit (`cpu` throttling)
  - `memory`: Not hard limit **but when resource consume too much there is a `OOM` (Out Of Memory) kill process that is done in the `kernel`**
              Therefore, here a container can use more than its limit and the kernel can kill it if there is memory pressure.

### Requests
- `requests`: is the minimum that the resource is consuming.

### Hugepages
- `hugepages-2Mi`: can also be used but alone so not in conjunction with `cpu` and `memory` and this would need to be customized as sytem default `pages`
                   are 4Ki and you might want to use that because you want better performance and have bigger pages so that data is processed in bigger chunks.
                   `hugepages` are a separate pool of memory reserverd at the `os` level.

- reduces `cpu` usage with those bigger chunks. here it from default `4Ki` pages, page would be `2Mi` chunks and there is also another one `hugepages-1Gi` (bigger chunks, but we are not going to use it here).
From ChatGPT: `Reducing Translation Lookaside Buffer (TLB) misses. Reducing CPU cycles wasted in virtual-to-physical memory mapping` 
  - typically used only for specific workload: `databases`, `in-memory caches like Redis`, `virtual machines`, `AI models`, or `networking software`.
  - eg. This tells the scheduler: "Schedule this pod on a node that has 512Mi worth of 2Mi hugepages."
  ```yaml
  requests:
    hugepages-2Mi: 512Mi
  limits:
    hugepages-2Mi: 512Mi
  ```

- if you want to use `hugepages` **with** `cpu` or `memory` or both, you need to have another container in your spec. that would be dedicated to limit the pod usage of those resources, so no effect ont hat specific container but would be taken into account by the sceduler when performing calculation of resources limits totals and policies enforcements on resources usage in `namespace` for eg. here. 

- need also to **change the settings** of `GRUB` in the node itself adding line in config file:
  ```bash
  # append to `GRUB_CMDLINE_LINUX`: `default_hugepagesz=2M hugepagesz=2M hugepages=512`
  # This allocates 512 x 2MB = 1GB of hugepage memory at boot.
  default_hugepagesz=2M hugepagesz=2M hugepages=512
  ```

- check this example where it is not set as custom:
```bash
cat /proc/meminfo

Outputs:
MemTotal:        3961464 kB
MemFree:          406324 kB
MemAvailable:    2053944 kB
Buffers:           56092 kB
Cached:          1795716 kB
SwapCached:            0 kB
Active:          2003880 kB
Inactive:         963176 kB
Active(anon):    1021784 kB
Inactive(anon):   133900 kB
Active(file):     982096 kB
Inactive(file):   829276 kB
Unevictable:           0 kB
Mlocked:               0 kB
SwapTotal:             0 kB
SwapFree:              0 kB
Zswap:                 0 kB
Zswapped:              0 kB
Dirty:                72 kB
Writeback:             0 kB
AnonPages:       1115248 kB
Mapped:           826060 kB
Shmem:             40436 kB
KReclaimable:      81552 kB
Slab:             254520 kB
SReclaimable:      81552 kB
SUnreclaim:       172968 kB
KernelStack:       15584 kB
PageTables:        25608 kB
SecPageTables:         0 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:     1980732 kB
Committed_AS:    6044628 kB
VmallocTotal:   34359738367 kB
VmallocUsed:       37396 kB
VmallocChunk:          0 kB
Percpu:           118272 kB
HardwareCorrupted:     0 kB
AnonHugePages:         0 kB
ShmemHugePages:        0 kB
ShmemPmdMapped:        0 kB
FileHugePages:         0 kB
FilePmdMapped:         0 kB
Unaccepted:            0 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
Hugetlb:               0 kB
DirectMap4k:      169792 kB
DirectMap2M:     4024320 kB
DirectMap1G:     2097152 kB
```

- From ChatGPT, advantages of `hugepages`:
```bash
Using hugepages (2MB or 1GB):
- Fewer pages = fewer entries in the page table
- Less overhead in TLB/cache misses
- Better memory access speed and CPU efficiency
```

- how to setup custom HugePages on the linux node:
Edit the kernel boot parameters (via GRUB):
```bash
sudo nano /etc/default/grub
```
Add this to `GRUB_CMDLINE_LINUX` to reserve 512 `hugepages` of 2MB each = 1GB of `RAM` 
```bash
default_hugepagesz=2M hugepagesz=2M hugepages=512
```
Then update GRUB and reboot:
```bash
sudo update-grub
sudo reboot
```
Verify the Node Reservation after reboot:
```bash
grep Huge /proc/meminfo
```
rollback:
```bash
sudo nano /etc/default/grub
# Remove or comment:
# default_hugepagesz=2M hugepagesz=2M hugepages=512
```
Re-run:
```bash
sudo update-grub
sudo reboot
```

- `yaml` file example for one pod:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: hugepages-demo
spec:
  containers:
  - name: app
    image: busybox
    command: ["sleep", "3600"]
    resources:
      limits:
        # 128 x 2Mi hugepages = 256Mi
        hugepages-2Mi: 256Mi
  restartPolicy: Never
```

### EmptyDir
- The use of `emptyDir` in `volumes` to share data between pods could be problematic if not controlled as there is no cap in how much resource it would use.
  on the `node`. Therefore, we need to control that with some limits.

- `ephemeral-storage` way:
`emptyDir` is under the hood using `ephemeral-storage` so we can play with that resource to limit its usage.(resume)

Volumes can be a local path or can be also in `RAM` so the volume is there but `ephemeral`. We will see `ephemeral-storage` after for the resource limitation of it but can use actualy that to limit the usage of `emptyDir` resources as the pod would be killed by kubernetes if it uses more resources than the limits allowed one the `ephemeral- storage` resource. (`cpu`, `memory`, `ephemeral-storage` are all also resources, keep this in mind)
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: limit-ephemeral
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c", "dd if=/dev/zero of=/data/file bs=1M count=500"]
    volumeMounts:
    - mountPath: /data
      name: cache-volume
    resources:
      requests:
        ephemeral-storage: "200Mi"
      limits:
        # this would be used as limit for the `volumes.emptyDir`
        ephemeral-storage: "500Mi"
  volumes:
  - name: cache-volume
    # now this volume is limited to the `spec.containers[0].resources.limits.ephemeral-storage` of 500Mi, more use would kill the pod
    # after it depends on its restart policy or type of pod...
    emptyDir: {}
````

- `sizeLimit` way:
**Works only when `ephemeral-storage` limit is set OR `medium` is used otherwise it is ignored if no `medium` is used and no `ephemeral-storage` is used.**
We can also set a size limit on the `emptyDir` `volumes`
In the next example we bind this `emptyDir` path to the `RAM` using `medium: Memory` option. So it will be here limited to `sizeLimit: 64Mi`.
If we don't use the option `medium: Memory` (`RAM`-based temporary volume) it will be just using the underlying physical filesystem so `disk` or `ssd` depend on the path indicated and node connected resources made available.
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: ram-cache
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c", "echo Hello > /cache/hello && sleep 3600"]
    volumeMounts:
    - name: ram-vol
      mountPath: /cache
  volumes:
  - name: ram-vol
    emptyDir:
      # this option would be using in-`RAM` `Memory` for this `ephemeral-storage` (udner the hood)
      medium: Memory
      sizeLimit: 64Mi
```

### Ephemeral-Storage
This is what is used under the hood by `emptyDir`, `logs`, `image layers` and more...
On `ephemeral-storage` can be set `limits` and `requests` and also what is nice here is that it **CAN** be used in combinaison of `cpu` and `memory` resources `limits/requests`
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: full-limits-example
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c", "echo Hello Junko && sleep 3600"]
    volumeMounts:
    - mountPath: /data
      name:  some-volume-normally-not-limited-but-here-limites-by-epheremal-storage
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
        ephemeral-storage: "100Mi"
      limits:
        cpu: "500m"
        memory: "256Mi"
        ephemeral-storage: "500Mi"
  volumes:
  - name: some-volume-normally-not-limited-but-here-limites-by-epheremal-storage
    emptyDir: {}
```

## Kubernetes Units For Calculating `RAM` and `CPU`

### Units Used For Resource Limits Indication: Base-10, Base-2 for RAM (memory)
We have two different ways that it is calculated depending on if it is `decimal` base-10, based calculation of `bytes`, or `binary` base-2 (more precise), based calculations.
So here have to understand that to calculate `RAM` (memory) we can use different ways, one being more precise thant the other (base-2 binary more precise (`i`)).
They both use different units.
- Decimal units	10	-> 1M = 1,000,000 -> expressed: k, M, G, etc.
- Binary units	2	1Mi = 1,048,576 -> expressed: Ki, Mi, Gi, etc.

Using the documentation example, let's explain how it is calculated:
```bash
128974848  
129e6  
129M  
123Mi
```
- 128974848 (`raw bytes`)
This is already in bytes.
```bash
128,974,848 bytes
```
- 129e6 (scientific notation = 129 million `decimal`)
```bash
129e6 = 129 * 10^6 = 129,000,000 bytes
```
- 129M (129 megabytes in base 10 `decimal`)
```bash
129M = 129 * 1,000,000 = 129,000,000 bytes
```
- 123Mi (123 mebibytes in base 2 `binary`)
```bash
123Mi = 123 * 1,048,576 = 128,974,848 bytes
# got that follwoing this logic:
1 MiB = 1024 KiB
1 KiB = 1024 bytes
# So,
1 MiB = 1024 × 1024 bytes = 1,048,576 bytes
```
Exactly equals the raw byte value: 128974848

### Unit used for resource calculation `CPU` this time:
Here we will just use `millicores`:
- cpu: 500m = 0.5 vCPU -> Half-core
- cpu: 1 = 1 vCPU -> full 1 core
**Never use Mi, Gi, M, or G with CPU** like we did for `RAM`

### Example for both
```yaml
# CPU Example (1 full CPU core)
resources:
  requests:
    cpu: "1"
  limits:
    cpu: "2"

# Memory Example (binary-based)
resources:
  requests:
    memory: "512Mi"
  limits:
    memory: "1Gi"

# Ephemeral Storage Example (decimal-based)
resources:
  limits:
    ephemeral-storage: "1G"

```


## Limit Ranges & Resource Quotas
source: (Doc for limit ranges)[https://kubernetes.io/docs/concepts/policy/limit-range/]

### LimitRange
It is a `namespaced` resource.
```bash
k api-resources | grep "limitranges"
limitranges                       limits       v1                                     true         LimitRange
```

- `LimitRange`:
  - Per-container or per-pod **`default`** `request/limit` values.
  - It does **not enforce a total cap** on the namespace.
  - it is going to **enforce** the `limits` set to it at `pod` `admission` stage and not on `running` `pods`.
  - there can be more than one `LimitRange` resource deployed to same namespace we don't know which will be used as `default`

So here it is just a `default` not something that is going to enforce anything.
It will for example, when you deploy a resource without any `limits` or `requests` nor any, put a `default` value for that resource.
So we can create some `LimitRange` in order to have `default` that we decide how much those are for some resources.
And only **new** `pods` on `admission` would be accepted or rejected (`403 forbidden`) following `limits` set in `LimitRange`
```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-resource-constraint
spec:
  limits:
  # here only `cpu` resources is set
  - default:
      cpu: 500m
    defaultRequest:
      cpu: 500m
    max:
      cpu: "1"
    min:
      cpu: 100m
    # limit on resource `container`
    type: Container
```
so if pod only have `requests` (min) limit set on `cpu` the other `limits` (max) will be set and given by the `default` `LimitRange` set `cpu` `limits`
```yaml
# inital `pod` applied to cluster by user
resources:
  requests:
    cpu: 200m
# final `pod ` spec.container[].resources`
resources:
  requests:
    cpu: 200m
  limits:
    cpu: 500m  # from LimitRange
```

**Example `LimitRange` for `pod` at the `namespace-level`**
```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: pod-limit-range
spec:
  limits:
  - type: Pod
    max:
      cpu: "2"
      memory: "4Gi"
```



### ResourceQuota
source (doc resource quota)[https://kubernetes.io/docs/concepts/policy/resource-quotas/]
it is a `namespaces` resource:
```bash
k api-resources | grep "resorucequota"
resourcequotas                    quota        v1                                     true         ResourceQuota
```

- `ResourceQuota`
While **`ResourceQuota`** **will enforce** `limits` on a `namespace` (in the `default` namespace in our exampel below as we didn't specify `namespace` in `metadata`). This is total sum of what all the resources in the `namespace` can consume `requests` if fine and can be passed over but `limits` not. so we have like that a range `min` `requests` and `max` `limits`.
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-resources
  namespace: dev
spec:
  hard:
    requests.cpu: "2"
    limits.cpu: "4"
    requests.memory: "2Gi"
    limits.memory: "4Gi"
```

Another ResourceQuota now not in `default` `namespace` so need to indicate the `namespace` in `metadata`:
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-resources
  namespace: dev
spec:
  hard:
    requests.cpu: "2"
    limits.cpu: "4"
    requests.memory: "2Gi"
    limits.memory: "4Gi"
```

- can set a priorityclass in `ResourceQuota` and `pods` could then reference any of those to group `pods` in a `certain` `policy` way of using `ResourceQuota`
Here an exampel of several `ResourceQuota` from the doc defined:
```yaml
apiVersion: v1
kind: List
items:
- apiVersion: v1
  kind: ResourceQuota
  metadata:
    name: pods-high
  spec:
    hard:
      cpu: "1000"
      memory: "200Gi"
      pods: "10"
    scopeSelector:
      matchExpressions:
      - operator: In
        scopeName: PriorityClass
        values: ["high"]
- apiVersion: v1
  kind: ResourceQuota
  metadata:
    name: pods-medium
  spec:
    hard:
      cpu: "10"
      memory: "20Gi"
      pods: "10"
    scopeSelector:
      matchExpressions:
      - operator: In
        scopeName: PriorityClass
        values: ["medium"]
- apiVersion: v1
  kind: ResourceQuota
  metadata:
    name: pods-low
  spec:
    hard:
      cpu: "5"
      memory: "10Gi"
      pods: "10"
    scopeSelector:
      matchExpressions:
      - operator: In
        scopeName: PriorityClass
        values: ["low"]

```
then the `pod` would reference one of those `ResourceQuota`
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: high-priority
spec:
  containers:
  - name: high-priority
    image: ubuntu
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello; sleep 10;done"]
    resources:
      requests:
        memory: "10Gi"
        cpu: "500m"
      limits:
        memory: "10Gi"
        cpu: "500m"
  priorityClassName: high
```

#### `scopeSelector` to track specific resources and restrict
So here it is a way to have a more fine grained control on resources `limits` consumption.

- resources tracked and that can be restricted:
```markdown
pods
cpu
memory
ephemeral-storage
limits.cpu
limits.memory
limits.ephemeral-storage
requests.cpu
requests.memory
requests.ephemeral-storage
```
Here we **restrict** for all `pods` in `namespace` `default` to use `crossNamespaceAffinity`
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: disable-cross-namespace-affinity
  namespace: foo-ns
spec:
  hard:
    pods: "0"
  scopeSelector:
    matchExpressions:
    - scopeName: CrossNamespacePodAffinity
      operator: Exists
```
- `operator`:
```markdown
In
NotIn
Exists
DoesNotExist
```
- `scopeName` requiring to use `operator: Exists`:
```markdown
Terminating
NotTerminating
BestEffort
NotBestEffort
```
- `scopeName` not requiring to use `operator: Exists`:
```markdown
CrossNamespacePodAffinity
PriorityClass
```
- if `operator: In/NotIn` we have to indicate `values`:
```yaml
  scopeSelector:
    matchExpressions:
      - scopeName: PriorityClass
        operator: In
        values:
          - middle
```

- example `pods` in `namespace` not allowed to have 'cross namespaces affinity'
example for doc that make `pod` limited to their own namespace `foo-ns` the pods created in `` as they won't be able to use `CrossNamespacePodAffinity`
as `spec.hard.pods: "0"`. so no `pod` is allowed to be in affinity out of the namespace `foo-ns`:
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: disable-cross-namespace-affinity
  namespace: foo-ns
spec:
  hard:
    pods: "0"
  scopeSelector:
    matchExpressions:
    - scopeName: CrossNamespacePodAffinity
      operator: Exists
```
- example *advanced** configuring `kube-apiserver`
`CrossNamespacePodAffinity` can be set as a **limited resource** by setting the `kube-apiserver` flag `--admission-control-config-file` 
where we would indicate the path of where is the below `yaml` file `kind: AdmissinConfiguration`

Here pods can use `namespaces` and `namespaceSelector` in `pod affinity` **only** if the `namespace` where they are created have a `ResourceQuota` object 
with `CrossNamespacePodAffinity` `scopeName` and a `hard` `limit` **greater than or equal to the number of pods** using those fields.

```yaml
apiVersion: apiserver.config.k8s.io/v1
kind: AdmissionConfiguration
plugins:
- name: "ResourceQuota"
  configuration:
    apiVersion: apiserver.config.k8s.io/v1
    kind: ResourceQuotaConfiguration
    limitedResources:
    - resource: pods
      matchScopes:
      - scopeName: CrossNamespacePodAffinity
        operator: Exists
```

- some other examples of `ResourceQuota` from ducomentation to understand it more, but need to check documentation for more advanced stuff like default `PriorityClass` consumption
(eg.: compute-resources.yaml)
```yaml apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-resources
spec:
  hard:
    requests.cpu: "1"
    requests.memory: "1Gi"
    limits.cpu: "2"
    limits.memory: "2Gi"
    requests.nvidia.com/gpu: 4
```

(eg.: object-counts.yaml from doc. but can also put all together in one `ResourceQuota` file)
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: object-counts
spec:
  hard:
    configmaps: "10"
    persistentvolumeclaims: "4"
    pods: "4"
    replicationcontrollers: "20"
    secrets: "10"
    services: "10"
    services.loadbalancers: "2"
```

### Example LimitRange Interaction and Logic With ResourceQuota
- example of rules and how `pod` `scheduling` would react to `LimitRange` and `ResourceQuota` set in same `namespace`:
we have: 
A `LimitRange` that sets default `cpu` `requests` = 150m, `limits` = 500m
A `ResourceQuota` that says `limits.cpu` = 2

We launch **5** `pods` with no `cpu` `requests/limits` in their specs.
Due to the default values from the `LimitRange`, each one gets 500m limit.

Now:
5 `pods` × 500m `limits` = 2500m = 2.5 cores
But your `ResourceQuota` is only 2 cores!

Result: The 5th `pod` will **not be scheduled** — even though it has valid `limits`, because it would **exceed the `namespace` `ResourceQuota`**.

### RuntimeClass
check `Runtime` running on specific `node`:
```bash
kubectl get node node1.creditizens.net -o jsonpath='{.status.nodeInfo.containerRuntimeVersion}'
Outputs:
containerd://1.7.25
```

`RuntimeClass` is to allocate resources to the runtime used, I use `runc` (as the `handler`) which is default to `containerd` `CNI` (container runtime interface) as the `container runtime` and we call allocated resources to it as well:

Supported Alternatives (`Pod-level` resource usage)
`pod` Overhead (with `RuntimeClass`)
`RuntimeClass` is a `cluster` scope resource and not `namespaced`
When using `RuntimeClass` (e.g. of others other than `runc`: `gvisor` or `kata-containers`), 
you can define a `Pod-level overhead`, which **adds additional `cpu/memory` usage per `pod`**:
```yaml
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: my-runtime
handler: runc
overhead:
  podFixed:
    memory: "128Mi"
    cpu: "250m"
```
Then reference it in your Pod:
```yaml
spec:
  runtimeClassName: my-runtime
```

**Important to know about `RuntimeClass`**
- All configs of our runtime `containerd` are here: `/etc/containerd/config.toml` and this is where it is also named `runc` for the handler name to reference `containerd` as runtime.(`[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.${HANDLER_NAME}]`)
- Also `RuntimeClass` accepts scheduling option to make sure that `pods` end out in nodes having the correct `label` which tells that a certain `Runtime` is ready and installed on that `node` using for example `runtimeclass.scheduling.nodeSelector`. Also if `node` have a `taint`, `toleration` can be set in `RuntimeClass`. Here also the intersection between `RuntimeClass`, `Pod`, `Node`, selectors/taints/tolerations would affect `pod` to where the right `Runtime` `handler` is installed. If it is not matching `pod` will be **evicted**. And all is done **at `pod` admission** stage. So lot of checks and more control over where workload ends out to run.
- `RuntimeClass` selects the container runtime handler to use for running the Pod.
- `RuntimeClass` does not control or enforce CPU, memory, or storage limits.
- `RuntimeClass` is not a resource quota or limit in itself.
- `ValidationAdmissionPolicy` could be used to restrict `pod` to a certain `namespace` using `RuntimeClass` but it is not what we are going to see here, we might see `ValidationAdmissionPolicy` in another chapter by itself for max `ReplicaSet` on `Deployments` policy limitation creation.

- `RuntimeClass` can be used in `pod` where you **NEED TO** set `requests` and `limits` for the `pod` resources consumption.
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: secure-pod
spec:
  runtimeClassName: my-runtime
  containers:
  - name: app
    image: myapp
    resources:
      requests:
        cpu: "500m"
        memory: "256Mi"
      limits:
        cpu: "1"
        memory: "512Mi"

```
- get `RuntimeClasses`:
but here normally you would not see anything as Kubernetes by default doesn't create a `RuntimeClass`.
This is something special that need to be set in `kubelet` config and use a special `admission` and probably activate a `feature gate` like `PodOverhead` for eg..
```bash
kubectl get runtimeclass
```
So the feature gate `PodOverhead` is enabled by default in kubernetes v1.28, but it can be manually activated in `kubelet` yaml file, by adding:
```yaml
# /var/lib/kubelet/config.yaml
--feature-gates=PodOverhead=true
# then restart kubelet
sudo systemctl restart kubelet
```

So why would I need to set an `pod` `overhead` to allocate resources for the `runtime` used by containers?
Because some runtime use some `cpu` and `memory` and it is not counted by `scheduler` so pod might look like `cheaper` in resources which can create `OOM Kills` or `CPU starvation` so we might want to have full control as with 2 pods it is fine but with 20000 pods it will have an impact so we **limit and indicate** how much resource can be used by the `runtime` and `scheduler` will take it into account in its calculation. therefore, better `pod` repartition accros `nodes`. 

### Scenarios

#### Scenarios analysis before decision on scenario:
`cpu` and `memory` units used and how it is calculated:
RAM -> decimal 10 based -> Decimal units 10 -> 1M = 1,000,000 -> expressed: k, M, G, etc.
RAM -> decimal 2 based -> Binary units 2 1Mi = 1,048,576 -> expressed: Ki, Mi, Gi, etc.

128974848 (raw bytes) This is already in bytes.
128,974,848 bytes

129e6 (scientific notation = 129 million decimal)
129e6 = 129 * 10^6 = 129,000,000 bytes

129M (129 megabytes in base 10 decimal)
129M = 129 * 1,000,000 = 129,000,000 bytes

123Mi (123 mebibytes in base 2 binary)
123Mi = 123 * 1,048,576 = 128,974,848 bytes (1 MiB = 1024 KiB -> 1 KiB = 1024 bytes, So 1 MiB = 1024 × 1024 bytes = 1,048,576 bytes)

CPU: just use milicores `m` of `integers` without any `m` so it will imply full cores.

then here just show normal resource request/limits
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: high-priority
spec:
  containers:
  - name: high-priority
    image: ubuntu
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello; sleep 10;done"]
    resources:
      requests:
        memory: "10Gi"
        cpu: "500m"
      limits:
        memory: "10Gi"
        cpu: "500m"
    # CPU Example (1 full CPU core)
    resources:
      requests:
        cpu: "1"
      limits:
        cpu: "2"

    # Memory Example (binary-based)
    resources:
      requests:
        memory: "512Mi"
      limits:
        memory: "1Gi"

    # Ephemeral Storage Example (decimal-based)
    resources:
      limits:
        ephemeral-storage: "1G"
```

- cat resource_request_limit_1_mormal.yaml 
can probably work this example to explain step by step what need to be installed and options to put in place `GRUB`,
`metric-server`, and then run it, find a command that would make the pod use too much resource to be evicted
and have a policy attached to `namespace` and even a gateway for admission with `limitrange`
so that we have one huge scenario which will introduce all step by step. `sizeLimit` for `emptyDir` would also be used would also be used.
`resourceQuota` for `namespace` `limits` and `scope` liked to `pod` which will be part of those `scope` `priorityClassName`.
So need to think in steps to introduce all step by step...
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test
  namespace: limited-resources
spec:
  containers:
  - name: reader
    image: nginx
    volumeMounts:
    - name: special-shared-volumes
      mountPath: /tmp/cache
    resources:
      requests:
        memory: "16Mi"
        cpu: "250m"
      limits:
        memory: "32Mi"
        cpu: "500m"

  # sidecar container as container which will be having limits in resources for eg.
  - name: writer
    image: busybox:1.36.1
    command: ["/bin/sh"]
    args: ["-c", "echo 'rigoleto italian restaurant shibuya' > /tmp/cache/which_restaurant.txt && sleep 3600"]

    volumeMounts:
    - name: special-shared-volumes
      mountPath: /tmp/cache
    # maybe add another container using `hugepages` and show how to set it on linus and how to get rid of it
    # - name: hugepage
      # mountPath: /hugepages

    # resources mixing `ephemeral-storage` and `memory` and `cpu`
    resources:
      # cpu/memory/epheremeral-storate can be set together in request but NO `resquests` for `hugepages`
      requests:
        memory: "1Mi"
        cpu: "500m"
        ephemeral-storage: "1M"
      # cpu/memory/ephemeral-storage/hugepages can all be set together in `limits`
      limits:
        # memory binary-based
        memory: "3Mi"
        # memory deciman-based
        # memory: "3M"
        # memmory scientific-based (pure numeric no need to use M/Mi and for templating from other tools or pragramms can be nice)
        # memory: "3e6"
        cpu: "1"
        # Ephemeral Storage Example (decimal-based)
        ephemeral-storage: "3M"
        # important: with `hugepages` only `limits` are possible to set and not `requests`
        #hugepages-2Mi: 100Mi

  volumes:
  - name: special-shared-volumes
    emptyDir: {}
  # `medium` can be used because only one `hugepage` volume is used in this `yaml` otherwise would just use it `volumes` without the `medium` like: `emptyDir: {}`
  # - name: hugepage
    # emptyDir:
      # medium: HugePages

```


`hugepage` for better cpu usage and side pod with request/limit as can't be set together. `hugepage` can be set on nodeii custom way at :
```bash
# append to `GRUB_CMDLINE_LINUX`: `default_hugepagesz=2M hugepagesz=2M hugepages=512`
sudo nano /etc/default/grub
default_hugepagesz=2M hugepagesz=2M hugepages=512
# Then update GRUB and reboot:
sudo update-grub
sudo reboot
# Verify the Node Reservation after reboot:
grep Huge /proc/meminfo
# delete the line added or comemnt it out and update-grub and reboot to go back to defaut `4Ki` page
```

`emptyDir` issue and how to control
control using `ephemeral-storage` way:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: limit-ephemeral
spec:
  containers:
  - name: app
    image: busybox
    # this will write 500 block of 1M resulting in a 500Mi(B) file: it is good to use this command so we can test limit storage
    command: ["sh", "-c", "dd if=/dev/zero of=/data/file bs=1M count=500"]
    volumeMounts:
    - mountPath: /data
      name: cache-volume
    resources:
      requests:
        ephemeral-storage: "200Mi"
      limits:
        # this would be used as limit for the `volumes.emptyDir`
        ephemeral-storage: "500Mi"
  volumes:
  - name: cache-volume
    # now this volume is limited to the `spec.containers[0].resources.limits.ephemeral-storage` of 500Mi, more use would kill the pod
    # after it depends on its restart policy or type of pod...
    emptyDir: {}
```
here in conjunction with `request/limit`:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: full-limits-example
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c", "echo Hello Junko && sleep 3600"]
    volumeMounts:
    - mountPath: /data
      name:  some-volume-normally-not-limited-but-here-limites-by-epheremal-storage
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
        ephemeral-storage: "100Mi"
      limits:
        cpu: "500m"
        memory: "256Mi"
        ephemeral-storage: "500Mi"
  volumes:
  - name: some-volume-normally-not-limited-but-here-limites-by-epheremal-storage
    emptyDir: {}
```
control using `sizeLimit` way:
```yaml
apiVersion: v1
kind: Pod
metadata:
name: ram-cache
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c", "echo Hello > /cache/hello && sleep 3600"]
    volumeMounts:
    - name: ram-vol
      mountPath: /cache
  volumes:
  - name: ram-vol
    emptyDir:
      # this option would be using in-`RAM` `Memory` for this `ephemeral-storage` (udner the hood)
      medium: Memory
      sizeLimit: 64Mi
```

use `LimitRange` to set default `resquest/limits` on pods that do not indicate it. tell that is working on pod admission to reject pod scheduling or not, but has is not enforcing anything on pod already running. so if pod have not set one of those the but limit range have set it, it will be set on pod as default.
`container`-level `LimitRange`: 
```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-resource-constraint
  namespace: <not need to indicate if `default` namespace used>
spec:
  limits:
  # here only `cpu` resources is set
  - default:
      cpu: 500m
    defaultRequest:
      cpu: 500m
    max:
      cpu: "1"
    min:
      cpu: 100m
    # limit on resource `container`
    type: Container
```
`pod`-level `LimitRange`:
```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: pod-limit-range
  namespace: <not need to indicate if `default` namespace used>
spec:
  limits:
  - type: Pod
    max:
      cpu: "2"
      memory: "4Gi"
```

limits for resources in the `namespace` total will use `ResourceQuota` for that
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-resources
  namespace: dev
spec:
  hard:
    requests.cpu: "2"
    limits.cpu: "4"
    requests.memory: "2Gi"
    limits.memory: "4Gi"
```
`ResoruceQuota` with `priorityClass` that would be referenced in `pod`:
```yaml
---
apiVersion: v1
kind: List
items:
- apiVersion: v1
  kind: ResourceQuota
  metadata:
    name: pods-high
    namespace: junko
  spec:
    hard:
      cpu: "1000"
      memory: "200Gi"
      pods: "10"
  scopeSelector:
      matchExpressions:
      - operator: In
        # can also be `CrossNamespacePodAffinity` to limit pods `CrossNamespacePodAffinity` but we are going to see it here, just look at documentation
        # need to set a `kind: AdmissionConfiguration` for `CrossNamespacePodAffinity` which will set that rule using `api-server` cluster wise but we are not going to see it here.
        scopeName: PriorityClass
        values: ["high"]
- apiVersion: v1
  kind: ResourceQuota
  metadata:
    name: pods-low
    namespace: junko
  spec:
    hard:
      cpu: "5"
      memory: "10Gi"
      pods: "10"
    scopeSelector:
      matchExpressions:
      - operator: In
        scopeName: PriorityClass
        values: ["low"]

---
apiVersion: v1
kind: Pod
metadata:
  name: high-priority
  namespace: junko
spec:
  containers:
  - name: high-priority
    image: ubuntu
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello; sleep 10;done"]
    resources:
      requests:
        memory: "10Gi"
        cpu: "500m"
      limits:
        memory: "10Gi"
        cpu: "500m"
  priorityClassName: high
```

talk about `kind: RuntimeClass` taht can be used to have even more control on how much resource are allocated to container runtime as it is not calculated by `scheduler` if not set. while if set it will be considered for pod `scheduling`. when running 2 pods it is fine but when running 20000 pods with different runtimes, those will consume resoruces that can affect performance and create issues of pods being evicted because OOM killes of CPU starvation so another resource consuming RAM and CPU but not calculated correctly creating issues int he cluster. It is better to have full control and leverage kubernetes native scheduler behaviour in our favor by privideing as much detailed information to it before it decides the repartition of the workload in the cluster.


## Issues

### `OCI runtime create failed: ... error setting cgroup config for procHooks process: ... cgroup.controllers: no such file or directory`

- when using `ephemeral-storage` need to have an option activated on the `GRUB`, `/etc/default/grub` file with the var `GRUB_CMDLINE_LINUX` need to add `systemd.unified_cgroup_hierarchy=1`: so i have appended it to already existing options: `GRUB_CMDLINE_LINUX="find_preseed=/preseed.cfg auto noprompt priority=critical locale=en_US systemd.unified_cgroup_hierarchy=1"`
after need to update and reboot:
```bash
sudo update-:grub
sudo reboot
```
then check:
```bash
stat /sys/fs/cgroup/cgroup.controllers
```
lesson: 
  - To make ephemeral-storage requests/limits work on Kubernetes 1.28 + containerd:
    - Kernel >= 5.2 (you have 6.8)
    - GRUB must enable cgroup v2 explicitly, with: `systemd.unified_cgroup_hierarchy=1`
    - `sudo nano /etc/containerd/config.toml` and check that `systemdCgroup` is set to `true`:
    ```bash
    # inside /etc/containerd/config.toml
    [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
      SystemdCgroup = true
    # restart containerd
    sudo systemctl restart containerd
    ```
    - and check the kernel version `>= 5.8` to be able to use `Cgroup.v2`
    ```bash
    uname -r
    outputs:
    6.8.0-57-generic
    ```

### `error during container init: procReady not received, openat2 ... cgroup.controllers: no such file or directory`
- wehn setting `limits` and `request` if it is too low and even the binary can't startm you will run in `memory` starvation because you provided fewer resoruces than the minimum required. Here `nginx` container has been started with less permitted request than required to start the binary so the pod would never start and gets error.
- `runc` can't finish setting up `Cgroup` therefore exists.

Solution: 
- increase the `memory` `requests` and `limits`
- run a pod discovery meaning create a pod and chech how much resource the container needs by running `k top pod <pod_name>`, then you will see `memory` and `cpu` usage and be able to set it in the `yaml` file, add alsway a buffer of `20%`.
- You can also set a `LimitRange` in order for you to have control on the minimum resource that are requested and limited by pods containers, so that if it is too low pod won;t be passing the `admission` control thanks to the `LimitRange`
```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: default-limits
  namespace: test
spec:
  limits:
  - default:
      memory: 64Mi
      cpu: 100m
    defaultRequest:
      memory: 32Mi
      cpu: 50m
    type: Container
```
- or use Prometheus/Grafana and see how much pod is using and then you will know roughly what are the `requests` and `limits` to set by taking the average high resoruce use and adding `20%` to be sure.

```bash
k get pods
Outputs:
NAME   READY   STATUS    RESTARTS      AGE
test   1/1     Running   1 (46m ago)   179m

k top pod test
Outputs:
NAME   CPU(cores)   MEMORY(bytes)   
test   0m           13Mi 
```
```bash
k top pod -A
NAMESPACE            NAME                                                 CPU(cores)   MEMORY(bytes)   
kube-system          calico-kube-controllers-85578c44bf-xqlmz             5m           56Mi            
kube-system          calico-node-7m4mc                                    90m          145Mi           
kube-system          calico-node-8v4pl                                    63m          98Mi            
kube-system          calico-node-mmh88                                    62m          145Mi           
kube-system          coredns-5d78c9869d-kpbtn                             3m           28Mi            
kube-system          coredns-5d78c9869d-l47pn                             3m           41Mi            
kube-system          etcd-controller.creditizens.net                      49m          78Mi            
kube-system          kube-apiserver-controller.creditizens.net            125m         317Mi           
kube-system          kube-controller-manager-controller.creditizens.net   31m          124Mi           
kube-system          kube-proxy-88h87                                     21m          61Mi            
kube-system          kube-proxy-bs58j                                     17m          61Mi            
kube-system          kube-proxy-mwnmk                                     13m          60Mi            
kube-system          kube-scheduler-controller.creditizens.net            7m           64Mi            
kube-system          metrics-server-596474b58-hn5tz                       9m           66Mi            
local-path-storage   local-path-provisioner-6548cc785f-wmv98              1m           46Mi
```

### `Metrics-server` activation on cluster to be able to run `k top...` command

isntall
```basg
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```
issue here was that the `metrics-server` `pod` in `namespace` `kube-system` intalled wasn't starting but just `running` in `0/1`. So this known issue so need to patch the `deployment` to get it running and then can use th ecommand `k top ...`
**"metrics-server tries to securely scrape kubelet metrics using HTTPS with valid certificates. But in your kubeadm cluster, kubelet uses self-signed certs, and by default, the metrics-server doesn't trust them."**
```bash
kubectl patch deployment metrics-server -n kube-system --type='json' -p='[{"op":"add","path":"/spec/template/spec/containers/0/args/-","value":"--kubelet-insecure-tls"}]'

```

Solution here is to patch it so that it ignores those self-signed certs of `kubeadm cluster`

### where are `emptyDir` shared volumes of contaienr located
- first of all the containers can be checked if running fine using `crictl` the `cli` tool of `containerd`. So first need to check where pod have been scheduled and then `ssh` to the node. and then run `sudo crictl ps -a` to find the container running int he pod and troubleshoot those if any issues.
- if no issues where is the shared volume path int he file system as i have created one at `/tmp/cache` but aren't seeing it when doing `ssh` to `node1.creditizens.net1` where the `pod` has been scheduled?
The location is actually in the `kubelet` folder at path like: `/var/lib/kubelet/pods/<pod-UID>/volumes/kubernetes.io~empty-dir/<volume-name>`
for us:
```bash
ls /var/lib/kubelet/pods/<pod-UID>/volumes/kubernetes.io~empty-dir/special-shared-volumes/
#here is real data on node1.creditizens.net
sudo ls -la /var/lib/kubelet/pods/9d0501b0-e367-41a4-819b-2fba105c16d7/volumes/kubernetes.io~empty-dir/special-shared-volumes
total 12
drwxrwxrwx 2 root root 4096 avril 15 03:43 .
drwxr-xr-x 3 root root 4096 avril 15 03:43 ..
-rw-r--r-- 1 root root   36 avril 15 03:43 which_restaurant.txt
# then
sudo cat /var/lib/kubelet/pods/9d0501b0-e367-41a4-819b-2fba105c16d7/volumes/kubernetes.io~empty-dir/special-shared-volumes/which_restaurant.txt
Outputs:
rigoleto italian restaurant shibuya
```
eg. how to get pod UID:
```bash
kubectl get pods -n limited-resources test -o jsonpath='{.metadata.uid}'
outputs:
9d0501b0-e367-41a4-819b-2fba105c16d7
# OR displya nice columns
kubectl get pods -n limited-resources -o custom-columns=PodName:.metadata.name,PodUID:.metadata.uid
PodName   PodUID
test      9d0501b0-e367-41a4-819b-2fba105c16d7
```

solution: here different from when we were using `storageclass` where in dynamic it was created in the `csi` path for the `pv` or to troubleshot `pvc` at `/var/lib/kubelet/plugins/kubernetes.io/csi/pv/<pv-name>/...` or even in static provisioning where the path needed to exist beforehands on the node corresponding to the same path indicated on the container. 
while here it is in the `kubelet` folder at a certain path for the `emptyDir` to live in and be deleted when pod stops.


## Commands helping to put some **stress on the memory** by increasing the size of resources used
```bash
# can be used in `yaml` files
i=0; while true; do echo "$i - writing some data to fill the volume" >> /cache/file.txt; i=$((i+1)); sleep 0.1; done
```
```bash
# can be ran inside a pod by `exec` into it creating straight away a big file (here 10MiB filesize)
dd if=/dev/zero of=/cache/bigfile bs=1M count=10
```

## Commands to put **stress on cpu**
stress CPU usage inside a container and trigger CPU limits, you can use tools like:

- `yes` command (simplest, built-in)
This will max out one CPU core by continuously printing y.
```bash
yes > /dev/null
```

- `dd` CPU-bound example
This reads and discards data rapidly — very CPU-intensive.
```bash
dd if=/dev/zero of=/dev/null bs=1M
```

- `sh` loop with math (portable)
A tight loop that constantly calculates something.
```bash
sh -c 'while true; do echo $((13*45)); done'
```

- Stress test with stress tool
If image includes stress (like ubuntu, debian, alpine with apk add stress), we can use:
# `--cpu 2`: spin up 2 CPU workers
# `--timeout 60`: run for 60 seconds
```bash
stress --cpu 2 --timeout 60
```

- if container doesn't include it, can use an image like:
```yaml
image: progrium/stress
command: ["stress"]
args: ["--cpu", "2", "--timeout", "60"]
```
(excalidraw walkthrough diagram)[https://excalidraw.com/#json=kR1Z7xUGN7pIUZDpRdOMt,TF01Tb_MHbDPCgbrR-JJlA]


___________________________________________________________

# Validation Admission Policy
Source: (doc admission policy)[https://kubernetes.io/docs/reference/access-authn-authz/validating-admission-policy/]
```bash
k api-resources | grep "validating"
validatingwebhookconfigurations                admissionregistration.k8s.io/v1        false        ValidatingWebhookConfiguration
```
"`Validating admission policies` offer a declarative, **in-process alternative** to `validating admission webhooks`"

**x3 resources are needed to have a `validatingAdmissionPolicy` setup:**
- a `ValidationAdmissionPolicy`: That is the main subset of the policy which enforces a behaviour check or restriction.
- `parameter resources`: Some resources defined with some expressions constraint or precision in order for the `ValidatingAdmissionPolicy` to know parameter of the `kind:` (ConfigMap, CRD, Pod, etc...) that need to be fulfilled. (optional can also not be set, if so, do not specify `spec.paramKind` in `ValidatingAdmissionPolicy`. What is interesting with `parameter resources` is that we can create our custom `yaml` with our custom `apiVersion:` and custon `kind:` and use any custum fields after `metadata:` (which even if custom can be namespaced or not). Then in the `ValidatingAdmissionPolicy` use `paramKind` to reference it and use the `matchExpression` to check against it and in the other hand in `ValidatingAdmissionPolicyBinding` have a `paramRef` and also reference it there.
- q `ValidatingAdmissionPolicyBinding`: here is the one making the `ValidationAdmissionPolicy` and the `parameter resources` to be linked. Even when we don't have `paramter resources` specified, this `ValidatingAdmissionPolicyBinding` is mandatory and the minimal set up is `ValidatingAdmissionPolicy` with `ValidatingAdmissionPolicyBinding` (and `spec.paramKind` not specified in `ValidatingAdmissionPolicy`)

## a `ValidatingAdmissionPolicy`
(eg. from kubernetes doc.)
```yaml
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicy
metadata:
  name: "demo-policy.example.com"
spec:
  # if not set would default to `Fail`, if set and not to `Fail` the `validations` would be ignored
  # can be set also to `Ignore`. here it check what happens if Kubernetes can't verify this validation expression.
  failurePolicy: Fail
  matchConstraints:
    # apply `admission policy` only to resource `deployment` when it is `created` and `updated` and it is part of the `v1` API and `apps` API group
    resourceRules:
    - apiGroups:   ["apps"]
      apiVersions: ["v1"]
      operations:  ["CREATE", "UPDATE"]
      resources:   ["deployments"]
  # if `true` all good, if `false` so different would enforce the above option `failurePolicy`
  # and `Fail` here as this is what it is set. (if other can be ignored as well)
  validations:
    - expression: "object.spec.replicas <= 5"
```
Here: `resourceRules` is the "selector" of what kind of object (Deployment, Pod, etc.) and on what kind of action (Create, Update, Delete) the policy must run.

## a `ValidatingAdmissionPolicyBinding`
different values that can be taken by `validationActions` in `ValidatingAdmissionPolicyBinding`:
- `Deny`: Validation failure results in a denied request. Can be used with `Audit`: [`Deny`, `Audit`] , can't be used with `Warn` as opposite effects.
- `Warn`: Validation failure is reported to the request client as a warning. Can be used with `Audit`: [`Warn`, `Audit`], can't be used with `Deny` as opposite.
- `Audit`: Validation failure is included in the audit event for the API request. can be used with either of both `Deny` and `Warn` BUT NOT alone.
**So here it will also be enforced depending on the other resource it is binded to the `validatingAdmissionPolicy`'s `failurePolicy`**
```yaml
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicyBinding
metadata:
  name: "demo-binding-test.example.com"
spec:
  # linking to `ValidatingAdmissionPolicy`
  policyName: "demo-policy.example.com"
  # here enforced check, what happends when the `ValidatingAdmissionPolicy` referenced in `policyName` return `false` (expression not validated)
  validationActions: [Deny]
  matchResources:
    namespaceSelector:
      matchLabels:
        environment: test
```

### is `validationActions` in `ValidatingAdmissionPolicy` redundant when we have `failurePolicy` in `ValidatingAdmissionPolicy`

Look like YES but actual it is NOT as they both check different things:
Role                                                    | Purpose
failurePolicy (in ValidatingAdmissionPolicy)            | What happens **if the admission check system itself is broken** or unreachable (for example, the expression engine is unavailable, server crashes) — **Should the API server fail or ignore** the check? (meta failure handling)
validationActions (in ValidatingAdmissionPolicyBinding) | **What** to do **when the expression runs fine but returns FALSE** (meaning the object **failed validation**) — **Should it deny, warn, or audit?** (actual check result handling)

#### Examples to understand the workflow (as a bit complicated)
ValidatingAdmissionPolicy defines:
```yaml
validations:
  - expression: "object.spec.replicas <= 5"
failurePolicy: Ignore
```
ValidatingAdmissionPolicyBinding defines:
```yaml
validationActions: [Deny]
namespaceSelector:
  matchLabels:
    environment: dev
```
If I create a Deployment in a namespace labeled environment=dev with replicas: 10, here is what happens:
- Selector matches → ValidationPolicy triggered
- Expression evaluated → returns false (because 10 > 5)
- Engine is working (no crash) → so failurePolicy not triggered.
- Expression false → validationActions: Deny → Request denied.

#### Another Example:
- .1) User sends object to API Server.
- .2) Binding: matchResources (selector) checked → Does this policy apply?
    - If yes, proceed.
    - If no, nothing else happens.
- .3) Policy: ValidatingAdmissionPolicy applied → Expression evaluated.
- .4) If evaluation:
    - Crashes → failurePolicy (Fail or Ignore) is checked.
    - Succeeds:
      - If expression true → allow.
      - If expression false → validationActions (Deny / Warn / Audit) decides.


**NOTES:**
- `validationActions` = what happens if rule is violated (false).
- `failurePolicy` = what happens if Kubernetes itself can't even run the check.

**So to resume the understanding:**
- I apply resoruce to cluster
- If in the `ValidatingAdmissionPolicyBinding` the selector i not triggered this `admissing policy` won't be enforeced. In the other hands, if it matches the s2lector (True) it will then check the `ValidatingAdmissionPolicy` 'binded' to it and indicated in `policyName`.
- Then in the `ValidatingAdmissionPolicy` the expression is checked if it is `true` all good `Admission: OK!`, if it is not good `false` (expression condition not respected) it will check the `failurePolicy` indicated and apply it `Fail` (Kubernetes can't validate expression) or `Ignore` (ignore that Kubernetes can't verify because of any issue or just not complying). So this is not yes stoppong anything. it is after.
- Now when the `failurePolicy` is triggered in the `ValidatingAdmissionPolicy` the `validationActions` of the `ValidatingAdmissionPolicyBinding` will apply `War n` and let it deploy with warning thereofre `admit it: OK! but warning !` OR `Deny` it and stop the deployment of the resource so `Not Admited`. `Audit` if present as works in combinaison of any of `Warn/Deny`, it will also include the event int he API Request events.

## a `paramter resource`
we use here example from documentation and will comment it out
custom resource created by admin, `apiVersion:` and `kind:` are all custom
```yaml
# fully custom  `apiVersion:` as there is no controller/CRD or check, it has just to be valid `CEL` language
apiVersion: rules.example.com/v1
# fully custom  `kind:` as there is no controller/CRD or check, it has just to be valid `CEL` language
kind: ReplicaLimit
metadata:
  name: "replica-limit-test.example.com"
  # can specify a namespace, if no namespace it will be `cluster-wise`
  # like `ValidatingAdmissionPolicy/and ValidatingAdmissionPolicyBinding` are cluster-wise`
  namespace: "default"
# custom parameter
maxReplicas: 3
```

`paramKind` used in `ValidatingAdmissionPolicy` to reference the resource `kind:` we are going to check `matchExpressions` against
```yaml
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicy
metadata:
  name: "replicalimit-policy.example.com"
spec:
  # this is evaluated only if the expression match could not be evaluated by kubernetes for any reason syntaxe error or other...
  # otherwise it is the `ValidatingAdmissionPolicyBinding` `validationActions` that is normally triggering the decision, `Deny` or `Warm` or all good if the `match expression` positively evaluated
  failurePolicy: Fail
  # here is the reference of the resource made available to this `ValidatingAdmissionPolicy`
  paramKind:
    apiVersion: rules.example.com/v1
    kind: ReplicaLimit
  matchConstraints:
    resourceRules:
    - apiGroups:   ["apps"]
      apiVersions: ["v1"]
      operations:  ["CREATE", "UPDATE"]
      resources:   ["deployments"]
  # now validation here can use `params` keyword to access fields in the `resource referenced in `paramKind`
  validations:
    - expression: "object.spec.replicas <= params.maxReplicas"
      # `reason` need to be a valid one: see doc to see list of those otherwise don't put it and Kubernetes will display default
      reason: Invalid
      # `message` can also be used and inside of it you can use `CEL` expressions like `${variable}`, `.size()`, `.startsWith()`, `||`, `&&` (see doc)
      # message: 
```
`paramRef` used in `ValidatingAdmissionPloicyBinding` to tell which resource is made available to check `matchExpression` against in `ValidatingAdmissionPolicy`
```yaml
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicyBinding
metadata:
  name: "replicalimit-binding-test.example.com"
spec:
  # this specified to what `ValidatingAdmissionPolicy` this `ValidatingAdmissionPolicyBinding` is binded to
  policyName: "replicalimit-policy.example.com"
  # This would be actioned if the `validation` `match expression` is `false` so a>9 is a=2 so false. But if this `match expression` evaluation fail at kubernetes level, so like `missing parameter`, `syntaxe error` or other, then the `failurePolicy` is here as backup to make it `Fail` or `Ignore` it.
  validationActions: [Deny]
  # show here the resource made available to check matchExpressions in `ValidatingAdmissionPolicy` referenced here at key `policyName`
  paramRef:
    name: "replica-limit-test.example.com"
    namespace: "default"
  # check this first to see if policy can be triggered, if this is `true` will start checks
  matchResources:
    namespaceSelector:
      matchLabels:
        environment: test
```

**Importnat Notes:**
- polices won't be created if one references and the other not, meaning if one has `paramRef` the other MUST have matching `paramKind`
- Multiple `ValidatingAdmissionPolicyBindings` to one `ValidatingAdmissionPolicy` possible. but not the other way around. But at the end only one would `matchExpressions`
- one `ValidatingAdmissionPolicy` can have multiple `matchConstraints.resourceRules`

## Scenario:
need to activate featuregate, CRDs in `kube-apiserver.yaml`: `--feature-gates=ValidatingAdmissionPolicy=true`
`kube-apiserver` will pickup the change and restart to activate it.
in order to use custom resource need to create `CRD's` defining this new resource so that we can use it in kubernetes and also we need to set `RBAC` for Kubernetes to be able to have read access to those resources.
see (CRD's Doc)[https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions]

### Issues
tried to add feature gate in kubernetes v1.28.15 using same way i did with `SidecarContainer=true` in custom config files customed from boiler plate `kubeadm config print init-defaults > <my custom yaml file>`, updated the controller node ip address and changed the name of the node to the DNS name of the controller node `controller.creditizens.net` and added the feature gates `ValidatingAdmissionPolicy` with the sidecontainer one.
but when used `sudo kubeadm upgrade apply v1.28.15 --config <my cusotm yaml config file>` i got an error even after upgrate to `v1.29.15` same error.
```bash
[upgrade/apply] FATAL: couldn't upgrade control plane. kubeadm has tried to recover everything into the earlier state. Errors faced: failed to obtain static Pod hash fo
r component kube-apiserver on Node controller.creditizens.net: Get "https://controller.creditizens.net:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-controller
.creditizens.net?timeout=10s": dial tcp 192.168.186.146:6443: connect: connection refused
To see the stack trace of this error execute with --v=5 or higher
```
- couldn't deploy `ValidatingAdmissionPolicyBinding` and `ValidatingAdmissionPolicy`, error message: `ensure CRD's are intalled` was the error when `k apply -f <validatingadmissionpolicy custom yaml file>`
  So here we need to activate the `--feature-gates` in `kube-apiserver.yaml`: `ValidatingAdmissionPolicy=true` 
  i had to upgrade to version v1.29.15 as it is easier to setup this feature gate as it is already there by default but need just to add some line in th ekubernetes yaml files.
  Then, I just added in `kube-apiserver.yaml`
```bash
    - --feature-gates=SidecarContainers=true,ValidatingAdmissionPolicy=true
    - --runtime-config=admissionregistration.k8s.io/v1beta1=true
```
  and in /var/lib/kubelet/config.yaml
```bash
featureGates:
  SidecarContainers: true
  ValidatingAdmissionPolicy: true
```
  then just reloaded daemon and restarted kubelet
```bash
sudo systemctl daemon-reload
sudo systemctl restart kubelet
```
And then applied my custom validating admission policy files and policy was enforced , worked fine. now will play with it to make a custom example 

## full scenario used with creation of CRD and RBAC for our custom resource
- `CRD` creation, custom `resource API creation`, `RBAC` roles and binding for kubernetes to be able to read it `system:authentication`
(cat junko-rules.yaml)
```yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  # must be spec.names.plural+"."+spec.group
  name: mangakissa-zones.mister.w.rules
spec:
  # +spec.group
  group: mister.w.rules
  versions:
    - name: v1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            replicasFriends:
              type: integer
            soundRules:
              type: string
            locationArea:
              type: string
  scope: Namespaced
  names:
    # spec.names.plural+
    plural: mangakissa-zones
    singular: mangakissa-zone
    kind: VolumeNaruto

---
# fully custom `apiVersion` as there will be checks so we need to create CRDs, it just has to be valid `CEL` language
apiVersion: mister.w.rules/v1
# fully custom `kind` as there will be checks so we need to create CRDs, it just has to be valid `CEL` language
kind: VolumeNaruto
metadata:
  name: "watch.naruto.mister.w.com"
  # can specify a namespace, if no namespace it will de `cluster-wise`
  # as `ValidatingAdmissionPolicy/Binding` are `cluster-wise`
  namespace: "mangakissa-zone"
# custom parameters that we can check on using the `expression` check mechanism
replicasFriends: 3
soundRules: "naruto no sound"
locationArea: "shibuya"

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: mangakissa-custom-resource-reader
rules:
- apiGroups: ["mister.w.rules"]
  resources: ["mangakissa-zones"]
  verbs: ["get", "watch", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: mangakissa-custom-resource-reader-binding
subjects:
- kind: Group
  name: system:authenticated
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: mangakissa-custom-resource-reader
  apiGroup: rbac.authorization.k8s.io
```

- `ValidatingAdmissionPolicy`
(cat validating-admission-policy-mangakissa.yaml)
```yaml
apiVersion: admissionregistration.k8s.io/v1beta1
kind: ValidatingAdmissionPolicy
metadata:
  name: "volume-naruto-policy.creditizens.com"
spec:
  # this is evaluated only if `expression` down there couldn't be evaluated
  # therefore any syntaxe error, or kubernetes can't evaluate it will come here and check the `failurePolicy`
  # can be `Fail` or `Ignore`, if not indicated, default to `Ignore`
  failurePolicy: Fail
  # here is the reference of the resource made available to this policy to check on
  matchConstraints:
    resourceRules:
    - apiGroups: ["apps"]
      apiVersions: ["v1"]
      operations: ["CREATE", "UPDATE"]
      resources: ["deployments"]
  # now validation here can use an exprssion to check if condition is satisfied or not
  # when these `validations` can be evaluated by Kubernetes `failurePolicy` up there is not triggered,
  # it is another file that we are going to create having a `validationActions` that will decide what to do
  # is the `User` APIRequest to `CREATE` or `UPDATE` the `deployments` good to go or not
  paramKind:
    apiVersion: mister.w.rules/v1
    kind: VolumeNaruto
  validations:
    # expression using `CEL` language so comprise any operation like `.contains()`, `&&`, `==` ..etc.. see doc...
    #- expression: "object.spec.replicas <= object.spec.inventedfield"
      # in the doc you can specify some limited reasons otherwise put nothing and it will default to kubernetes ones
      #reason: Invalid
      # we are not going to use, but a message could also be entered, a text that you want this can be custom and also accepts arguments using `$(ARGUMENT)`
      # messages: ...
    # so keyword `object` is used to reference the resource created by user fields concerned in this policy
    # the keyword `params` is used to reference the field used in our custom `CEL` compliant resource
    - expression: "object.metadata.annotations['creditizens-vip-friends'].contains(params.soundRules)"
```

- `ValidatingAdmissionPolicyBinding`
(cat validating-admission-policy-bindings-mangakissa.yaml )
```yaml
apiVersion: admissionregistration.k8s.io/v1beta1
kind: ValidatingAdmissionPolicyBinding
metadata:
  name: "volume-naruto-policy-binding.creditizens.com"
spec:
  # this field is referencing the `ValidatingAdmissionPolicy`: one policy to one binding
  policyName: "volume-naruto-policy.creditizens.com"
  # this would be actioned if the `expression` in the conterpart admission policy can be evaluated by Kubernates
  # actions can be `Deny`(APIRequest rejected) , `Warn`(APIRequest can pass with a warning),
  # `Deny + Audit` (Rejected with event written), `Warn + Audit` (Can Pass with event written), `Audit` can't be used alone
  validationActions: ["Deny"]
  # here the resource made available to check the expression in `ValidatingAdmissionPolicy` conterpart need to be matching this selector
  # so not all deployments will be checked, only the ones in any namespace having the label `location=shibuya-level-5`
  # therefore, the is what is checked first and only when this is satisfied the Admission policy check starts to be triggered and pass check those rules...
  # now we are providing the reference of the config file that is going to be used as reference in the `validations.expression`
  paramRef:
    name: "watch.naruto.mister.w.com"
    namespace: "mangakissa-zone"
    # if the external resource reference is not there anymore it will deny any APIRequest
    parameterNotFoundAction: "Deny"
  matchResources:
    namespaceSelector:
      matchLabels:
        location: shibuya-level-5
        # location: ometesando-level-2
```
Then just create deployment yaml files to play with annotations changing the `ValidatingAdmissionPolicy` `validation.expression`
also something that it is not in the documentation of Kubernetes is that using `paramRef` in `ValidatingAdmissionPolicyBinding` needs also in addition of `namespace` and `name`, a ` parameterNotFoundAction` ("Deny" or "Allow") taht would look if the referenced resource still exists. 

- exmaple deployment used with `annotations` and can change the `annotations` or comment it out
(cat mangakissa-admission-2-friends.yaml)
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: mangakissa-admission-friend
  name: mangakissa-admission-2-friend
  namespace: mangakissa-zone
  annotations:
    creditizens-vip-friends: "naruto no sound"
spec:
  replicas: 2
  selector:
    matchLabels:
      app: mangakissa-admission-friend
  template:
    metadata:
      labels:
        app: mangakissa-admission-friend
    spec:
      containers:
      - image: nginx
        name: nginx
```

______________________________________________________________________

# Cronnobs
source: (doc cronjobs)[https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#schedule-syntax]

**Important from doc**: 
  - "Jobs that you define should be idempotent", meaning consistent result even if it runs multiple times. So running it would have same effects everytime.
  - `controller` stop running `cronjobs` when it counts 100 misses. Missed can be concurrent job not allowed to run together as set to `forbid` plus misses `jobs` in general and the `startingDeadlineSeconds` is also in the matrix for failed jobs if it set to a value greater than zero.

The name of the `cronjob` will be used by `controller` plane to `.metadata.name` the pod running that job.
- imperative commands:
```bash
# get list of jobs
k get jobs
# get logs from job
k logs job/<jobname-5489455....>
# can patch job to suspend its executions
kubectl patch cronjob/<name_of_cronjob> -p '{"spec": {"suspend": true}}'
# exec in a cronjob pod 
kubectl exec --stdin --tty job/<pod_name_123...> -- sh
```

- example job yaml file
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello-junko
spec:
  # run every 5mn, kubernetes can't set per seconds schedule, so you need to set at least a minutes
  schedule: "*/5 * * * *"

  # can use time zone: see documentation
  # timeZone: "Etc/UTC"

  # this will tell how long before the job starts running when it is time for it run on the above `schedule` and misses that time
  # it will allow the job to run. eg. job have to run once a day, if it misses because for example `concurrency` is not allowed you can set this field to few hours in seconds to allow it to start anytime again between the missed schedule time and this `startingDeadlineSeconds` time (after it)
  # startingDeadlineSeconds: 10  # do not set it under 10 seconds as the job won't be scheduled as `cronjob controller` checks every 10 seconds

  # this can be `forbid` telling it to not run concurrently with other jobs or `Allow` the default which allow concurrency
  # another one is `Replace` would just run one job at a time so 'No Concurrency'
  # but when it can run would stop the running job to start a new one even if the previous is not completed yet. so would not wait for previous job to finish.
  # this field tales into consideration (when used as optional) the field `startingDeadlineSeconds`
  # concurrencyPolicy: Forbid

  # can also re-apply this Cronjob to patch it and suspend the job. has effect on subsequent job scheduled which will be suspended and not run
  # but the current running job will not be interupted
  # suspend: true

  # this indicated how much logs to keep. default to 3 for `success` one and to 1 for `failure` one.
  # can be set to `0` to not keep any logs. but better to put a number for easy debugging
  # successfulJobsHistoryLimit: 10
  # failedJobsHistoryLimit: 3

  # `backoffLimit` is how many times kubernetes will try to run the `cronjob` if attempts fail to run it before killing iti
  # backoffLimit: 10

  jobTemplate:
    spec:
      # can also use TTL which is going to to delete the job and logs
      # even if it didn't execute or executed not the full amount of times (success/failure) after this `TTL` time
      # so here no stale job staying on the cluster and prevents using manual `kubectl delete job <this cronjob>`
      # ttlSecondsAfterFinished: 600
      template:
        metadata:
          # can also label the cronjob
          labels:
            cron-job-location: shibuya
        spec:
          containers:
          - name: hello-junko
            image: busybox:1.36.1
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - date; echo Hello Junko! from Kubernetes Shibuya!
          restartPolicy: OnFailure

```

## so each job will be a pod so need to limit that
```bash
k get jobs -o wide
NAME                       COMPLETIONS   DURATION   AGE     CONTAINERS   IMAGES           SELECTOR
shibuya-cronjob-29087591   1/1           3s         3m32s   hello        busybox:1.36.1   batch.kubernetes.io/controller-uid=c81bf556-cd53-4ff0-902f-779cf250cce4
shibuya-cronjob-29087592   1/1           3s         2m32s   hello        busybox:1.36.1   batch.kubernetes.io/controller-uid=56136263-5925-4749-9e33-a51ee73b4be1
shibuya-cronjob-29087593   1/1           4s         92s     hello        busybox:1.36.1   batch.kubernetes.io/controller-uid=bb41dede-e96e-4996-aedd-06ff1b395be2
shibuya-cronjob-29087594   1/1           4s         32s     hello        busybox:1.36.1   batch.kubernetes.io/controller-uid=97596346-1dd5-4de1-adaf-a7d72e44e975
```

so might be interesting to use the optional field `ttlSecondsAfterFinished: <sometime>` so the pod will be deleted automatically after finishing and some `TTL` time.

## Helm way to install cronjob in cluster
source: (helm doc)[https://helm.sh/docs/intro/install/]
- easiest way with latest version of helm fetched and installed
```bash
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
```
- apt repo updated way
```bash
curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null
sudo apt-get install apt-transport-https --yes
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
sudo apt-get update
sudo apt-get install helm
```

- then create a helm chart, delete the templates present theri by default, and put your `cronjob` yaml file there.
```bash
helm create shibuya-cronjob
cd shibuya-cronjob
# delete all default templates
sudo rm -r templates/*
# create your cronjob yaml file
nano templates/my_cronjob_shibuya.yaml
# update the chart information
nano Chart.yaml
# inside
apiVersion: v2
name: shibuya-cronjob
description: “cronjob shibuya using Helm for versioning and easy change”
type: application
version: 1.1.0
# apply to cluster and you will have the `shibuya-cronjob.yaml` handled by `Helm` like for any applications Helm would do normally
helm install shibuya-cronjob .
# alternatively could even push this Helm chart to your repository on Helm or other
heml push...
```

## `schedule` part in cronjob explanation
source: (kubernetes doc schedule cronjob)[https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#schedule-syntax]
```markdown
# ┌─ minute (0 - 59)
# │ ┌─ hour (0 - 23)
# │ │ ┌─ day of the month (1 - 31)
# │ │ │ ┌─ month (1 - 12)
# │ │ │ │ ┌─ day of the week (0 - 6) (Sunday to Saturday) OR sun, mon, tue, wed, thu, fri, sat
# │ │ │ │ │
# │ │ │ │ │ 
# │ │ │ │ │
# * * * * *
```

## scenario 
use `Helm` and a cronjob to run every minutes to write a sentence in a volume that would be fetched by another pod, or maybe a cronjob to update a configmap so that we would see in internet browser that the message changes in the page. something simple like showing `date` with `hour` for example.

So here have decided to run an example using some concepts seen before, i could have used persisent volumes but will just use `hostPath` shared volume on a node, `affinity` to have the pod on a specific node and `cronjob` with also affinity to have access to that node volume. `nginx` hatml page would be changed with a message showing date and time of an event `every minutes`, a `ttlSecondsAfterFinished` option is used in the `cronjob` for the stale job to be deleted after a certain time.
- `cat templates/shibuya-cronjob.yaml`
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: shibuya-cronjob
spec:
  schedule: "*/1 * * * *"
  successfulJobsHistoryLimit: 7
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      ttlSecondsAfterFinished: 120
      template:
        spec:
          containers:
          - name: hello
            image: busybox:1.36.1
            #imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - echo "<h1 style='display:flex;flex-direction:row;align-items:center;justify-content:center;color:darkblue;'>Junko will be here on the $(date | awk '{print $2,$3}') at $(date | awk '{print $4}')</h1>" > "/tmp/index.html"
            # - echo "junko will be here on the $(date)" > "/tmp/index.html"
            volumeMounts:
            - name: shared-volume
              mountPath: "/tmp"
          restartPolicy: OnFailure
          volumes:
          - name: shared-volume
            hostPath:
              path: "/tmp"
              #type: DirectoryOrCreate

          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                - matchExpressions:
                  - key: location
                    operator: In
                    values:
                    - shibuya
```

- `cat templates/test_nginx.yaml`
```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: test-nginx
  name: test-nginx
spec:
  containers:
  - image: nginx
    name: test-nginx
    volumeMounts:
    - name: shared-volume
      mountPath: "/usr/share/nginx/html"

  volumes:
  - name: shared-volume
    hostPath:
      path: "/tmp"
      #type: DirectoryOrCreate

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: location
            operator: In
            values:
            - shibuya
```
______________________________________________________________________
# Next
- [ ] do those kubernetes concepts:
    - [x] Storage
    - [x] Backup and Recovery
    - [x] Resources Limits
    - [x] Admission Policy
    - [x] Cronjob, Jobs
    - [ ] Damonsets
    - [ ] Kubernetes Kustomize
    - [ ] Helm
- [ ] update the cluster versions until we reach 1.32 (we are at 1.27)
    so we will have to do same process several times and fix any compatibility issues along the way.
    need to check supported versions ranges for each kubeadm updated version
- [ ] create thsoe scripts in bash that will upgrade controller node and worker nodes
      and then create an ansible playbook having variables set ina file for containerd version and kubeadm version to upgrade nodes
      then create a rust app that would accept as input terminal variables the versions and start the ansible playbook
      -  probably need to use ansible until it works to automate one version upgrade 1.28 to 1.29 fully
      -  then create the rust application as it would be easier as we are sure that the playbook is not failing otherwise too much debugging layers.
