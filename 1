 Create Certificates For The Cluster
Go to source Kubernetes Documentation Which Has All Details: [Cert Creation Doc](https://kubernetes.io/docs/tasks/administer-cluster/certificates/)

## Kubeadm when initialized creates certificates automatically
- The certificates generated by kubeadm include:​
    - CA Certificates: These are the root certificates used to sign other certificates within the cluster.​
    - API Server Certificates: Used by the Kubernetes API server to establish secure communications.​
    - Kubelet Client Certificates: Used by the kubelet to authenticate to the API server.​
    - Etcd Certificates: Used for secure communication with the etcd key-value store.​

# KUBEADM ISSUE
- Kubelet not starting
- Certs are all expired for every components: `controller`, `api-server`, `scheduler`, `etcd`

1. Can renew certs using a simple command:
```bash
sudo kubeadm certs check-expiration
kubeadm certs renew all
# OR for specific ones
sudo kubeadm certs renew admin.conf
```

Which will renew the `admin.conf` embedded certs.

2. Embedded certs are base64 encoded
```bash
base64 -w 0 /etc/kubernetes/pki/ca.crt > ca.crt.base64
base64 -w 0 /etc/kubernetes/pki/admin.crt > admin.crt.base64
base64 -w 0 /etc/kubernetes/pki/admin.key > admin.key.base64
base64 -w 0 /etc/kubernetes/pki/apiserver-kubelet-client.crt > apiserver-kubelet-client.crt.base64
base64 -w 0 /etc/kubernetes/pki/apiserver-kubelet-client.key > apiserver-kubelet-client.key.base64
base64 -w 0 /etc/kubernetes/pki/front-proxy-client.crt > front-proxy-client.crt.base64
base64 -w 0 /etc/kubernetes/pki/front-proxy-client.key > front-proxy-client.key.base64
base64 -w 0 /etc/kubernetes/pki/apiserver-etcd-client.crt > apiserver-etcd-client.crt.base64
base64 -w 0 /etc/kubernetes/pki/apiserver-etcd-client.key > apiserver-etcd-client.key.base64
```

- can also manually convert those keys and past those in the configuration files:
eg:.
```bash
cat /etc/kubernetes/pki/apiserver-etcd-client.crt | base64
```

3. Cert can't be found for `admin.conf` in `../pki/..` folder:
Extract Keys from the `admin.conf` file for example to get the cert
- can also install `yq` which is a tool helping from the terminal to fetch `YAML` fields data, eg.:
```bash
sudo add-apt-repository ppa:rmescandon/yq
sudo apt update
sudo apt install yq -y
```
- then use it like that:
```bash
yq e '.users[0].user.client-certificate-data' /etc/kubernetes/admin.conf | base64 -d | sudo tee /etc/kubernetes/pki/admin.crt
```

## **Important**
- **Make Backups of certs and inital `YAML` files**

4. Documentation to understand certificates with some tables:
[Best Practices Kubernetes certificates](https://kubernetes.io/docs/setup/best-practices/certificates/)



# Controller Components Having a `.conf` File (present in `/etc/kubernetes/) Because Need To Authenticate To `Api-Server`

Note: Certs are base64 encoded and embedded directly within the admin.conf file. If needed to be extracted, decode necessary to get original cert back.

1. admin.conf
- Purpose: Provides administrative access to the Kubernetes API server, typically used by cluster administrators.​
- Default Location: /etc/kubernetes/admin.conf​
- Certificate Authority: /etc/kubernetes/pki/ca.crt​
- Client Certificate: Embedded within the admin.conf file under users.user.client-certificate-data​
- Client Key: Embedded within the admin.conf file under users.user.client-key-data​

2. controller-manager.conf
- Purpose: Used by the Kubernetes Controller Manager to authenticate with the API server.​
- Default Location: /etc/kubernetes/controller-manager.conf​
- Certificate Authority: /etc/kubernetes/pki/ca.crt​
- Client Certificate: Embedded within the controller-manager.conf file under users.user.client-certificate-data​
- Client Key: Embedded within the controller-manager.conf file under users.user.client-key-data​

3. scheduler.conf
- Purpose: Utilized by the Kubernetes Scheduler to authenticate with the API server.​
- Default Location: /etc/kubernetes/scheduler.conf​
- Certificate Authority: /etc/kubernetes/pki/ca.crt​
- Client Certificate: Embedded within the scheduler.conf file under users.user.client-certificate-data​
- Client Key: Embedded within the scheduler.conf file under users.user.client-key-data​
 
4. kubelet.conf
- Purpose: Allows the Kubelet agent, which runs on each node, to authenticate with the API server.​
devopscube.com
- Default Location: /etc/kubernetes/kubelet.conf​
- Certificate Authority: /etc/kubernetes/pki/ca.crt​
- Client Certificate: Typically located at /var/lib/kubelet/pki/kubelet-client-current.pem​
- Client Key: Typically located at /var/lib/kubelet/pki/kubelet-client-current.pem​



# Controller Components Without `.conf` file (present in `/etc/kubernetes/manifest/`) Because Need To Always Be Alive (Critical Components)

5. API Server (kube-apiserver):
- Purpose: The API server acts as the central management entity, validating and configuring data for API objects such as pods, services, and replication controllers.​
- Certificate Authority File: /etc/kubernetes/pki/ca.crt​
- Server Certificate and Key Files: /etc/kubernetes/pki/apiserver.crt and /etc/kubernetes/pki/apiserver.key​
- Client Certificates for etcd and kubelet:
  - etcd Client Certificate and Key Files: /etc/kubernetes/pki/apiserver-etcd-client.crt and /etc/kubernetes/pki/apiserver-etcd-client.key
    Description: Used by the API server to authenticate itself to the etcd server.
- kubelet Client Certificate and Key Files: /etc/kubernetes/pki/apiserver-kubelet-client.crt and /etc/kubernetes/pki/apiserver-kubelet-client.key
  Description: Used by the API server to authenticate itself to kubelets.

6. etcd:
- Purpose: etcd is a consistent and highly-available key-value store used as Kubernetes' backing store for all cluster data.​
- Certificate Authority Files: /etc/kubernetes/pki/etcd/ca.crt and /etc/kubernetes/pki/etcd/ca.key​
  Description: The root CA certificate and key for etcd, used to sign etcd server and peer certificates.​
- Server Certificate and Key Files: /etc/kubernetes/pki/etcd/server.crt and /etc/kubernetes/pki/etcd/server.key​
  Description: Used by etcd to serve secure (HTTPS) endpoints.​
- Peer Certificates Files: /etc/kubernetes/pki/etcd/peer.crt and /etc/kubernetes/pki/etcd/peer.key​
  Description: Used for secure communication between etcd peers in a cluster.​
- Client Certificates Files: /etc/kubernetes/pki/etcd/healthcheck-client.crt and /etc/kubernetes/pki/etcd/healthcheck-client.key​
  Description: Used by clients performing health checks on etcd.

# Folder Separation Of Concerns For Cluster-Wide Components and Node Components
## Folder For Cluster-Wide Components
- /etc/kubernetes/pki/ contains cluster-wide control plane certificates (API server, etcd, CA).
## Folder For Node Components
- /var/lib/kubelet/pki/ stores node-specific kubelet certificates.
Here for example the `Kubelet` client certificates will be stored as `Kubelet` on each node need a different key to authenticate to the `Api-Server`
**Here Unique `.pem` File As It Holds `crt` and `key`**: simplifies automatic renewal and storage (one file instead of two)

# `Admin.conf` and `Ca.crt`

## `Admin.conf`
- `Admin.conf`: is the same file that you are going to find in /home/<admin_user>/.kube/config
```bash
mkdir -p $HOME/.kube
sudo cp /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

## `Ca.crt` = **Root CA**
- `Ca.crt`: is the **authority** certificate for all the cluster controller components.
- Sign in all clients and server certificates.

`Admin.conf` is which is copied and changed permission to the `admin_user` home directory in `~/.kube/config`
is used by `kubectl` commands to authenticate to the cluster where `Api Server` will use the `Ca.crt` **Authority** to verify signature and permissions.

# Diagram of configs dependences on certs
(diagram)[https://excalidraw.com/#json=elokPxtHr6KkfK5rcWw0j,sOYYyGGT7QpDV1iBqvlfEg]


# MISSING KEYS, CERTS ETC....
## `sa.pub` file (/etc/kubernetes/pki/sa.pub) Missing
It is while checking into details the `yaml` files present in `/etc/kubernetes/manifest/` folder that I realized that
the `kube-apiserver.yaml` file is using also `sa.pub` that verifies signature service account tokens.
To recreate it I had to run this command:
```bash
openssl rsa -in /etc/kubernetes/pki/sa.key -pubout -out /etc/kubernetes/pki/sa.pub
```

## `controller-manager.crt and .key` (/etc/kubernetes/pki/..) Missing
- Need to recreate the keys and then to encode base64 and to update the `controller-manager.conf` file with those key/crt in the `user.client` section of the file

1. Generate a New Private Key
```bash
sudo openssl genrsa -out /etc/kubernetes/pki/controller-manager.key 2048
```

2. Create a Certificate Signing Request (CSR)
```bash
sudo openssl req -new -key /etc/kubernetes/pki/controller-manager.key \
-out /etc/kubernetes/pki/controller-manager.csr \
-subj "/CN=system:kube-controller-manager/O=system:kube-controller-manager"
```

3.  Sign the Certificate Using the Kubernetes CA
```bash
sudo openssl x509 -req -in /etc/kubernetes/pki/controller-manager.csr \
  -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key \
  -CAcreateserial -out /etc/kubernetes/pki/controller-manager.crt \
  -days 365 -sha256
Outputs:
Certificate request self-signature ok
subject=CN = system:kube-controller-manager, O = system:kube-controller-manager
```

4. Verify the Certificate
```bash
sudo openssl x509 -in /etc/kubernetes/pki/controller-manager.crt -noout -text | grep -E 'Subject:|Issuer:'
        Issuer: CN = kubernetes
        Subject: CN = system:kube-controller-manager, O = system:kube-controller-manager
Outputs:
Issuer: CN = kubernetes
        Subject: CN = system:kube-controller-manager, O = system:kube-controller-manager
```

## `admin.crt and .key` (/etc/kubernetes/pki/..) Missing
- need to recreate those following same steps as above but just changing names so:
  - Generate key
  - Create CSR
  - Sign using Kubernetes `ca.crt` and `ca.key`
  - Verify Certificate (just checking...)
```bash
sudo openssl genrsa -out /etc/kubernetes/pki/admin.key 2048
sudo openssl req -new -key /etc/kubernetes/pki/admin.key \
  -out /etc/kubernetes/pki/admin.csr \
  -subj "/CN=kubernetes-admin/O=system:masters"
sudo openssl req -new -key /etc/kubernetes/pki/admin.key \
  -out /etc/kubernetes/pki/admin.csr \
  -subj "/CN=kubernetes-admin/O=system:masters"
sudo openssl x509 -req -in /etc/kubernetes/pki/admin.csr \
  -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key \
  -CAcreateserial -out /etc/kubernetes/pki/admin.crt \
  -days 365 -sha256
Outputs:
Certificate request self-signature ok
subject=CN = kubernetes-admin, O = system:masters
sudo openssl x509 -in /etc/kubernetes/pki/admin.crt -noout -text | grep -E 'Subject:|Issuer:'
Outputs:
Issuer: CN = kubernetes
Subject: CN = kubernetes-admin, O = system:masters

```


# How to recreate `.pem` file that `kubelet` need and which is located at `/var/lib/kubelet/pki/` ?
Normally Kubernetes create it auromatically but you can use a simple command to recreate it:
- just use the `kubeadm` command:
```bash
sudo kubeadm certs renew all
sudo systemctl restart kubelet
```

OR manually (but i don't see why we do it manually when it should present in each node and it is a different .pem content for each node)
```bash
sudo openssl genrsa -out /var/lib/kubelet/pki/kubelet-client.key 2048
sudo openssl req -new -key /var/lib/kubelet/pki/kubelet-client.key \
  -out /var/lib/kubelet/pki/kubelet-client.csr \
  -subj "/CN=system:node:controller.creditizens.net/O=system:nodes"
openssl x509 -req -in /var/lib/kubelet/pki/kubelet-client.csr \
  -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key \
  -CAcreateserial -out /var/lib/kubelet/pki/kubelet-client-current.pem \
  -days 365 -sha256
sudo systemctl restart kubelet
```

**Important**:
- The `Kubelet` `.pem` file located at `/var/kubernetes/pki/` is unique in each nodes, therefore, can be manually created only on the `controller` node.
- If a worker node loses or have issues with that file, it needs to request for a new one as it got the file from when it had joined the cluster:
```bash
sudo kubeadm join --token <your-token> --discovery-token-ca-cert-hash sha256:<hash>
```
- If a worker node need a new `.pem` file it need to approve pending `csr`:
```bash
kubectl get csr
kubectl certificate approve <csr-name>
# if too many <csr-name> in `pending` mode and too many to do it manually just run this command
# which will" get all csr | filter pending one | run the cert approval for each of those
kubectl get csr | awk '/Pending/ {print $1}' | xargs kubectl certificate approve
```

# `/var/lib/kubelet/pki/` `.pem` file symlink is pointing to outdated `.pem` file
The `...current.pem` file is pointing to an outdated one, so we need to make a backup of the outdated one and use the sigle command to renew certs, see issue when `ls -la` folder:
```bash
sudo ls -la /var/lib/kubelet/pki
total 28
drwxr-xr-x 2 root root 4096 mars  12 20:52 .
drwx------ 8 root root 4096 juil.  7  2023 ..
-rw------- 1 root root 1062 mars  12 20:53 kubelet-client-2023-07-07-09-39-43.pem
-rw-r--r-- 1 root root  960 mars  12 20:53 kubelet-client.csr
lrwxrwxrwx 1 root root   59 juil.  7  2023 kubelet-client-current.pem -> /var/lib/kubelet/pki/kubelet-client-2023-07-07-09-39-43.pem
-rw------- 1 root root 1704 mars  12 20:52 kubelet-client.key
-rw-r--r-- 1 root root 2371 juil.  7  2023 kubelet.crt
-rw------- 1 root root 1675 juil.  7  2023 kubelet.key
```
**Fix:**
- we backup the outdated .epm file or just delete it
```bash
sudo mv kubelet-client-2023-07-07-09-39-43.pem BAK_kubelet-client-2023-07-07-09-39-43.pem
# OR just : sudo rm -rf kubelet-client-2023-07-07-09-39-43.pem
```
- if new file present: we can just create new symlink
```bash
sudo ln -sf /var/lib/kubelet/pki/kubelet-client-<new-date>.pem /var/lib/kubelet/pki/kubelet-client-current.pem
```
- if not what is my case (the best ever), we create a new one from scratch:
```bash
sudo kubeadm certs renew kubelet
sudo ln -sf /var/lib/kubelet/pki/kubelet-client.pem /var/lib/kubelet/pki/kubelet-client-current.pem
sudo systemctl restart kubelet
```
- i have finally deleted everything from thi `/var/lib/kubelet/pki/` folder to recreate all certs so that everything is renewed and good for learning
```bash
sudo rm -rf /var/lib/kubelet/pki/*
```
so we recreate all like that:
```bash
# recreate key
sudo openssl genrsa -out /var/lib/kubelet/pki/kubelet-client.key 2048
# check
sudo ls -lah /var/lib/kubelet/pki/
Outputs:
total 20K
drwxr-xr-x 2 root root 4,0K mars  12 21:23 .
drwx------ 8 root root 4,0K juil.  7  2023 ..
-rw------- 1 root root 1,7K mars  12 21:30 kubelet-client.key
-rw-r--r-- 1 root root 2,4K juil.  7  2023 kubelet.crt
-rw------- 1 root root 1,7K juil.  7  2023 kubelet.key
# create csr
sudo openssl req -new -key /var/lib/kubelet/pki/kubelet-client.key \
    -out /var/lib/kubelet/pki/kubelet-client.csr \
    -subj "/CN=system:node:controller.creditizens.net/O=system:nodes"
# verify the csr
sudo openssl req -in /var/lib/kubelet/pki/kubelet-client.csr -noout -text
Outputs:
Certificate Request:
    Data:
        Version: 1 (0x0)
        Subject: CN = system:node:controller.creditizens.net, O = system:nodes
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                Public-Key: (2048 bit)
                Modulus:
                    00:e6:d6:13:89:65:c9:61:c4:bc:7f:bd:3d:0e:99:
                    bb:50:9f:6f:74:48:31:20:ef:12:de:92:a6:b1:b6:
                    31:56:6f:fb:99:15:2f:b8:aa:4b:a1:d9:6d:ec:a3:
                    95:7f:45:11:6e:0b:8e:7f:2b:b0:3d:80:3d:7d:f7:
                    ce:04:61:c5:f7:79:06:d8:40:ea:7a:d2:b4:e2:c9:
                    cb:5d:84:2e:98:f5:f0:e2:9d:d8:89:87:3f:77:74:
                    ec:03:ec:25:1b:da:82:eb:d0:2a:57:42:77:d7:b0:
                    a1:88:2b:e5:43:b4:25:01:44:ec:4b:05:88:34:10:
                    b6:f6:58:9f:3e:f8:e5:73:e3:1b:6c:7b:04:14:a8:
                    27:14:36:74:f3:63:67:56:d9:d1:c6:05:19:98:18:
                    0b:fd:ea:b6:69:12:4c:99:79:aa:58:b1:5b:b5:3b:
                    1f:10:c8:47:7a:8e:6a:79:49:8b:52:8f:b8:b4:ef:
                    5f:1c:02:2f:97:ce:35:c1:3b:db:09:f6:9a:61:ef:
                    e0:88:d8:04:d6:cb:76:33:77:95:aa:19:3b:ff:80:
                    56:ed:c0:01:a6:f9:07:2b:78:4f:fa:89:7f:ab:10:
                    41:4e:d6:67:c8:65:b4:2c:a2:31:c4:67:a7:3f:56:
                    81:90:5f:e2:d4:e4:84:18:44:63:43:70:23:02:b5:
                    58:9b
                Exponent: 65537 (0x10001)
        Attributes:
            (none)
            Requested Extensions:
    Signature Algorithm: sha256WithRSAEncryption
    Signature Value:
        78:b3:b9:9d:10:a3:da:c5:c4:b5:87:e1:5f:fd:83:d7:21:27:
        f0:d7:fd:48:c7:f8:b7:e0:70:b7:de:66:00:71:2d:72:bb:e3:
        6f:25:0f:b7:f8:04:47:94:5f:5b:7a:4a:00:12:ea:b8:b7:54:
        5a:87:76:5a:07:79:68:8b:8e:d5:f8:2c:50:f9:cf:c9:97:6f:
        71:20:12:48:4a:c9:66:0d:a0:ec:41:fe:67:46:5b:9d:63:a7:
        6b:85:c1:80:92:a6:82:43:f3:0f:67:c2:08:97:05:5b:7d:f2:
        95:d1:93:1b:3f:d8:60:9f:da:3f:50:32:b3:46:3a:dd:31:58:
        c0:26:7f:f9:1f:77:ea:f5:f7:94:cb:bb:b2:4a:d8:17:02:a1:
        13:b1:0a:8c:84:5c:dd:af:df:d6:f5:e5:81:91:4f:f7:00:d6:
        58:21:33:8e:70:ce:4b:cc:2a:5a:83:ed:ea:cb:30:b3:8f:31:
        85:f1:f8:b6:8c:fb:66:0a:9b:b2:18:29:b6:7d:57:9d:52:b4:
        9e:66:f2:eb:37:8b:91:49:15:17:60:df:84:bd:5e:dd:1e:1b:
        38:4a:ec:ec:eb:5c:cf:a2:c3:b5:0c:8b:7a:a7:03:68:7e:6d:
        7e:d9:c4:ab:26:e8:a7:40:ea:67:0d:5d:7f:30:15:8e:a9:91:
        e5:e3:7f:6b
# sign the csr with Kubernetes CA
sudo openssl x509 -req -in /var/lib/kubelet/pki/kubelet-client.csr \
    -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key \
    -CAcreateserial -out /var/lib/kubelet/pki/kubelet-client-current.pem \
    -days 365
Outputs:
Certificate request self-signature ok
subject=CN = system:node:controller.creditizens.net, O = system:nodes
# change permissions
sudo chown root:root /var/lib/kubelet/pki/kubelet-client-current.pem
sudo chmod 600 /var/lib/kubelet/pki/kubelet-client-current.pem
# check cert validity
sudo openssl x509 -in /var/lib/kubelet/pki/kubelet-client-current.pem -noout -text | grep "validity"
# fix symlink
sudo ln -sfn /var/lib/kubelet/pki/kubelet-client-current.pem /var/lib/kubelet/pki/kubelet-client.pem
# check symlink
sudo ls -la /var/lib/kubelet/pki/
Outpus:
total 28
drwxr-xr-x 2 root root 4096 mars  12 21:35 .
drwx------ 8 root root 4096 juil.  7  2023 ..
-rw-r--r-- 1 root root  960 mars  12 21:31 kubelet-client.csr
-rw-r--r-- 1 root root 1062 mars  12 21:33 kubelet-client-current.pem
-rw------- 1 root root 1704 mars  12 21:30 kubelet-client.key
lrwxrwxrwx 1 root root   47 mars  12 21:35 kubelet-client.pem -> /var/lib/kubelet/pki/kubelet-client-current.pem
-rw-r--r-- 1 root root 2371 juil.  7  2023 kubelet.crt
-rw------- 1 root root 1675 juil.  7  2023 kubelet.key
# create missing `kubelet.crt` and `kubelet.key`
sudo openssl genrsa -out /var/lib/kubelet/pki/kubelet.key 2048
sudo openssl req -new -key /var/lib/kubelet/pki/kubelet.key -out /var/lib/kubelet/pki/kubelet.csr \
    -subj "/CN=kubelet"
sudo openssl x509 -req -in /var/lib/kubelet/pki/kubelet.csr \
    -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key \
    -CAcreateserial -out /var/lib/kubelet/pki/kubelet.crt -days 365
sudo chmod 600 /var/lib/kubelet/pki/kubelet.key
sudo chmod 644 /var/lib/kubelet/pki/kubelet.crt
# check
sudo openssl x509 -in /var/lib/kubelet/pki/kubelet.crt -noout -text
# restart and check kubelet service
sudo systemctl restart kubelet
sudo systemctl status kubelet
```

# SOLUTION FOR KUBELET .PEM CERT/KEY RENEWAL AND PRESENCE
- the first issue was that certificates were outdated, therefore, i have changed them all
- next issue was that the `.pem` file of kubelet wasn't holding `crt` and `key` but only the `key`: 
    - therefore, i have delete those files from the `/var/lib/kubelet/pki/` and recreated everything from scratch
      but changed the process by manually adding the two keys in the `.pem` files 
- lesson need a good understanding of all the files and permissions and where those are located, their config files. 
    That is why I have made the detailed diagram that shows where are the leys, certs and configs and used arrows to show where they pointing to.
- After when `kubelet` was back i had to wait a bit before getting the `kubectl` command back and the visibility on `nodes`, `pods` etc...
- terminal output review of what has been done for the last step to fix it:
```bash
# stop kubelet (not need it wouldn't start LOL)
sudo systemctl stop kubelet
# Remove Incorrect Files
sudo rm -rf /var/lib/kubelet/pki/kubelet-client*.pem
sudo rm -rf /var/lib/kubelet/pki/kubelet.crt /var/lib/kubelet/pki/kubelet.key
sudo rm -rf /var/lib/kubelet/pki/kubelet-client.csr
'''
- Only this left in the folder but can empty it fully and recreate all keys:
sudo ls -la /var/lib/kubelet/pki/
total 20
drwxr-xr-x 2 root root 4096 mars  12 23:24 .
drwx------ 8 root root 4096 juil.  7  2023 ..
-rw-r--r-- 1 root root  960 mars  12 23:24 kubelet-client.csr
-rw------- 1 root root 1704 mars  12 23:14 kubelet-client.key
-rw-r--r-- 1 root root  887 mars  12 21:37 kubelet.csr
'''

'''
If wanted to recreate all kubelet.key, kubelet.csr, kubelet.crt can run:
sudo openssl genrsa -out /var/lib/kubelet/pki/kubelet.key 2048
sudo openssl req -new -key /var/lib/kubelet/pki/kubelet.key -out /var/lib/kubelet/pki/kubelet.csr \
    -subj "/CN=kubelet"
sudo openssl x509 -req -in /var/lib/kubelet/pki/kubelet.csr \
    -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key \
    -CAcreateserial -out /var/lib/kubelet/pki/kubelet.crt -days 365
sudo chmod 600 /var/lib/kubelet/pki/kubelet.key
sudo chmod 644 /var/lib/kubelet/pki/kubelet.crt
'''

# Generate a New CSR (Certificate Signing Request)
sudo openssl genrsa -out /var/lib/kubelet/pki/kubelet-client.key 2048
sudo openssl req -new -key /var/lib/kubelet/pki/kubelet-client.key \
-out /var/lib/kubelet/pki/kubelet-client.csr \
-subj "/CN=system:node:controller.creditizens.net/O=system:nodes"
# Sign the CSR to Generate the Correct Certificate
sudo openssl x509 -req -in /var/lib/kubelet/pki/kubelet-client.csr \
-CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key \
-CAcreateserial -out /var/lib/kubelet/pki/kubelet-client.crt \
-days 365
# Create the Correct kubelet-client-current.pem
# This file must contain both the private key and the certificate.
# that is the super command that manually added `crt` and `key` in the `.pem` file
sudo cat /var/lib/kubelet/pki/kubelet-client.key /var/lib/kubelet/pki/kubelet-client.crt | sudo tee /var/lib/kubelet/pki/kubelet-client-current.pem
# Fix Permissions
sudo chown root:root /var/lib/kubelet/pki/kubelet-client-current.pem
sudo chmod 600 /var/lib/kubelet/pki/kubelet-client-current.pem
# restart kubelet
sudo systemctl restart kubelet
# check status (should be fine)
sudo systemctl status kubelet
# check journal eventually
sudo journalctl -u kubelet --no-pager --lines=50
# wait a bit and then start running kubectl commands
kubectl get pods
kubectl get nodes
...etc...
```

# For worker nodes how to fix the issue of certificate of kubelet expired
- make sure you have an ssh connection between worker node and controller node as we are going to more files to be signed there as CA authority `.key` file is only there
```bash
# on worker and controller
sudo apt install ssh
sudo systemctl status ssh
ssh-keygen
# then from controller to worker copy the key so that controller is part of the known hosts
ssh-copy-id -i ~/.ssh/id_rsa.pub creditizens@node1.creditizens.net
```
- on the worker node you can delete everything from the folder `/var/lib/kubelet/pki/` then restart `kubelet service`: `sudo systemctl restart kubelet`.
  Then this will automatically populate the folder `var/lib/kubelet/pki/` with new `kubelet.crt` and `kubelet.key` that you are going to use for next command:
```bash
sudo openssl req -new -key /var/lib/kubelet/pki/kubelet.key \
    -out /var/lib/kubelet/pki/kubelet-client.csr \
    -subj "/CN=system:node:node1-creditizens.net/O=system:nodes"
```

- Copy the CSR to the Controller Node
Since the worker node doesn’t have /etc/kubernetes/pki/ca.key, we must sign the CSR on the controller.
Run this command on the worker node to transfer the CSR file:
```bash
scp /var/lib/kubelet/pki/kubelet-client.csr creditizens@controller.creditizens.net>:/tmp/
```

- Sign the CSR on the Controller Node
Now, on the controller node, sign the CSR using the CA:
```bash
sudo openssl x509 -req -in /tmp/kubelet-client.csr \
    -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key \
    -CAcreateserial -out /tmp/kubelet-client.crt \
    -days 365
```
This signs the CSR, producing kubelet-client.crt, which must be combined with the key.

- Copy the Signed Certificate Back to the Worker Node
On the controller, transfer the signed certificate back to the worker:
```bash
scp /tmp/kubelet-client.crt creditizens@node1.creditizens.net:/tmp/
```

- Now, on the worker node, move it into place:
```bash
sudo mv /tmp/kubelet-client.crt /var/lib/kubelet/pki/kubelet-client.crt
```

- Create the Correct kubelet-client-current.pem
The kubelet-client-current.pem file must contain both the certificate and the private key.
Run this command on the worker node to combine them into a proper PEM file:
```bash
sudo cat /var/lib/kubelet/pki/kubelet-client.crt /var/lib/kubelet/pki/kubelet.key | sudo tee /var/lib/kubelet/pki/kubelet-client-current.pem
```

-  Set the Correct Permissions
```bash
sudo chown root:root /var/lib/kubelet/pki/kubelet-client-current.pem
sudo chmod 600 /var/lib/kubelet/pki/kubelet-client-current.pem
```

- Restart Kubelet
```bash
sudo systemctl restart kubelet
sudo systemctl status kubelet
```


# ISSUE WITH SCHEDULER CRASHBACKLOOP BECAUSE OF CERTIFICATE EXPIRED
- state of Scheduler in Cluster before fix
```bash
# kubectl get pods -n kube-system
kube-scheduler-controller.creditizens.net            0/1     CrashLoopBackOff   57 (2m27s ago)   615d
# logs
kubectl logs kube-scheduler-controller.creditizens.net -n kube-system
I0313 17:04:47.970993       1 serving.go:348] Generated self-signed cert in-memory
E0313 17:04:47.972942       1 run.go:74] "command failed" err="error loading config file \"/etc/kubernetes/scheduler.conf\": illegal base64 data at input byte 25"
```

- Check authority certificate expiry date
```bash
sudo openssl x509 -in /etc/kubernetes/pki/front-proxy-ca.crt -noout -dates
Outputs:
notBefore=Jul  7 07:39:41 2023 GMT
notAfter=Jul  4 07:39:41 2033 GMT  STILL VALID!
```

- Check client certiifcate expiry date
```bash
sudo openssl x509 -in /etc/kubernetes/pki/front-proxy-client.crt -noout -dates
Outputs:
notBefore=Jul  7 07:39:41 2023 GMT
notAfter=Mar 10 18:21:00 2026 GMT EXPIRED NEED CREATE A NEW ONE!
```

### HOW TO CREATE A NEW ONE
```bash
# Delete scheduler client keys
sudo rm -rf /etc/kubernetes/pki/front-proxy-client.crt /etc/kubernetes/pki/front-proxy-client.key
```
```bash
# generate a new key
sudo openssl genrsa -out /etc/kubernetes/pki/front-proxy-client.key 2048
```
```bash
# create certificate signing request
sudo openssl req -new -key /etc/kubernetes/pki/front-proxy-client.key -subj "/CN=front-proxy-client" -out /etc/kubernetes/pki/front-proxy-client.csr
```
```bash
# sign the key with the authority certificate
sudo openssl x509 -req -in /etc/kubernetes/pki/front-proxy-client.csr -CA /etc/kubernetes/pki/front-proxy-ca.crt -CAkey /etc/kubernetes/pki/front-proxy-ca.key -CAcreateserial -out /etc/kubernetes/pki/front-proxy-client.crt -days 365
```

# SCHEDULER IS RUNNING FINE NOW BUT LOGS SHOW TLS ISSUES

- Check Authority CA expiry date:
```bash
openssl x509 -in /etc/kubernetes/pki/ca.crt -noout -dates
Outputs:
notBefore=Jul  7 07:39:41 2023 GMT
notAfter=Jul  4 07:39:41 2033 GMT
```

- Copy the Authority CA cert to `/usr/local/share...` and update certificates:
```bash
sudo cp -f /etc/kubernetes/pki/ca.crt /usr/:local/share/ca-certificates/kubernetes.crt
# for cert to be trusted server wide this need to be done on each `controller` nodes
sudo update-ca-certificates
```

# TO DELETE ALL PENDING OR RUNNING POD WITH ONE COMMAND
- Pending
```bash
kubectl delete pods -n kube-system --field-selector=status.phase=Pending
```
- Running
```bash
kubectl delete pods -n kube-system --field-selector=status.phase=Running
```

____________________________________________________________________________

After all of those struggles finally found the right way to do it:

# Utilities command to check cert expiry and more
- check certificates expiry dates
```bash
(sudo) openssl x509 -in <crt_file_path> -noout -dates
```
- check journal of a specify service
```bash
journalctl -u <service_name> -n <number_of_line_(end of file)_logs> --no-pager
```

# On All Worker Nodes:
Those files will be recreated at the end when we re-join worker to the cluster

1. delete all keys and config files
```bash
# maybe need to do those one by one manually and make sure to do the symlinked file first before it linked file
# otherwise you will need to create a dummy one until you delete that file and then delete your dummy file
sudo rm -rf /var/lib/kubelet/pki/*
# delete ca.crt and kubelet config
sudo rm -rf /etc/kubernetes/pki/ca/crt
sudo rm -rf /etc/kubernetes/kubelet.conf
```

# On the Controller Node
We used `kubeadm` to create the cluster so let's use it's utilities commands to renew all certs and config files
We will also delete all files that are not needed but needs to be regenreated

1. Remove expired kubelet certificates/config
```bash
# do it manually for all files in the folder and delete symlinked file (the file with `->`) before the other
sudo rm -f /var/lib/kubelet/pki/*
sudo rm -f /etc/kubernetes/kubelet.conf
```

2. Recreate kubelet.conf for control-plane
```bash
sudo kubeadm init phase kubeconfig all
# if `kubelet.conf` is not recreated use the following command to target it alone (it will recreate it)
sudo kubeadm init phase kubeconfig kubelet
```

3. Ensure rotation enabled (should already be done)
```
# ensure rotateCertificates: true
sudo nano /var/lib/kubelet/config.yaml
```

4. Restart kubelet
```
sudo systemctl restart kubelet
```

5. Check kubelet and cluster status
```bash
sudo systemctl status kubelet
kubectl get nodes
```

Now `kubectl` command should work and controller node should appear fine
but the other `worker` nodes should be `NotReady`.

It is time now to regenerate a token to join the cluster again
which will recreate certs and config files in `worker` nodes.


6. create the token that will be applied on worker node to join the cluster
```bash
kubeadm token create --print-join-command
```

# On Each Worker Node
7. past the command to join the cluster
    after having made sure that you have deleted all `.crt`, `.key` and `.pem`
    from `/var/lib/kubelet/pki/` and `/etc/kubernetes/pki/`
    and `.conf` file from `/etc/kubernetes/` folders
```bash
sudo kubeadm token create --print-join-command
```

8. restart kubelet and check it (accessory can also restart containerd but I didn't need myselfu)
```bash
sudo systemctl restart kubelet
sudo systemctl status kubelet
```

# Check The Controller Node
9. check that `workers` are nw back to work with status `Ready`
```bash
kubectl get nodes -o wide
```

10. Test a deployment and scale it and expose it and check if you see the app in browser, then tear down
```bash
# create namesapce
kubectl create ns nginx
# create deplyment
kubectl create deployment test-nginx -ns nginx --image=nginx
# scale deplyment
kubectl scale deployment test-nginx  -ns nginx --replicas=3
# check deployment and pods
kubectl get deployments test-nginx -ns nginx
kubectl get pods -l app=test-nginx -ns nginx
# expose deplyment through service creation for it
kubectl expose deployment test-nginx -ns nginx --port=80 --type=NodePort
kubectl get svc test-nginx -ns nginx
# can curl or just url in the browser the port is the one on the `right side 80:<port_for_brower>`
curl http://localhost:31234
# tear all down
kubectl delete svc test-nginx -ns nginx
kubectl delete deployment test-nginx -ns nginx
kubectl delete ns nginx
```

________________________________________________________________________________________________________________
# cNext
- [ ] update the cluster versions until we reach 1.32 (we are at 1.27)
    so we will have to do same process several times and fix any compatibility issues along the way.
    need to check supported versions ranges for each kubeadm updated version


# Take Snapshot of ETCD (for Safety)

## 1. Get the etcd Pod Name
```bash
kubectl get pods -n kube-system | grep etcd
Outputs:
etcd-controller-01   1/1   Running   0   1d
```

## 2. Create a Snapshot of etcd
```bash
sudo ETCDCTL_API=3 etcdctl snapshot save /var/lib/etcd/etcd-snapshot-$(date +%F-%T).db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key
```
This saves a snapshot with a timestamped filename in /var/lib/etcd/.
You can change the path if needed.

## 3. Verify the Snapshot Integrity
```bash
sudo ETCDCTL_API=3 etcdctl snapshot status /var/lib/etcd/etcd-snapshot-YYYY-MM-DD-HH-MM-SS.db
```

## 4. Etra: Restoring etcd from Snapshot (if Needed)
```bash
sudo ETCDCTL_API=3 etcdctl snapshot restore /var/lib/etcd/etcd-snapshot-YYYY-MM-DD-HH-MM-SS.db \
  --data-dir /var/lib/etcd-new
```
**Then, update the etcd manifest in `/etc/kubernetes/manifests/etcd.yaml` to point to `/var/lib/etcd-new`, and restart the control plane.**

# Uprgade Cluster:
- **[source: Kubernetes Documentation](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/#:~:text=During%20upgrade%20kubeadm%20writes%20the,etc%2Fkubernetes%2Ftmp)**
###  **Rules:**
- kubeadm cluster updates cn be done onl one minor version at a time 1.27 -> 1.28 -> 1.29...
- kubelet and containerd versions cn e max 3 versions older see documentation to see their corresponding versions but it is recommended to have those not more than on version older
- Need to do controller nodes first one at a time
- Need to to do the worker nodes one at a time
- Need for sure to know the architecture of taints/toleration/resource limits to make sure for seamless workload move to other nodes.
- Need also to see upgrades matching of Calico (but used the latest available the free one)
- Need to Backup etcd even if kubeadm doesn it automaicall placing backups to `/etc/kubernetes/tmp`
- Need to make sure version are `unhold` then upgrade and `hold` thse back for verions to not upgrade and have full control on stable cluster state 


**Important:** For `1` to `3` you can do the upgrade and then cordon and drain or the otherway around 
    as new state application to the cluster is done only with the command of `4` of `4'`

## 1. Cordon Node to Separa 
```bash
kubectl cordon <controller-node-name OR worker-node-name>
```

## 2. Drain Node: Make Node Unschedulable To prepare For Maintenance
```bash
kubectl drain <node-to-drain> --ignore-daemonsets --delete-emptydir-data
```

## 3. Upgrade Kubeadm to next minor version 1.27 -> 1.28
- make sure you have the repo in `apt` if not install it:
```bash
sudo apt update && sudo apt install -y curl apt-transport-https
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /usr/share/keyrings/kubernetes-archive-keyring.gpg
echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
# check the versions
kubeadm version
kubectl version
kubelet --version
containerd --version
# then run next command with the right version:
`sudo apt install -y kubeadm=1.28.10-00`
```
# get rid of deprecated kubernetes repo list
sudo rm /etc/apt/sources.list.d/kubernetes.list

# disable swap
sudo apt update
sudo apt install -y curl apt-transport-https
# get the signing key
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /usr/share/keyrings/kubernetes-archive-keyring.gpg
# add kubernetes apt repo
echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
# update package list
sudo apt update

# on all nodes disable swap and add kernel settings
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

# set up some kernel configs
sudo tee /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF

# load necessary kernel modules
sudo modprobe overlay
sudo modprobe br_netfilter
sudo tee /etc/sysctl.d/kubernetes.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF
Outputs:
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1


# reload changes
sudo sysctl --system
# check versions available to do the update for kubeadm kubelet and kubectl
sudo apt-cache madison kubeadm
sudo apt-cache madison kubelet
sudo apt-cache madison kubectl
```bash
sudo apt-mark unhold kubeadm kubelet kubectl && \
sudo apt-get update && \
sudo apt-get install -y kubeadm=1.28.15-1.1 kubelet=1.28.15-1.1 kebectl=1.28.15-1.1 && \
sudo apt-mark hold kubeadm kubelet kubectl
```

## . Install Compatible Version of Containerd (Optional but better have it updated even if it is Ok for few years...)
```bash
sudo apt-get install -y containerd.io=<required-version>
sudo apt install containerd.io=1.7.25-1
sudo systemctl restart containerd
```

## 4. Verify Upgrade Plan and Ugrade (Only in the first Control Plane: other ones are going to pick it up)
This is for the first control plane node only
```bash
# if having only one control node need to uncordon it  and restart kubelet and containerd
# to have the node `Ready` otherwise you won't be able to upgrade
kubectl uncorn controller.creditizens.net
sudo systemctl restart kubelet containerd
sudo kubeadm upgrade plan
sudo kubeadm upgrade apply v1.28.x
```

## 4'. Upgrade Other Control Planes (Optional if more than one control plane)
This is after having ugraded the first control plane and the other ones don't use `apply` but use `upgrade node` instead
Also no need to `uprgade plan` in those controller nodes.
```bash
sudo kubeadm upgrade node
```



## . Restart Kubelet
```bash
sudo systemctl daemon-reload
sudo systemctl restart kubelet
```

## . Bring Node Back Online
```bash
kubectl uncordon <node-to-uncordon>
```

## . Optionally Check that critical add-ons (CoreDNS, kube-proxy) are running the updated versions
```bash
kubectl get daemonset kube-proxy -n kube-system -o=jsonpath='{.spec.template.spec.containers[0].image}'


_________________________________________________

For worker nodes
- on controller node
kubectl cordon <controller-node-name OR worker-node-name>
```## 2. Drain Node: Make Node Unschedulable To prepare For Maintenance
```bash
kubectl drain <node-to-drain> --ignore-daemonsets --delete-emptydir-data

- on worker node
sudo apt update && sudo apt install -y curl apt-transport-https
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /usr/share/keyrings/kubernetes-archive-keyring.gpg
echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update -y


# on all nodes disable swap and add kernel settings
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

# set up some kernel configs
sudo tee /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF

# load necessary kernel modules
sudo modprobe overlay
sudo modprobe br_netfilter
sudo tee /etc/sysctl.d/kubernetes.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF
Outputs:
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1

sudo sysctl --system

kubeadm version
kubectl version
kubelet --version
containerd --version

# just check if version is there otherwise error with signing key or repo in `/etc/source.list.d/kubernetes.list`
sudo apt-cache madison kubeadm
sudo apt-cache madison kubelet
sudo apt-cache madison kubectl

sudo apt-mark unhold kubeadm kubectl kubelet
sudo apt install kubeadm=1.28.15-1.1 kubectl=1.28.15-1.1 kubelet=1.28.15-1.1 -y
sudo apt update

containerd --version

# optional trick to get the terminal show you the true version to get but from `containerd.io=<version>`
sudo apt install containerd=1.7
Outputs:
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
Package containerd is a virtual package provided by:
  containerd.io 1.7.25-1
You should explicitly select one to install.

# othersize just install it direcly, this is done having checked the version matching at: [containerd.io doc](https://containerd.io/releases/#:~:text=Kubernetes%20Version%20containerd%20Version%20CRI,36%2Bv1)
sudo apt install containerd.io=1.7.25-1

containerd --version
kubelet --version
Kubernetes v1.28.15
kubectl version
kubeadm version

# perform the upgrade
kubeadm upgrade node

sudo systemctl daemon-reload
sudo systemctl restart kubelet
sudo systemctl restart kubelet containerd
sudo systemctl status kubelet containerd
```

## . Restart Kubelet
```bash
sudo systemctl daemon-reload
sudo systemctl restart kubelet


- on controller node
```bash
kubectl uncordon <node-to-uncordon>
```

___________________________________________________________________________________________

# Kubernetes Plugins 

External utilities can be installed and used to interact with Kubernetes.
The easiest way to manage those is by installing `Krew` which will be used in combinaison of `kubectl`.

### install `krew`
source: [Krew Install](https://krew.sigs.k8s.io/docs/user-guide/setup/install/)
1. Install using this command (do not omit the parenthesis):
```bash
(
  set -x; cd "$(mktemp -d)" &&
  OS="$(uname | tr '[:upper:]' '[:lower:]')" &&
  ARCH="$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\(arm\)\(64\)\?.*/\1\2/' -e 's/aarch64$/arm64/')" &&
  KREW="krew-${OS}_${ARCH}" &&
  curl -fsSLO "https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz" &&
  tar zxvf "${KREW}.tar.gz" &&
  ./"${KREW}" install krew
)
```
2. add to `~/.bashrc` the export to add it to `PATH`:
```bash
export PATH="${KREW_ROOT:-$HOME/.krew}/bin:$PATH"
source ~/.bashrc
```
3. we can now use `krew` with `kubectl` to install plugins
```bash
# here plugin that will display the yaml/josn in a `neat` clean way when for example extracting those from cluster
kubectl krew install neat
```
4. now use the plugin to get here nice `yaml` or `json` output of resources
```bash
kubectl get deployment nginx -n nginx -o yaml | kubectl neat > nginx-deployment.yaml
```
- outputs:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "3"
  labels:
    app: nginx
  name: nginx
  namespace: nginx
spec:
  progressDeadlineSeconds: 600
  replicas: 3
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: nginx
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
```
___________________________________________________________________________________________________________
# Kubernetes Patching Deployment
Here we will use a side `.yaml` file to patch our deployment:
- if the keys names are the same as in the initial deployment the field will be updated
- if the keys names are different those will be added to the deployment
- therefore, **make sure you choose the name of the keys carfully**

- here have added a `configMap` as a mounted volume to the `nginx` deployment
```bash
cat config-map.yaml 
```
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-html
  namespace: nginx
data:
  index.html: |
    <h1 style="color:red;">Creditizens Customized Page Red</h1>
```

- here we create the side file that target the fields that will be updated
```bash
cat nginx-deployment-patching.yaml 
```
```yaml
spec:
  template:
    spec:
      containers:
      - name: nginx
        image: nginx
        volumeMounts:
        - name: nginx-html-conf
          mountPath: /usr/share/nginx/html/
      volumes:
      - name: nginx-html-conf
        configMap:
          name: nginx-html
```

- now patch deployment and you will see that nginx pages are displaying the red message `Creditizens Page Red`
```bash
# apply patch
kubectl patch deployment nginx -n nginx --type merge --patch-file nginx-deployment-patching.yaml
# update deployment (will start new pod and make the rolling update of those with default 25% surge)
kubectl rollout restart deployment/nginx -n nginx
```

_______________________________________________________________________________________

# Kubernetes `kubectl` auto-completion
source: [aliases and completion `kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#enable-shell-autocompletion)
- install bash completion
```bash
sudo apt install bash-completion
```
- install `kubectl` completion and make an short alias
```bash
echo 'source <(kubectl completion bash)' >>~/.bashrc
echo 'alias k=kubectl' >>~/.bashrc
echo 'complete -o default -F __start_kubectl k' >>~/.bashrc
source ~/.bashrc
```
_______________________________________________________________________________________

# Kubernetes RollBack ('rollout undo')
source: [rollout history and rollout undo](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-back-a-deployment)
- check the history of different revisions. The biggest number is the lastest and the `1` is the oldest
```bash
 k rollout history  deployment nginx -n nginx
```
- You can have the a description recorded btu not using flag `--record` which is deprecated but using an `annotation`
eg.: patching a deployment and adding an annotation type `kubernetes.io/change-cause:<the cause of the change>`
```yaml
metadata:
  annotations:
    kubernetes.io/change-cause: "Shibuya is not accessible today, VIOLET ALERT!"
spec:
  template:
    spec:
      containers:
      - name: nginx
        image: nginx
        volumeMounts:
        - name: nginx-violet
          mountPath: /usr/share/nginx/html/
      volumes:
      - name: nginx-violet
        configMap:
          name: nginx-html
```
- rollback is done still using the keyword `rollout` but with `undo` and `--to-revision=<nbr of revision shown in history>`
```bash
- example output of some with the command used that use `--record` deprecated flag, some the `annotation` and some nothing:
k rollout history  deployment nginx -n nginx
Outputs:
deployment.apps/nginx 
REVISION  CHANGE-CAUSE
4         <none>   # example of nothing specified
5         <none>
6         <none>
7         <none>
8         <none>
10        kubectl patch deployment nginx --namespace=nginx --type=merge --   # example of `--record` flag used (deprecated)
11        kubectl patch deployment nginx --namespace=nginx --type=merge --
12        Shibuya is not accessible today, VIOLET ALERT!    # example of the `annotations: kubernetes.io/change-cause: "<cause to put here>" 
13        Shibuya is not accessible today, VIOLET ALERT
14        kubectl patch deployment nginx --namespace=nginx --type=merge --patch-file=nginx-deployment-patching.yaml --record=true
15        kubectl patch deployment nginx --namespace=nginx --type=merge --patch-file=nginx-deployment-patching.yaml --record=true
```
```bash
# now we perform the rolback
k rollout undo deployment/nginx -n nginx --to-revision=9
Outputs:
deployment.apps/nginx rolled back
```

Now at every use of the command `k rollout undo` or `k rollout restart` we will have a new `revision` numbered line in the history. 

rollout is just for: `kind:` `Deployments` or`ReplicaSets` or `StatefulSets`

**NOTE:**
When a `node` is down you lose all and `kubernetes` only can recreate pods in other nodes if they use `replicasets` under the how.
So only `kind:` `kind:` `Deployments` or`ReplicaSets` or `StatefulSets` are getting the change to have their pods redeployed in healthy nodes.
The standalone pods, are not recreated and lost, if using `hostPath` or
`local` `persistent volumes` it is lost as it leaves in the `node` so you better use `ebs`,
`nfs` or remote persistant volumes so that new pod recreated in other nodes will be able to reach those.
`DaemonSet`, `CongiMap`, `Secret`, `ServiceAccount`, `Service`: 
  all of those will survive! so for standalone `pods` better change `kind` from `kind: Pod` to `kind: Deployment`
  so that they will be rescheduled in another healthy `node`.


_________________________________________________________________________________________________________________

# Kubernetes Commands `CLI`

- Use `kubeconfig` file to authenticate to cluster
```bash
kubectl get pods --kubeconfig ~/.kube/config -A
```

- `--raw` flag to see sensitive data and raw bytes data
source: [doc about --raw](https://kubernetes.io/docs/reference/kubectl/generated/kubectl_config/kubectl_config_view/)
source: [doc about --raw](https://kubernetes.io/docs/reference/kubectl/generated/kubectl_get/)
```
--raw string
Raw URI to request from the server. Uses the transport specified by the kubeconfig file.
```

Eg.:
```bash
# this shows sensitive data of the kubeconfig certs/keys (base64 encoded so as those are displayed in the file)
kubectl config view --raw
# can use raw URL to have resources info
kubectl get --raw /api/v1/namespaces/default/pods
{"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"104637"},"items":[]}
```

- Authentication RBAC
```bash
kubectl auth whoami
ATTRIBUTE   VALUE
Username    kubernetes-admin
Groups      [system:masters system:authenticated]
```

- Create User And Provide Access Using Cert Creation Way
source: [User Permission Access: Cert Creation Way](https://kubernetes.io/docs/reference/access-authn-authz/authentication/)
source: [Authorisation](https://kubernetes.io/docs/reference/access-authn-authz/authorization/)
```bash
# create key
openssl genrsa -out key_creditizens.pem
```
```bash
# create certificate signing request with username and organizations (different groups)
openssl req -new -key key_creditizens.pem -out creditizens.csr -subj "/CN=creditizens/O=devops/O=sre/O=genaiops"
```
```bash
# extract the base64 encode key `csr` in a one liner for the next .yaml file that we are going to embedded into
cat creditizens.csr | base64 | tr -d '\n'
```
source: (but version 1.27 not available so will try with my hold notes) [doc for v1.28](https://v1-28.docs.kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/)
source where `yaml` file is found: [Managing TLS in a Cluster](https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/)
```yaml
# create `yaml` file `kind: CertificateSigningRequest`
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  # this will be username
  name: creditizens
spec:
  groups:
  - system:authenticated
  request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ2pUQ0NBWFVDQVFBd1NERVVNQklHQTFVRUF3d0xZM0psWkdsMGFYcGxibk14RHpBTkJnTlZCQW9NQm1SbApkbTl3Y3pFTU1Bb0dBMVVFQ2d3RGMzSmxNUkV3RHdZRFZRUUtEQWhuWlc1aGFXOXdjekNDQVNJd0RRWUpLb1pJCmh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBS09DQWVCR3h4WVQ5U1VHNGtWZVRNNlJmVlhEdlVMTTBuM3cKYlNKa1AyYW5XRHozSm5QQ003MFBLNmo3cWNpakxyL0czcVkrMzVvL0pYeXJVL0hJOVRxaXNxeTRvT1kxa1ZEawo2amNKVGhhd1hJNExtT0dDNFVkSENVVGhQYTVBZkZMMjZ1RmNNWDZ0ZGh2MWtTa0NSNXJUSmlPbFg5MWtsWm83ClE5S3lhdDVNQ1hNTm1iZXFkVTRVY1NWZ0NaOTlJNHdLSy9KQytVMHR4amNPUzdNd04wekJJL2VSM29XOFhZS3AKMEFXWUw1MVRhRHNpM1hLSW9WMTFrVXFqeERtNGI4Qy9pMFRJbDRDMVJIUmtCNmVJbnQwbFcxSjk0aDZ1UmEvNAphYUlnS1lxcEQ0WjBpaE9CRGl5dkNVcTVRS1FvTVVXWXlsZ2IwUzVhcHVEbkRiNUxkVTBDQXdFQUFhQUFNQTBHCkNTcUdTSWIzRFFFQkN3VUFBNElCQVFCTUs1ZGJUekZWOXBIM3g3Mk9SMm56TzhoQnQzSGdySGplZ1ZkVlZ4dEUKR3N6bHZXemw2S3gxb25zQlhmdTcvWWdIbUlMZG5EdmdTaCtzSUhKUkNpNDR3TmRhQmZtdHl1L0pSTFFUbmVEZQozb0JSY0tnZHZrclRQVWZieittMGRXUGVRemVWU3BUS1F6dytBK0ZGcURtbTYwano2ZWc4ZnNjVkpZSCtBTlhjCm1zcGZFeTBhdDNEbjJpelNZZmRaVlNrWTc2cjFUclpNNXdaWVA5WXo1U1Y5NHVMR2ZhYWlrblErMEw0Zm80OUoKM0JESFllRXMzZHZFbFJ3Zk9MSVpBaUYwOEowNmdYMmtHajFiTk1taHNDdGY4T2RTVzZwYktBUVhxMDVZeDJxNwp1ODR0UU9zZXlLNDFwNVpCUDdEYVBsU3ZocVFDMFZKbHVtNDE0TzZxR0JodwotLS0tLUVORCBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0K
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - digital signature
  - key encipherment
  - client auth
  # this is just full name
  username: "creditizens metaverse"
```
```bash
# apply yaml file
kubectl apply -f auth_creditizens_to_api_server_component_create.yaml 
```
source: [approve csr](https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/)
```bash
# get csr using `kubectl` command and approve user
kubectl get csr
# approve the name of the csr which the the username choosen int he `yaml` file
kubectl certificate approve creditizens
```
```bash
# check the yaml version of the kuebrnetes `csr`
kubectl get csr creditizens -o yaml
# then extract from it the certificate and save it in a file for the user to be able to use that `crt` (situated at `.status.certificate`)
kubectl get csr creditizens -o jsonpath='{.status.certificate}' | base64 -d > creditizens.crt
```
source: [imperative command](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/imperative-command/)
source: [imperative all kubetcl command doc](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands)
```bash
# create namespace (Optional as you can leave it to default namespace)
kubectl create ns aishibuya
# create role
kubectl create role aidevops --verb=create,get,update,delete,list --resource=pods -n aishibuya
```
```bash
# create a role binding
kubectl create rolebinding deployment-ai-apps-creditizens --user=creditizens --role=aidevops -n aishibuya
```
```bash
# now set credentials for the user using key(`.pem` or `.key` if used `-in <....>.key` when using the openssl command but choose `.pem` way) and crt
# so now those credential are added to kubeconfig
kubectl config set-credentials creditizens --client-key=key_creditizens.pem --client-certificate=creditizens.crt --embed-certs=true
```
```bash
# set the context for this user that will use previously added credentials in the kubeconfig file (~/.kue/config)
kubectl config set-context creditizens --user=creditizens --cluster=kubernetes
```
```bash
# switch to the new user context and it will be limited to the namespace `aishibuya` and only `pod` resource (delete, get, list, create..)
kubectl config use-context creditizens
```
```bash
# swith context
kubectl config use-context kubernetes-admin@kubernetes
# delete context created and it will disappear from the '~/.kube/config' file
kubectl config delete-context creditizens
# check user is not anymore in `~/.kube/config`
cat ~/.kube/config
```


_______________________________________________________________________________________________________________

# Labels & Selectors
source: (Kubernetes Documentation)[https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/]

## Labels & Selectors Using `ReplicaSets` as example

(label names standards depending on DNS type name possibility or not)[https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-label-names]

**Important:**
- make sure that the `Selectors` do not match the labels of some resources implementing `ReplicaSets` for example.
  otherwise you risk to have that `pod` being `acquired` by the resource implementing `Replicasets`

We will provide examples of `kind: Deployment` or `kind: ReplicaSet` that will have a certain number of `replicas` count. And we will see what happen when you create a pods before that with a `selector` that have same label as the `replicaSet` implemented resource.
Answer is that:
  - if the `pod` is created after the `ReplicaSet`, the `pod` will be kiled instantly as the `ReplicaSet` have already reach maximum of `replicas` counts deployed in the cluster.
  - if the `pod` is created before the `ReplicaSet`, when the `ReplicaSet` will be deployed to the cluster, it will acquire those pods having the `Selector` matching its `labels`, therefore it will only add pods on top of what it has in `replicas` count OR will aquire pods until it fulfills its scaling `replicas` count and would kill all other pods having `selector` matching it `labels`
**So Be Very Carefull Here!**

### `ReplicaSets` restart policy
Source: (restart policy doc)[https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy]
- `Always`: default, always restarts container on terminaison
- `OnFailure`: restarts container on error
- `Never`: Never restarts container, dead/terminaison/error/other just bye bye!

### `ReplicaSets` `Selector` of `pod` `Labels` to acquire in yaml
- the yaml file has more field but this is extraction of what we are focusing on this example
```yaml
spec:
  selector:
    matchLabels:
      tier: junko
  # `template` part need also to therefore have a `label` and MUST match the `selector` `matchlabels` to not be rejected by the API Server
  template:
    metadata:
      labels:
        tier: junko
```

### Using REST API to delete `ReplicaSet`
There is different ways to interact with the custer and here we are not passing by the API Server but using REST API
to perform action on the Cluster. It is passing through the `Front Proxy`
```bash
kubectl proxy --port=8080
curl -X DELETE  'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend' \
  # option `propagationPolicy have to be `Foreground` or `Background`, if using instead `orphan` it will delete only the replicaset and not affect pods.
  # which will be still there but not manage anymore by `ReplicaSet` so not recreated if they die(error) or terminate
  -d '{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Foreground"}' \
  -H "Content-Type: application/json"
```
- Here `propagationPolicy` can be `orphan`, but why use this behavior of getting rid of the `replicaset` but keeping the `pods` in the Cluster as standalone `pods`?
  - for example, if we want to debug or test a new version in a specific pod and don't want to replicaset to restart the pods,
    we can't just scale down it to `0`. We can use this strategy to get rid of the `replicaset` temporaly and then recreate a new one that would `acquire` those pods back based on `labels` for example.
  - imperative command:
    - `kubectl delete replicaset <rs-name> --cascade=orphan`
    - `kubectl delete replicaset my-rs --grace-period=0 --cascade=orphan --force`

### 'acquire` rules
`Pods` -> `ReplicaSet` -> `Deployment`
- `Deployment` does manage the `ReplicaSet` and not the `Pod` so tell it: `Yo! Make sure that the pods are running with this container and replicas`
- `ReplicaSet` does manage the `Pod` and his boss the `Deployment` manages him OR `ReplicaSet` can be in cluster just to manage `Pod` without being part of `Deployment`
- `Pod` is managed by `ReplicaSet` or just running as a standalone `Pod`
- Therefore:
  - `Deployment` will adopt `ReplicaSet` if:
    - `ReplicaSet` has **no `ownerReferences`**
    - `ReplicaSet`’s `spec.selector` **matches** **`Deployment`'s selector**
    - `ReplicaSet`’s template **matches** **`Deployment`'s `Pod` template**
  - `ReplicaSet` can adopt a `Pod` if:
     - `Pod` **matches** the `ReplicaSet`’s **label selector**
     - `Pod` has **no `ownerReference`** (i.e., it’s "orphaned"): `kubectl get pod <pod_name> -o jsonpath='{.metadata.ownerReferences}'`
     - `Pod` `template spec` is an **exact match** with the **`ReplicaSet`'s template** (containers, volumes, etc.)


### When Scaling Down `ReplicaSets` How Pods Deletion Are Prioritized:
- 1: first will be deleted any pods in `pending` state or `unschedulable`
- 2: then,  will come any pods with the `annotation`: `controller.kubernetes.io/pod-deletion-cost`. The lower number one is delete first and so on.
- 3: then, `pods` on `nodes` with more `replicas` are delete first compared to pod on nodes having less replicas.
- 4: then comes, `pods` created more recently comes first if their creation time differs
- 5: randomly delete `pods` if all above matches

Note about the `annotation: controller.kubernetes.io/pod-deletion-cost`: between `-2147483648 and 2147483647` and it is `best-effort` so doesn't guarantees any deletion in the order (info about pod deleition cost annotation)[https://kubernetes.io/docs/reference/labels-annotations-taints/#pod-deletion-cost]

## `Labels` and `NodeSelectors` Using `Pods` as Example

### `pod` yaml example
taken from kubernetes documentation, here the pod will be selecting node with label `accelerator:nvidia-tesla-p100`
```yaml
spec:
  containers:
    - name: cuda-test
      image: "registry.k8s.io/cuda-vector-add:v0.1"
      resources:
        limits:
          nvidia.com/gpu: 1
  nodeSelector:
    accelerator: nvidia-tesla-p100
```

### `Set-Based` Requirement meaning filtering on the label keys and using keywords for values
- keywords for values:
  - `!`: means NOT and placed in front of key to excluse those keys
- keywords for values:
  - `in`: values are `EQUAL` to in the set of values supplied
  - `notin`: values are `NOT EQUAL` to in the set of values supplied
  - `exists`: a label that does exist (only check key not value)
  - `DoesNotExist`: a label that does not exist (only check key not value)
  - `,`: comma means `AND`

eg. of meanings :
```yaml
:
# key=`citynode`, values accepted=`tokyo` AND (because of comma: `,`) hokkaido
citynode in (tokyo, hokkaido)
# key=`nevereurope`, values=france AND emgland
nevereurope notin (france, england)
# key exclusive without value indicated. key=`shibuya`
shibuya
# key=NOT EQUAL to (because exclamation mark: `!`) shinjuku
!shinjuku
### more complexe requirements
# (key=`mangakissa`, with values=`ueno` AND `omotesando`), AND (key=`appdeploymentgroup`, values: NOT EQUAL to `production`)
mangakissa in (ueno, omotesando), appdeploymentgroup!=production
# checks if key `shomikitazawa` exist (not value)
exists: shimokitazawa
```

`Set-Based` requirements uses keywords while the other `Equality-Based` requirements are using `=`, `==` `!=`
We can use any of those two in our imperative commands:
```bash
kubectl get pods -l appdeploymentgroup!=production,magakissa=shibuya
kubectl get pods -l 'appdeploymentgroup in (staging),mangakissa in (shibuya)'
```

- in this `yaml` example from the `Kubernetes` documentation, satisfaction in when `ALL` `matchExpressions` **MUST** be satisfied
```yaml
selector:
  matchLabels:
    component: redis
  matchExpressions:
    - { key: tier, operator: In, values: [cache] }
    - { key: environment, operator: NotIn, values: [dev] }
```

## What is `ANDed` (`AND`) and What is `ORed` (`OR`)

### `matchExpressions`
- Here all conditions must be true (AND logic)
```yaml
matchExpressions:
  - key: env
    operator: In
    values: [prod, dev]
  - key: app
    operator: Exists
```
This matches only Pods that: have env=prod or env=dev `AND` have the label app (any value)

### `nodeSelector`
- Here `nodeSelector`: Only `AND`, no `OR`
```yaml
spec:
  nodeSelector:
    env: prod
    region: us-west
```
strictly: env == prod `AND` region == us-west, **No** support for `OR` here.

### `nodeAffinity` (same for: `podAffinity`)
- `AND`: `requiredDuringSchedulingIgnoredDuringExecution` (hard constraints) only supports `AND` across matchExpressions.
  So here inside one `matchExpressions` if there more than one option it is `AND` so all of those have to be true
```yaml
requiredDuringSchedulingIgnoredDuringExecution:
  nodeSelectorTerms:
    - matchExpressions:
        - key: env
          operator: In
          values: [prod]
        - key: region
          operator: In
          values: [us-west]
```
That means: (env == prod) `AND` (region == us-west)

- `OR`: `requiredDuringSchedulingIgnoredDuringExecution` also support `OR` but this time across several `match#xpressions` each `matchExpressions` being sets of `ORs`

You can provide multiple `nodeSelectorTerms` `matchExpressions`, and those terms are `ORed`.
Here match nodes that are either: env=prod `OR` region=us-west
```yaml
requiredDuringSchedulingIgnoredDuringExecution:
  nodeSelectorTerms:
    - matchExpressions:
        - key: env
          operator: In
          values: [prod]
    - matchExpressions:
        - key: region
          operator: In
          values: [us-west]
```
(env == prod) `OR` (region == us-west): inside each individual `matchExpressions` terms are `ANDed`, the `nodeSelectorTerms` several `matchExpressions` between themselves are `ORed`

## Label Usefulness To Have View in Cluster With Different Parts Grouped in Custom Columns

- We get the cluster pods and show the labels to see what is present and be able to create columns like pivoting the output
```bash
k get pod --all-namespaces --show-labels -o wide
NAMESPACE     NAME                                                 READY   STATUS    RESTARTS        AGE     IP                NODE                         NOMINATED NODE   READINESS GATES   LABELS
kube-system   calico-kube-controllers-85578c44bf-xqlmz             1/1     Running   4 (133m ago)    4d15h   172.16.247.185    controller.creditizens.net   <none>           <none>            k8s-app=calico-kube-contr
ollers,pod-template-hash=85578c44bf
kube-system   calico-node-8v4pl                                    1/1     Running   19 (133m ago)   623d    192.168.186.146   controller.creditizens.net   <none>           <none>            controller-revision-hash=
754b4dfc9f,k8s-app=calico-node,pod-template-generation=1
kube-system   calico-node-wh76j                                    1/1     Running   6 (133m ago)    623d    192.168.186.147   node1.creditizens.net        <none>           <none>            controller-revision-hash=
754b4dfc9f,k8s-app=calico-node,pod-template-generation=1
kube-system   calico-node-zk8zl                                    1/1     Running   7 (133m ago)    623d    192.168.186.148   node2.creditizens.net        <none>           <none>            controller-revision-hash=
754b4dfc9f,k8s-app=calico-node,pod-template-generation=1
kube-system   coredns-5d78c9869d-kpbtn                             1/1     Running   5 (133m ago)    5d17h   172.16.247.183    controller.creditizens.net   <none>           <none>            k8s-app=kube-dns,pod-temp
late-hash=5d78c9869d
kube-system   coredns-5d78c9869d-l47pn                             1/1     Running   4 (133m ago)    4d15h   172.16.247.184    controller.creditizens.net   <none>           <none>            k8s-app=kube-dns,pod-temp
late-hash=5d78c9869d
kube-system   etcd-controller.creditizens.net                      1/1     Running   5 (133m ago)    5d18h   192.168.186.146   controller.creditizens.net   <none>           <none>            component=etcd,tier=contr
ol-plane
kube-system   kube-apiserver-controller.creditizens.net            1/1     Running   6 (133m ago)    5d18h   192.168.186.146   controller.creditizens.net   <none>           <none>            component=kube-apiserver,
tier=control-plane
kube-system   kube-controller-manager-controller.creditizens.net   1/1     Running   14 (133m ago)   5d18h   192.168.186.146   controller.creditizens.net   <none>           <none>            component=kube-controller
-manager,tier=control-plane
kube-system   kube-proxy-88h87                                     1/1     Running   4 (133m ago)    5d17h   192.168.186.147   node1.creditizens.net        <none>           <none>            controller-revision-hash=
747c75b954,k8s-app=kube-proxy,pod-template-generation=2
kube-system   kube-proxy-bs58j                                     1/1     Running   5 (133m ago)    5d17h   192.168.186.148   node2.creditizens.net        <none>           <none>            controller-revision-hash=
747c75b954,k8s-app=kube-proxy,pod-template-generation=2
kube-system   kube-proxy-mwnmk                                     1/1     Running   5 (133m ago)    5d17h   192.168.186.146   controller.creditizens.net   <none>           <none>            controller-revision-hash=
747c75b954,k8s-app=kube-proxy,pod-template-generation=2
kube-system   kube-scheduler-controller.creditizens.net            1/1     Running   14 (133m ago)   5d18h   192.168.186.146   controller.creditizens.net   <none>           <none>            component=kube-scheduler,
tier=control-plane
```

- We are going to make groups of `label` `keys` to have columns and group of resources
 
```bash
k get pod --all-namespaces -Lcontroller-revision-hash -Lk8s-app -Lcomponent 
NAMESPACE     NAME                                                 READY   STATUS    RESTARTS         AGE     CONTROLLER-REVISION-HASH   K8S-APP                   COMPONENT
kube-system   calico-kube-controllers-85578c44bf-xqlmz             1/1     Running   4 (3h21m ago)    4d16h                              calico-kube-controllers   
kube-system   calico-node-8v4pl                                    1/1     Running   19 (3h21m ago)   623d    754b4dfc9f                 calico-node               
kube-system   calico-node-wh76j                                    1/1     Running   6 (3h21m ago)    623d    754b4dfc9f                 calico-node               
kube-system   calico-node-zk8zl                                    1/1     Running   7 (3h21m ago)    623d    754b4dfc9f                 calico-node               
kube-system   coredns-5d78c9869d-kpbtn                             1/1     Running   5 (3h21m ago)    5d18h                              kube-dns                  
kube-system   coredns-5d78c9869d-l47pn                             1/1     Running   4 (3h21m ago)    4d16h                              kube-dns                  
kube-system   etcd-controller.creditizens.net                      1/1     Running   5 (3h21m ago)    5d19h                                                        etcd
kube-system   kube-apiserver-controller.creditizens.net            1/1     Running   6 (3h21m ago)    5d19h                                                        kube-apiserver
kube-system   kube-controller-manager-controller.creditizens.net   1/1     Running   14 (3h21m ago)   5d19h                                                        kube-controller-manager
kube-system   kube-proxy-88h87                                     1/1     Running   4 (3h21m ago)    5d19h   747c75b954                 kube-proxy                
kube-system   kube-proxy-bs58j                                     1/1     Running   5 (3h21m ago)    5d19h   747c75b954                 kube-proxy                
kube-system   kube-proxy-mwnmk                                     1/1     Running   5 (3h21m ago)    5d19h   747c75b954                 kube-proxy                
kube-system   kube-scheduler-controller.creditizens.net            1/1     Running   14 (3h21m ago)   5d19h                                                        kube-scheduler 
```

## `-l` to label pods or get pods based on labels, `-L` to create colum based on those labels keys

- from `Kubernetes` documentation
```bash
kubectl label pods -l app=nginx tier=fe

kubectl get pods -l app=nginx -L tier
Output:
NAME                        READY     STATUS    RESTARTS   AGE       TIER
my-nginx-2035384211-j5fhi   1/1       Running   0          23m       fe
my-nginx-2035384211-u2c7e   1/1       Running   0          23m       fe
my-nginx-2035384211-u3t6x   1/1       Running   0          23m       fe
```

# Node Affinity and Anti-Affinity

- here we can use `nodeSelector` which only select nodes with specific labels
- vs Node `affinity`/ `anti-affinity` adds some more option and control over the node selection:
  - `soft` and `preferred` keyword can be used to tell `scheduler` to schedule `pod` event no `nodes` are matching
  - or create rules based on other `pods` `labels`

From kubernetes Documentation. (source)[https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/]:
- `Node affinity` functions like the `nodeSelector` field but is more expressive and allows you to specify soft rules.
- `Inter-pod` `affinity/anti-affinity` allows you to constrain `Pods` against `labels` on other `Pods`.

## Node Affinity
- two keywords, Like `nodeSelector` but with more detailed specification:
  - `requiredDuringSchedulingIgnoredDuringExecution`: `required` so MUST be equal to those specifications.
  - `preferredDuringSchedulingIgnoredDuringExecution`: `preferred` so NOT STRICT RULE but preferred rule.

Eg. from kubernetes documentation (just spec of pod part targeting our subject):
(`operator` can be any of those values: `In`, `NotIn`, `Exists`, `DoesNotExist`, `Gt` or `Lt`)
```yaml
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution: # MUST: for the rules coming after that
        nodeSelectorTerms:
        - matchExpressions: # if many `matchExpressions` it is `AND` so all have to match for `pod` to be scheduled
          - key: topology.kubernetes.io/zone
            # can be: `In`, `NotIn`, `Exists`, `DoesNotExist`, `Gt` or `Lt`
            operator: In
            values: # OR `antarctica-east1` OR `antarctica-west1`
            - antarctica-east1
            - antarctica-west1
      preferredDuringSchedulingIgnoredDuringExecution: # PREFERRED but not obliged to: for the rules coming after that
      - weight: 1 # can be from 1 to 100. If many different `preferences` `weight` is calculated. `Nodes` satisfying highest score are prioritized
        preference:
          matchExpressions:
          - key: another-node-label-key
            # can be: `In`, `NotIn`, `Exists`, `DoesNotExist`, `Gt` or `Lt`
            operator: In
            values:
            - another-node-label-value
```

## ** Notes from documentation **
Note:
- specify both `nodeSelector` and `nodeAffinity`: both must be satisfied for the Pod to be scheduled onto a node.
- specify multiple terms in `nodeSelectorTerms` with `nodeAffinity` types:  `Pod` can be scheduled to node if one specified terms can be satisfied (`OR`).
- specify multiple expressions in a single `matchExpressions` associated with a term in `nodeSelectorTerms`: `Pod` can be scheduled on node only if all expressions satisfied (`AND`)

## Node Anti-Affinity
Is done using keyword in `operator`:
- `NotIn`
- `DoesNotExist`


# Taints & Tolerations
source: (doc taints and tolerations)[https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/]
**NOTE:**
if `.spec.nodeName` is specified in a `pod` then it will bypass the scheduler and not `taint` will be checked. It will be put in the `node` specified even if that one `pod` had a `toleration` not matching that `node` `taint` but it the node `taint` has a `NoExecute` `effect` in the `taint`, th epod will be ejected.

## Taints
- Taints are applied to `nodes` to tell which `pod` label is accepted to be scheduled in the `node`
In this example, `node1` is `tainted` to not accept any `pod` sheduled having `label` key `special` with value `mangakissa`
and the effect `:` `NoSchedule` to 'Not Schedule'.
If this `taint` already exists it can be deleted by adding same line but with a minus at the end `-` like `NoSchedule-` 
```bash
kubectl taint nodes node1 special=mangakissa:NoSchedule
```

## Toleration
- Tolerations are applied to `pods` to tell to scheduler where the `pod` can be scheduled
In this example of the `PodSpec` part of `pod` definition, we have two different ways of doing it: one with key/value and another with only key

```yaml
# key/value
tolerations:
# node taint need to match those fields when `Equal` `operator:` is used
- key: "special" # node tiant need to match key
  operator: "Equal"
  value: "mangakissa" # node taint need to match value
  effect: "NoSchedule" # node taint need to match effect

# key only
tolerations:
# node taint need to match those fields when `Exists` `operator:` is used
- key: "special" # node taint need to match key
  operator: "Exists"
  effect: "NoSchedule" # node taint need to match effect
```

### rules for `operator` in `toleration` `pod.spec`:
- `operator:` default value is `Equal` then a `value:` field should be specified
- if `operator:` is `Exists` then no `value:` field should be specified, it will match on `key:` and `effect:`

### rules for empty field
- if `key:` is empty then `operator:` have to be `Exists` and it will match all key/values. AND, `effect:` still need to match at the same time
- if `effect:` is empty then it will match all `effect:` with `key: <your_key>``

### values taken by `effect`
- `NoExecute`:
  - `taint` not tolerated: pods are evicted from the node
  - `taint` tolerated:
    - `tolerationSeconds` indicated in `pod`: after that timelapse the `pod` will be evicted from `node`
    - `tolerationSeconds` not indicated in `pod`: No eviction, `pod` will remain forever in `node`
- `NoSchedule`: existing `pod` on `node` not evicted, only `pod` with matching `toleration` can be schedule in `node` otherwise not
- `PreferredNoSchedule`: it is a `soft NoSchedule`, so scheduler will try to not put `pod` on `node` tainted
  or `pod` spec without `toleration` indicated, but it is not guaranteed.

### rules for mutiple `toleration:` indicated in same pod
Here the scheduler will work fine but use those as filters, checking `nodes` `taints`, the `pod` `effects` and `operators`

## list of `kubernetes` automatic `taints`
- `node.kubernetes.io/not-ready`: Node is not ready. This corresponds to the `NodeCondition` `Ready="False"`.
- `node.kubernetes.io/unreachable`: Node is unreachable from the node controller. This corresponds to the `NodeCondition` `Ready="Unknown"`.
- `node.kubernetes.io/memory-pressure`: Node has memory pressure.
- `node.kubernetes.io/disk-pressure`: Node has disk pressure.
- `node.kubernetes.io/pid-pressure`: Node has PID pressure.
- `node.kubernetes.io/network-unavailable`: Node's network is unavailable.
- `node.kubernetes.io/unschedulable`: Node is unschedulable.
- `node.cloudprovider.kubernetes.io/uninitialized`: For cloud to have time to make setup. After a controller from the `cloud-controller-manager` initializes this `node`, the `kubelet` removes this `taint`.

```yaml
tolerations:
- key: "node.kubernetes.io/unreachable"
  operator: "Exists"
  effect: "NoExecute"
  tolerationSeconds: 6000 # unless you set it like that it is automatically set to 300 (5mn) by the the controller
```

## example `node drain` of `taints` automatically applied to the `node`
When a `node` is `drained`, the `node controller` or `kubelet` adds `taints` with `NoExecute effect`. 
It also adds `node.kubernetes.io/not-ready` and `node.kubernetes.io/unreachable`


scenarios:
- 1) We can here justify that scenario with a troubleshooting in staging environment, so temporaly delete replicaset and then re-acquire pods: 
create deployment nginx with 1 replica only and use labels
create a pod with a label selector same as the deployment one to show that it dies
create a pod a label selector different from the deployment to show that it stays alive
delete the deployment tearing it down
keep the pod and deploy again with 3 replicas and show that that pod will be acquired by the replicaset (so replicaset will be only deploying 2 new pods and acquiring the existing one)
show labels grouping using -L which will create a column int he output of `get pods` using the key of the label

- 2) Pod deletion priority setup using annotation
- 1: first will be deleted any pods in `pending` state or `unschedulable`
- 2: then,  will come any pods with the `annotation`: `controller.kubernetes.io/pod-deletion-cost`. The lower number one is delete first and so on.
So here we could do a scenario to have pods being annotated and see how the deployment replicas are scaled down meaning in which order
We could create an error first having the full replicaspulling an image that doesn't exist
then patch some replicas to fix those pods and get those running
and then have the scaling down showing in which order those pods would be deleted
then do the deployment again with healthy pods 
then annotate some of those pods with the `annotation`: `controller.kubernetes.io/pod-deletion-cost` and then scale down to see in which order those are scale down


- 3) Scenario in which we would use `nodeselector` to affect pods in specific nodes
```
selector:
  matchLabels:
    component: redis
  matchExpressions:
    - { key: tier, operator: In, values: [cache] }
    - { key: environment, operator: NotIn, values: [dev] }
```
this would need us to label nodes and then have selectors in deploymed pods
this also could be used with `selector` and use `matchExpression` in order to show IN/NOTIN/EXIST/DOESNOTEXIST 
little explanation of those:
```
# key=`citynode`, values accepted=`tokyo` AND (because of comma: `,`) hokkaido
citynode in (tokyo, hokkaido)
# key=`nevereurope`, values=france AND emgland
nevereurope notin (france, england)
# key exclusive without value indicated. key=`shibuya`
shibuya
# key=NOT EQUAL to (because exclamation mark: `!`) shinjuku
!shinjuku
### more complexe requirements
# (key=`mangakissa`, with values=`ueno` AND `omotesando`), AND (key=`appdeploymentgroup`, values: NOT EQUAL to `production`)
mangakissa in (ueno, omotesando), appdeploymentgroup!=production

```

```
selector:
  matchLabels:
    component: redis
  matchExpressions:
    - { key: tier, operator: In, values: [cache] }
    - { key: environment, operator: NotIn, values: [dev] }
```

- 4) Scenario in which we would show how labelling helps groups resources and have nice Cluster resources view by grouping those in representative custom columns
```
k get pod --all-namespaces --show-labels -o wide
```
```
k get pod --all-namespaces -Lcontroller-revision-hash -Lk8s-app -Lcomponent
```

- 5) Scenario in which we would use a `nodeSelector` and then a node `affinity` to show that we can set more specific rules using `affinity`
- `requiredDuringSchedulingIgnoredDuringExecution`: `required` so MUST be equal to those specifications.
- `preferredDuringSchedulingIgnoredDuringExecution`: `preferred` so NOT STRICT RULE but preferred rule.
here show emphasis in explaning the ORed and ANDed of matchExpressions
```
# ANDed
requiredDuringSchedulingIgnoredDuringExecution:
  nodeSelectorTerms:
    - matchExpressions:
        - key: env
          operator: In
          values: [prod]
        - key: region
          operator: In
          values: [us-west]
```
```
# ORed
requiredDuringSchedulingIgnoredDuringExecution:
  nodeSelectorTerms:
    - matchExpressions:
        - key: env
          operator: In
          values: [prod]
    - matchExpressions:
        - key: region
          operator: In
          values: [us-west]
```

- 6) Scenario in which we will use again Japanese locations to show how `node` `taint` and `effect` work with `pod` `toleration`
  - `NoExecute`: first create a deployment with 3 replicasets, then taint one node with effect `NoExecute` to `evict` the pod and show that it is recreated in another node, then taint another node with the same way to show that pods are again evited and will recreated somewhere else. then untaint one node and delete one of the deplyed pods which will be redeployed in the untainted node. Then do the same with the other tainted node by untainting it and deleting the one of the two pods which are on the same node and it will we rescheduled in another node.
    - now use yaml file only:
      ```yaml
      tolerations:
      - key: "special"
        operator: "Equal"
        value: "mangakissa"
        effect: "NoSchedule"
        tolerationSeconds: 30 # how long before being evicted
      ```

  - `NoSchedule`: use from here only `yaml` file
      ```yaml
      tolerations:
      - key: "special"
        operator: "Equal"
        value: "mangakissa"
        effect: "NoSchedule"
      ```
     Then taint the node with matching `taint` and `effect` and you should see that existing pods are not evicted
     then create a standalone pod with a nodeSelector or Affinity to that node, to show that it won't be schedule there as it doesn't have the toleration for that node taint.
     then add the toleration to the pod yaml file and show that now it can be scheduled there
     then get rid of the toleration of the pod keeping the affinity or nodeselector and taint the node with `preferredNoSchedule` the soft version and see that pod will be scheduled there (maybe need to try...)

  - `NoExecute`: create two pods with affinity of node selector just to maake sure those two are scheduled in the same node.
    both will have a toleration with effect `NoExecute` but one has the `tolerationSeconds` and the other not
    then taint the node where those two pods are located with same taint matching their toleration therefore effect `NoExecute`
    You will see that the pod not having the `tolerationSeconds` will stay in the node while the other one will be evicted after those `tolerarionSeconds`
_______________________________________________________________________________
(From documentation for: `ownerReferences`)[https://kubernetes.io/docs/concepts/overview/working-with-objects/owners-dependents/]
# Pod being acquired
cat pod.yaml
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-standalone
  labels:
    app: nginx
spec:
  containers:
    - name: nginx
      image: nginx
```

`kubectl get pod <standalone_pod_name> -o jsonpath='{.metadata.ownerReferences}'`

cat replicaset.yaml
```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-rs
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx
```

`kubectl get pod <standalone_pod_name> -o jsonpath='{.metadata.ownerReferences}'`

# Replicset being acquired

cat replicaset.yaml
```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-orphan-rs
  # make sure to not forget to add here the label matching the one of the deployment otherwise this replicaset won't be acquired by the deployment
  labels: 
    app: nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
```

`kubectl get rs <replicaset_name> -o jsonpath='{.metadata.ownerReferences}'`


cat deployment.yaml
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-adopter
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
```
`kubectl get rs <replicaset_name> -o jsonpath='{.metadata.ownerReferences}'`

____________________________________________________________________

# DNS ACCESSING PODS THROUGH SERVICES USING NAMES

## PODS IN SAME NAMESPACE
As IPs can change there is in Kubernetes native `DNS` resolution:
- services need to be created first (before pod for the pod to be able to capture the env var set automaticcally by Kubernetes) if using IPs instead
  but in general using DNS name of the POD and SVC and NAMESPACE and CLUSTER would reolve to accessing the pod.
- `resolv.conf` file is where the DNS of the service will be indicated

```bash
k exec -it -n nginx nginx-pod -- sh
# cat /etc/resolv.conf
search nginx.svc.cluster.local svc.cluster.local cluster.local localdomain
nameserver 10.96.0.10
options ndots:5
```
So here the service name `nginx` is indicated in the `resolv.conf` file for the pod to be reached at `<pod_name>.nginx.svc.cluster.local`
So another pod in another namespace could pass throught the `ClusterIP` service named `nginx` in the namespace `nginx` and find the pod

**Experiement DNS Using Imperative Commands**
We will make a service that has a `spec.selector` pointing to a pod `label` and will create the pod with the same label , all in same namespaces
As we are using `names` we will be able to use `DNS` names and `not unstable` `IPs`, therefore, there is no rule in the order in which we will create the service, it can be after or before the pod creation

. 1) Run pod with label
```bash
k run nginx --image=nginx \
  --restart=Never \
  --namespace=nginx \
  --labels=app=nginx
```

. 2) Expose the pod to enable `DNS` resolution through a service `nginx-service` with a `selector` pointing to the `pod` `label`
After that we need to expose the service to get `DNS` `nginx-service.nginx.svc.cluster.local` ready and callable from anyu other pod in the cluster:
Here we do something easy to understand but policies can be created to limit access to only pods inside the same namespace for example as namespaces are made for that to separate concerns:
```bash
k expose pod nginx \
  --port=80 \
  --target-port=80 \
  --name=nginx-service \
  --namespace=nginx \
  --selector=app=nginx
```

. 3) Create another temparary pod to test that `nginx` pod is reachable through `DNS` resolution `nginx-service.dev.svc.cluster.local`
```bash
k run debug --rm -it --image=busybox --restart=Never \
  --namespace=nginx -- /bin/sh

# curl not available so used wget which pooled the `index.html` page meaning that the pod is accessible through the service
wget http://nginx-service
Outputs:
Connecting to nginx-service (10.110.245.196:80)
saving to 'index.html'
index.html           100% |************************************************************************************************************************|   615  0:00:00 ETA
'index.html' saved

# then we cat the file pulled
cat index.html 
Outputs:
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
outputs

# nslookup
# nslookup nginx-service.nginx.svc.cluster.local
Server:         10.96.0.10
Address:        10.96.0.10:53


Name:   nginx-service.nginx.svc.cluster.local
Address: 10.110.245.196
```
so here we get confirmation


## DNS RESOLUTION FROM POD IN ANOTHER NAMESPACE

. 4) Check if other pods in other namespaces can use the `DNS` to reach the pod
Here pod created in `default` namespace and will access the pod in `nginx` namespace using the `DNS` which maps the `pod` through the `service` name
```bash
k run debug --rm -it --image=busybox --restart=Never -- /bin/sh
If you don't see a command prompt, try pressing enter.

# nlookup way
/ # nslookup nginx-service.nginx.svc.cluster.local
Outputs:
Server:         10.96.0.10
Address:        10.96.0.10:53


Name:   nginx-service.nginx.svc.cluster.local
Address: 10.110.245.196

# wget way but this this time as we are not in same namesapce we need to provide full `DNS` with namespace
/ # wget nginx-service.nginx.svc.cluster.local
Outputs
Connecting to nginx-service.nginx.svc.cluster.local (10.110.245.196:80)
saving to 'index.html'
index.html           100% |************************************************************************************************************************|   615  0:00:00 ETA
'index.html' saved

# then we check that the file is nginx index.html
/ # cat index.html 
Outputs:
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```

## SETTING POD DNS RESOLV.CONF CONTENT FROM YAML CREATION
eg: source: (form doc kubernetes)[https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/]
```yaml
apiVersion: v1
kind: Pod
metadata:
  namespace: default
  name: dns-example
spec:
  containers:
    - name: test
      image: nginx
  # can be `Default`, `Clusterfirst`, `ClusterFirstWithHostNet` or `None`
  # when `None` we can custom config  `resolv.comf` of the pod
  dnsPolicy: "None"
  # all fields are not required, you can just choose to use one
  # or `nameservers`, or `searches` or `options`
  # but if you want more control you probably want to use them all
  dnsConfig:
    # get the `nameserver ip of the cluster by running this command:
    # kubectl get svc -n kube-system kube-dns
    :nameservers:
      - 10.96.0.10
    # here we put what services we want to reach using Kubernetes DNS way
    searches:
      # <service_anme.name_space.cluster.local>
      # or <namespace.cluster.local>
      - ns1.svc.cluster.local
      - my.dns.search.suffix
    # example of options that you find in `/etc/resolv.conf`
    options:
      # `ndots` is if lower thant `2` dots here
      # will try append the above `searches` to the `curl <domain>`
      # so will try `<domain>.ns1.svc.cluster.local`
      # then will try `<domain>.my.dns.search.suffix` if the first doesn't work and so on...
      - name: ndots
        # higer value more checks and latency, lower value better performance but could skip some
        value: "2"
      - name: edns0
      - name: timeout
        value: "2"
```

- `dnsPolicy` can take different values:
dnsPolicy	| Description	|Use When
------------+---------------+-----------
ClusterFirst (default)	| Uses cluster DNS (CoreDNS). Can resolve Kubernetes service names across namespaces.	| ✅ Most common for standard Pods|
------------------------+---------------------------------------------------------------------------------------+----------------------------------+
ClusterFirstWithHostNet	| Same as above but used when Pod uses hostNetwork: true	| ✅ Use when Pod shares host network|
------------------------+-----------------------------------------------------------+-------------------------------------+
Default	Uses the node’s /etc/resolv.conf. | Can’t resolve Kubernetes service DNS.	| ❌ Avoid unless you want to use external DNS only|
------------------------------------------+-----------------------------------------+--------------------------------------------------+
None	| Lets you manually define DNS config via dnsConfig in the Pod spec	| ✅ Use if you want full custom DNS entries, e.g., override search domains or stub resolvers|
--------+-------------------------------------------------------------------+--------------------------------------------------------------------------------------------+

**Summary**
- `ClusterFirst`: is the default one using CorDNS of Kubernetes
- `ClusterfirstWithHostNet`: Only when `hostNetwork: true`, when pod shares host network
- `Default`: not the 'default' as the name says this only for external DNS
- `None`: when setting custom DNS in pods via `dnsConfig` (/etc/resolv.conf)
  - `nameservers` ip of the cluster is given by CoreDNS `kube-dns` in `kube-system` namespace:
```bash
kubectl get svc -n kube-system kube-dns
Outputs
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   630d
```


## SETUP NETWORK POLICY AS DNS IS RESOLVING ANY SERVICE MAPPED TO POD INSIDE THE CLUSTER
source: (Network Policies Doc)[https://kubernetes.io/docs/concepts/services-networking/network-policies/]

**Important:** By default the Kubernetes allow all traffic and only when you set a rules it will deny all other rules:
`kind: NetworkPolicy` is deny all when set and in the policy you are going to allow `ingress`/`egress`
therefore, you just set the policy and put what is allowed in the rest will be denied.
And this denial is activated because you have set a rule `ingress` or `egress`. without rule all is allowed.


here we have an example in how to setup a policy.
`ingress` and `egress` can be setup, refer to documentation for `egress` as here we are going to do only `ingress`
`egress` is the same anyway but just you replace `ingress` examples with `egress` or just refer to doc as it might change


### INGRESS:
- (Ingress rules) allows connections to all pods in the default namespace with the label role=db on TCP port 6379 from:
      - any pod in the default namespace with the label role=frontend
      - any pod in a namespace with the label project=myproject
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  ingress:
  - from:
            # we just see `namespaceSelector` and `podSelector` not the third one `ipBlock`
    - namespaceSelector:
        matchLabels:
          project: myproject
    - podSelector:
        matchLabels:
          role: frontend
    ports:
    - protocol: TCP
      port: 6379
```

### DENY ALL

eg: deny all traffic
```yaml
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: {}
  policyTypes:
  - Ingress
```

### ANDed VS ORed RULES

eg: ANDed vs ORed
- ANDed
```yaml
  ingress:
  # here `single` element in the `- from` (`- namespaceSelector`) which make it `ANDed`
  - from:
    # note here that here only `namespaceSelector` have the `-` which enabled the `AND`for the next rule `podSelector`
    - namespaceSelector:
        matchLabels:
          user: alice
      podSelector:
        matchLabels:
          role: client
```
- ORed
```yaml
  ingress:
  # here two elements in the `- from` (`- namespaceSelector`, `- podSelector`) which makes it ORed
  - from:
    - namespaceSelector:
        matchLabels:
          user: alice
    # note that here we have `-` which is enabling the `OR` instead of `AND`
    - podSelector:
        matchLabels:
          role: client
```


1) scenario that shows how dns works, very simple, within same namespace creating pod and exposing with a service and creating a third pod that would use dns to access the pod using DNS (curl/wget/nslookup whatever works)
then do same scenario but this time the third pod is created outside of the namespace and curl/wget/nslookup again to show that DNS means that pods from other namespaces can resolve to the pod using DNS call

2) do another scenario showing now how to setup a pods and determining the reolv.conf of the pod content so that pod can call that other pod through another service dedicated to that other pod. maybe customize nginx in that namespace with different one and different messages index.html pages and having each different services attached to those. and go inside pod to show that it can resolve that pod
eg: source: (form doc kubernetes)[https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/]
apiVersion: v1
kind: Pod
metadata:
  namespace: default
  name: dns-example
spec:
  containers:
    - name: test
      image: nginx
  dnsPolicy: "None"
  dnsConfig:
    nameservers:
      - 192.0.2.1 # this is an example
    searches:
      - ns1.svc.cluster-domain.example
      - my.dns.search.suffix
    options:
      - name: ndots
        value: "2"
      - name: edns0

3) now make it more complexe by making this example more interesting by adding a network policy as before even if not in resolv conf the pod could use the kubernetes native dns call to reach the other pod anyway, but this policy would say no ingress accepted from other namespaces for that pod specifically but the other pod would be still reachable
source services can be created with name on ports which would be included in the DNS to target that port so the service behind it, can be nice to use to target the different nginx behind it with different html pages: (doc)[https://kubernetes.io/docs/concepts/services-networking/service/]
o here say that network plugins are required and that we are using `Calico` already installed in the cluster and it is a prerequisite
  - example from doc:
    - (Ingress rules) allows connections to all pods in the default namespace with the label role=db on TCP port 6379 from:
      - any pod in the default namespace with the label role=frontend
      - any pod in a namespace with the label project=myproject
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  ingress:
  - from:
            # we just see `namespaceSelector` and `podSelector` not the third one `ipBlock`
    - namespaceSelector:
        matchLabels:
          project: myproject
    - podSelector:
        matchLabels:
          role: frontend
    ports:
    - protocol: TCP
      port: 6379
```

eg: deny all traffic
```yaml
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: {}
  policyTypes:
  - Ingress
```

eg: ANDed vs ORed
- ANDed
```yaml
  ingress:
  # here `single` element in the `- from` (`- namespaceSelector`) which make it `ANDed`
  - from:
    # note here that here only `namespaceSelector` have the `-` which enabled the `AND`for the next rule `podSelector`
    - namespaceSelector:
        matchLabels:
          user: alice
      podSelector:
        matchLabels:
          role: client
```
- ORed
```yaml
  ingress:
  # here two elements in the `- from` (`- namespaceSelector`, `- podSelector`) which makes it ORed
  - from:
    - namespaceSelector:
        matchLabels:
          user: alice
    # note that here we have `-` which is enabling the `OR` instead of `AND`
    - podSelector:
        matchLabels:
          role: client
```

### ALLOW ALL INGRESS
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-ingress
spec:
  podSelector: {}
  ingress:
  - {}
  policyTypes:
  - Ingress
```

k run nginx-pod --port=80  --labels=location=shibuya --image=nginx
k run debug --rm -it --image=busybox --restart=Never   --namespace=nginx -- /bin/sh
k expose pod nginx   --port=80   --target-port=80  --namespace=nginx
k expose pod nginx-pod --port=80 --target=port=80 --name=nginx-service --selector=location=shibuya --type=NodePort

# networkpolicy to prevent all ingress traffic to hachiko pod and permit only the right labelled namespace and the right labelled pod
cat network-policy-aishibuya-hachiko.yaml 
# network-policy-aishibuya-hachiko.yaml NetworkPolicy
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: aishibuya-network-policy
  namespace: aishibuya
spec:
  podSelector:
    matchLabels:
      shibuya-location: hachiko
  policyTypes:
  - Ingress
  ingress:
  - from:
    # ANDed as no `-` at `podSelector`
    - namespaceSelector:
        matchLabels:
          # need to label the default namespace with that `zone=tokyo`
          zone: tokyo
      podSelector:
        matchLabels:
          shibuya-fan: access
    ports:
    - protocol: TCP
      port: 80
#spec:
#  podSelector: {}
#  policyTypes:
#  - Ingress


# pods running in defaultnamespace which will be used in `exec -it` mode to use dns lookups, busybox for nslookup and nginx pods with different labels for curl commands
cat pod_resolv_conf_services_without_netpolicy_busybox.yaml 
# pod_resolv_conf_services_having_netpolicy_busybox.yaml Pod
apiVersion: v1
kind: Pod
metadata:
  namespace: default
  name: busy-tokyo
  #name: nginx2-tokyo
spec:
  containers:
    #- name: nginx2-tokyo-connect-to-aishibuya
    #  image: nginx
    - name: busy-tokyo-connect-to-aishibuya
      image: busybox:1.35
      # this necessary as busybox exits and pod will never stay alive so we need to sleep it for a while so we can do our work
      command: ["sleep", "36000"]
  # can be `Default`, `Clusterfirst`, `ClusterFirstWithHostNet` or `None`
  # when `None` we can custom config  `resolv.comf` of the pod
  dnsPolicy: "None"
  dnsConfig:
    # obtained ip by running: `kubectl get svc -n kube-system kube-dns`
    nameservers:
      - 10.96.0.10
    # here we put what services we want to reach using Kubernetes DNS way
    searches:
      # <name_space.cluster.local>
      # OR also can add: <service_name.namespace.svc.cluster.local>
      - aishibuya.svc.cluster.local
    options:
      - name: ndots
        value: "5"

cat pod_resolv_conf_services_without_netpolicy_nginx2.yaml
# pod_resolv_conf_services_having_netpolicy_nginx2.yaml Pod
apiVersion: v1
kind: Pod
metadata:
  namespace: default
  #name: busy-tokyo
  name: nginx2-tokyo
spec:
  containers:
    - name: nginx2-tokyo-connect-to-aishibuya
      image: nginx
    #- name: busy-tokyo-connect-to-aishibuya
      #image: busybox:1.35
      # this necessary as busybox exits and pod will never stay alive so we need to sleep it for a while so we can do our work
      #command: ["sleep", "36000"]
  # can be `Default`, `Clusterfirst`, `ClusterFirstWithHostNet` or `None`
  # when `None` we can custom config  `resolv.comf` of the pod
  dnsPolicy: "None"
  dnsConfig:
    # obtained ip by running: `kubectl get svc -n kube-system kube-dns`
    nameservers:
      - 10.96.0.10
    # here we put what services we want to reach using Kubernetes DNS way
    searches:
      # <name_space.cluster.local>
      # OR also can add: <service_name.namespace.svc.cluster.local>
      - aishibuya.svc.cluster.local
    options:
      - name: ndots
        value: "5"

cat pod_resolv_conf_services_having_netpolicy.yaml 
# pod_resolv_conf_services_having_netpolicy.yaml Pod
apiVersion: v1
kind: Pod
metadata:
  namespace: default
  name: nginx-tokyo
  labels:
    shibuya-fan: access
spec:
  containers:
    - name: nginx-tokyo-connect-to-aishibuya
      image: nginx
  # can be `Default`, `Clusterfirst`, `ClusterFirstWithHostNet` or `None`
  # when `None` we can custom config  `resolv.comf` of the pod
  dnsPolicy: "None"
  dnsConfig:
    # obtained ip by running: `kubectl get svc -n kube-system kube-dns`
    nameservers:
      - 10.96.0.10
    # here we put what services we want to reach using Kubernetes DNS way
    searches:
      # <name_space.cluster.local>
      # OR also can add: <service_name.namespace.svc.cluster.local>
      - aishibuya.svc.cluster.local
    options:
      - name: ndots
        value: "5"


k get pods
NAME           READY   STATUS    RESTARTS   AGE
busy-tokyo     1/1     Running   0          93m
nginx-tokyo    1/1     Running   0          50m
nginx2-tokyo   1/1     Running   0          5s

# pods applied to the cluster with different config maps for different messages display and configmaps in one unique file
cat nginx_aishibuya_tsutaya_pod.yaml 
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: nginx
    shibuya-location: tsutaya
  name: nginx-tsutaya
  namespace: aishibuya
spec:
  containers:
  - name: nginx
    image: nginx
    volumeMounts:
    - name: creditizens-aishibuya-custom-message
      mountPath: /usr/share/nginx/html/
  volumes:
  - name: creditizens-aishibuya-custom-message
    configMap:
      name: aishibuya-tsutaya-message


cat nginx_aishibuya_hachiko_pod.yaml 
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: nginx
    shibuya-location: hachiko
  name: nginx-hachiko
  namespace: aishibuya
spec:
  containers:
  - name: nginx
    image: nginx
    volumeMounts:
    - name: creditizens-aishibuya-custom-message
      mountPath: /usr/share/nginx/html/
  volumes:
  - name: creditizens-aishibuya-custom-message
    configMap:
      name: aishibuya-hachiko-message

cat config_maps_for_aishibuya.yaml 
# config_map_for_aishibuya.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: aishibuya-hachiko-message
  namespace: aishibuya
data:
  index.html: |
    <html>
    <h1 style="color:green;">Hachiko Statute Will Be Renovated</h1>
    </br>
    <h1 style="color:green;">The Refreshed Hachiko Will Be Available From December 2025.</h1>
    </html>

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: aishibuya-tsutaya-message
  namespace: aishibuya
data:
  index.html: |
    <html>
    <h1 style="color:orange;">Starbuck Will Be Closed Till November 2025</h1>
    </br>
    <h1 style="color:red;">But Tsutaya Upper Stairs Will Still Be Open.</h1>
    </html>


# the services applied to cluster
cat hachiko-service.yaml 
apiVersion: v1
kind: Service
metadata:
  labels:
    app: nginx
  name: hachiko-service
  namespace: aishibuya
spec:
  ports:
  - name: access-hachiko
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    shibuya-location: hachiko
  type: ClusterIP

cat tsutaya-service.yaml 
apiVersion: v1
kind: Service
metadata:
  labels:
    app: nginx
  name: tsutaya-service
  namespace: aishibuya
spec:
  ports:
  - name: access-tsutaya
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    shibuya-location: tsutaya
  type: ClusterIP

k get svc -n aishibuya 
NAME              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
hachiko-service   ClusterIP   10.101.194.141   <none>        80/TCP    13m
tsutaya-service   ClusterIP   10.111.135.216   <none>        80/TCP    13m


# getting th epods to see if matching service backend (just after this step)
k get pods -o wide -n aishibuya 
NAME            READY   STATUS    RESTARTS   AGE   IP              NODE                    NOMINATED NODE   READINESS GATES
nginx-hachiko   1/1     Running   0          71m   172.16.210.82   node2.creditizens.net   <none>           <none>
nginx-tsutaya   1/1     Running   0          70m   172.16.206.71   node1.creditizens.net   <none>           <none>

# can't reach the pod that has an ingress policy because the label of the pod not matching even if the namespace label matches (we used ANDed rule)
kubectl get endpoints hachiko-service -n aishibuya
NAME              ENDPOINTS          AGE
hachiko-service   172.16.210.82:80   10s  # correctly pointing to only one pod behind the service and it is the right one (right ip, see above get pods)
k exec -it nginx2-tokyo -- bash
root@nginx2-tokyo:/# curl tsutaya-service.aishibuya.svc.cluster.local
<html>
<h1 style="color:orange;">Starbuck Will Be Closed Till November 2025</h1>
</br>
<h1 style="color:red;">But Tsutaya Upper Stairs Will Still Be Open.</h1>
</html>
root@nginx2-tokyo:/# curl hachiko-service.aishibuya.svc.cluster.local
^C
root@nginx2-tokyo:/# curl hachiko-service.aishibuya.svc.cluster.local
^C



# can access to the right pod as it is matching policy labels for namespace and pod origin
k get pods
NAME           READY   STATUS    RESTARTS   AGE
busy-tokyo     1/1     Running   0          114m
nginx-tokyo    1/1     Running   0          71m
nginx2-tokyo   1/1     Running   0          21m
k exec -it nginx-tokyo -- bash
root@nginx-tokyo:/# curl tsutaya-service.aishibuya.svc.cluster.local
<html>
<h1 style="color:orange;">Starbuck Will Be Closed Till November 2025</h1>
</br>
<h1 style="color:red;">But Tsutaya Upper Stairs Will Still Be Open.</h1>
</html>
root@nginx-tokyo:/# curl hachiko-service.aishibuya.svc.cluster.local
<html>
<h1 style="color:green;">Hachiko Statute Will Be Renovated</h1>
</br>
<h1 style="color:green;">The Refreshed Hachiko Will Be Available From December 2025.</h1>
</html>
root@nginx-tokyo:/# 

# can still nslookup but this stops at the service and gets the ips of backend and ports
k exec -it busy-tokyo -- sh
/ # nslookup _access-tsutaya._tcp.tsutaya-service.aishibuya.svc.cluster.local
Server:		10.96.0.10
Address:	10.96.0.10:53


Name:	_access-tsutaya._tcp.tsutaya-service.aishibuya.svc.cluster.local
Address: 10.103.229.167

/ # nslookup _access-hachiko._tcp.hachiko-service.aishibuya.svc.cluster.local
Server:		10.96.0.10
Address:	10.96.0.10:53

Name:	_access-hachiko._tcp.hachiko-service.aishibuya.svc.cluster.local
Address: 10.101.83.147


_____________________________________________________________________

# Init Containers & Sidecar Containers

- `Init Containers`:
  source: (init container doc)[https://kubernetes.io/docs/concepts/workloads/pods/init-containers/]
  - first `kublet` makes `network` and `storage` ready then `init container` can start
  - will always start, run, and finish running before any other container starts
  - if many defined: will start sequentially and one starts when the other is done completely
  - defined in the pod at the container level
  - it is NOT possible to define any king of `..probes` (readiness, liveliness, lifecycle...etc...)
  - BUt `resource limits` and `security` can be defined
  - can be used to set conditions to be met before `pod` starts main container app.
  - can be used to have access to some secrets, or install tools (awk, dig, python...etc...) making the app container lighter
  - can be used to wait for a `service` to be ready or even use pod ip to inject in jinja template for pod configuration necessity
  - can become a `side container` and live as long as pod is alive if a `sidecar container` is defined inside of it
  - shares same `namespace`, `network` and `volumes` (use of `emptyDir`) as the main container app. but not `probes can be defined`
  - changing the `container image` wouldn't restart `pod` but just the `container`

- `Sidecar Containers`:
  source: (sidecar doc)[https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/]
  - unlike `Init Containers` it will be started alongside normal containers
  - it is possible to define `..probe` (readiness, liveliness...etc.., lifecycle), then this probe would be used for the `pod` `probe` like if `readinessProbe` would be used for `pod` `readinessProbe`
  - shares resources with other containers of the same pod application
  - used to set logging, monitoring, data synchronization, security
  - can be an `init container` with `restatPolicy` set to `always` which will make it run during the full lifetime of the pod. OR just a normal pod which is the way to do it normally but make sure it is just to do some side stuff.
  - would be terminated after the main pod container app. and would be created just after `init containers`
  - can be defined inside an `init container` and would live as long as the `pod` is alive. so here here it is like an `init container` that would not exit and next `init container` would start without waiting for this one to exit when the `sidecar` defined inside the `init container` would start
  - lives alongside main container pod and have its own `restarPolicy` and can be `scaled` separately but shares `namespace`, `networking` and `resources` (limits, volumes...) with the `pod`. = `independent lifecycle`

## Scenario 1: Creating a Pod With Init Container Writing Custom Nginx HTML Page

So here instead of using `kind: ConfigMap` as we did in the past, we are going to use an `init container` to customize
the `nginx` `html` page.
It is like making a copy of from the `init container` pod path location of the volume having that cutom `index.html` to the `nginx` `volumeMount` `mountPath` location.
Doing a `port-forward` of the pod would provide access to the browser to show the custom page!

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: tower-109-promo-message
  namespace: department-stores
spec:
  volumes:
    - name: html-message
      emptyDir: {}

  initContainers:
    - name: init-html
      # can use custom image if want to have full control
      # meaning that here you would create a container which have copied the necessary files and just retrieve it from it
      image: busybox:1.35
      # command can be `wget` pulling file from the repo git or copy `cp` command to copy from custom image registry or other like copy from filesytem local, can be a jinja file which will be populated by the app container later...etc..
      # here we just `echo` command a sentence simply
      # best practice is to make this command `idempotent` so make sure that file doesn't already exist as we might get an error (not done in this example)
      command: ['sh', '-c', 'echo "<h1 style="color:#800f71">Shibuya 109 is Running a Sakura Promo During April: ALL 30% OFF!!</h1>" > /department-store/109/messages/index.html']
      volumeMounts:
        - name: html-109-message
          mountPath: /department-store/109/messages/

  containers:
    - name: nginx
      image: nginx
      volumeMounts:
        - name: html-109-message
          mountPath: /usr/share/nginx/html
```

- another example: cat tower-109-promo-message.yaml 
```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: tower-109-promo-message
  name: tower-109-promo-message
  namespace: department-stores
spec:
  # main container app
  containers:
    - name: nginx
      image: nginx
      volumeMounts:
        - name: html-109-message
          mountPath: /usr/share/nginx/html/

  # init container
  initContainers:
      # can use custom image if want to have full control
      # meaning that here you would create a container which have copied the necessary files and just retrieve it from it
    - name: init-109-html
      image: busybox:1.35
      # command can be `wget` pulling file from the repo git or copy `cp` command to copy from custom image registry or other like copy from filesytem local, can be a  jinja file which will be populated by the app container later...etc..
      # here we just `echo` command a sentence simply
      # best practice is to make this command `idempotent` so make sure that file doesn't already exist as we might get an error (not done in this example)
      command: ['sh', '-c', 'echo "<h1 style="color:#800f71">Shibuya 109 is running a Spring Sakura Promo: ALL -30%!!!!</h1>" > /depratment-stores/109/message/html/index.html']
      volumeMounts:
        - name: html-109-message
          mountPath: /depratment-stores/109/message/html/

  # now we need the shared volumes between both
  volumes:
    - name: html-109-message
      emptyDir: {}
```

```bash
# make it available for the browser using exposition type `port-fowrward
kubectl apply -f nginx-with-init.yaml
kubectl port-forward pod/nginx-with-init 8080:80
curl http://localhost:8080
```

### Order Of Execution
The order of `Init Containers`, `Sidecar Containers`, even `pods` is determined by the `resource/limits`
The more resources is asked the first it would be ran.
So always `Init Container` first runs, then it is checked which one has highest `resource/limits` to determine which one starts first, therefore, not rquesting resource and limits, means it is the highest.
Then `Sidecars`, and then `pods` would run and also here `resource/limits` would deternmine in which orderi
Otherwise, for each group it would be done sequentially, in the order of how those are defines in the `yaml` file's `.spec.initContainers`.

### Order of Deletion
When termination pod, first the main application container would stop, then the `sidecars` so it following the inverse order of the execution when starting the pod.

## Scenario 2: Nginx logs are being capture by a sidecar container (2 ways)
- way 1: `sidecar container` is an `init container` with `restartPolicy` equal to `always`
- way 2: `sidecar container` is a normal pod capturing nginx logs
- Both ways use the technique of setting `emptyDir: {}` for shared volumes between the two (like I do in my `Python` apps when i create a dynamic `.env` file to share data between processes)

### Way 1:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-init-sidecar
spec:
  initContainers:
    - name: log-agent
      image: busybox
      command: ["sh", "-c", "echo '###### running from sidecar Init Container Way ########' > /share/index.html && tail -f /shared/index.html"]
      volumeMounts:
        - name: shared-content
          mountPath: /shared
      restartPolicy: Always  # ✅ THIS IS CRUCIAL

  containers:
    - name: nginx
      image: nginx
      volumeMounts:
        - name: shared-content
          mountPath: /usr/share/nginx/html

  volumes:
    - name: shared-content
      emptyDir: {}
```


**VERY IMPORTANT**
- **Feature SidecarContainers need to be activated on the cluster as of the v1.28.15 of kubeadm it is not activated, after from next version it will be** 
  - **first get the `kubeadm` config file boilerplate and update it for all component activating the feature `SidecarContainers` also adding the controller ip address and the controller hostname as it is ran on the controller node**:
```bash
cat kubeadm_config_to_patch_sidecar_feature_enablement_boilerplate.yaml
```

```yaml
# get this boilerplate file that you need to update manually using: `kubeadm config print init-defaults > kubeadm-config.yaml`
apiVersion: kubeadm.k8s.io/v1beta3
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  ### need to update this to `advertiseAddress:` to the control plane api address running `k get nodes -o wide` or `hostname -I | awk '{print $1}'`
  ### advertiseAddress: 1.2.3.4
  advertiseAddress: 192.168.186.146
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/containerd/containerd.sock
  imagePullPolicy: IfNotPresent
  ### make sure the hsotname matched the controller node one `run: `hostname`` and get the result of that field here
  name: controller.creditizens.net
  taints: null
  ### this activate the sidecar feature
  kubeletExtraArgs:
    feature-gates: "SidecarContainers=true"
---
apiServer:
  timeoutForControlPlane: 4m0s
  ### this to add the feature `SidecarContainers` to the API server
  extraArgs:
    feature-gates: "SidecarContainers=true"
apiVersion: kubeadm.k8s.io/v1beta3
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
### updated controller manager from default `controllerManager: {}` to add the feature `SidecarContainers`
#controllerManager: {}
controllerManager:
  extraArgs:
    feature-gates: "SidecarContainers=true"
dns: {}
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.k8s.io
kind: ClusterConfiguration
kubernetesVersion: 1.28.0
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
### updated scheduler fromd efault `scheduler: {}` to add the feature `SidecarContainers`
#scheduler: {}
scheduler:
  extraArgs:
    feature-gates: "SidecarContainers=true"

# then run : sudo kubeadm upgrade apply <your_kubeadm_actual_version> --config=<name_of_this_pathcer_yaml_file>
```


- After that that worked fine showing that it is activated as otherwise it wouldn't accept the `restartPolicy: Always` inside the `initContainers`:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: manga-kissa-abunai
  labels:
    app: manga-kissa-abunai
spec:

  containers:
    - name: proxy-mangakissa
      image: nginx
      volumeMounts:
        - name: mangakissa-shared-volume
          mountPath: /usr/share/nginx/html/

  initContainers:
    - name: log-mangakissa-events
      image: busybox:1.35
      command: ['sh', '-c']
      args:
        - |
          mkdir -p /mangakissa/events/
          echo "*****Running From Init Container: Log-MangaKissa-Events****** \n <h1 style=\"color:red;\">Manga Kissa Abunai!</h1>" > /mangakissa/events/index.html
          sleep 3600
      volumeMounts:
        - name: mangakissa-shared-volume
          mountPath: /mangakissa/events/
      restartPolicy: Always

  volumes:
    - name:  mangakissa-shared-volume
      emptyDir: {}

```

- you can check the `real cluster config and see that `SidecarContainers` feature activation is there and that is why the pod could run without any issues
```bash
kubectl get configmap kubeadm-config -n kube-system -o yaml > real_cluster_config.yaml
cat real_cluster_config.yaml
```
```yaml
apiVersion: v1
data:
  ClusterConfiguration: |
    apiServer:
      extraArgs:
        authorization-mode: Node,RBAC
        feature-gates: SidecarContainers=true
      timeoutForControlPlane: 4m0s
    apiVersion: kubeadm.k8s.io/v1beta3
    certificatesDir: /etc/kubernetes/pki
    clusterName: kubernetes
    controllerManager:
      extraArgs:
        feature-gates: SidecarContainers=true
    dns: {}
    etcd:
      local:
        dataDir: /var/lib/etcd
    imageRepository: registry.k8s.io
    kind: ClusterConfiguration
    kubernetesVersion: v1.28.15
    networking:
      dnsDomain: cluster.local
      serviceSubnet: 10.96.0.0/12
    scheduler:
      extraArgs:
        feature-gates: SidecarContainers=true
kind: ConfigMap
metadata:
  creationTimestamp: "2023-07-07T07:39:53Z"
  name: kubeadm-config
  namespace: kube-system
  resourceVersion: "413627"
  uid: 744a0490-3f7a-4d87-bedd-4b225edd0758
```

### Way 2:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-normal-sidecar
spec:
  # so here we have two containers running and defined, one being a sidecar container
  containers:
    - name: log-agent
      image: busybox
      command: ["sh", "-c", "echo '###### running from sidecar Container Normal Way ########' > /share/index.html && tail -f /shared/index.html"]
      volumeMounts:
        - name: shared-content
          mountPath: /shared

    - name: nginx
      image: nginx
      volumeMounts:
        - name: shared-content
          mountPath: /usr/share/nginx/html

  volumes:
    - name: shared-content
      emptyDir: {}
```

- another example that works fine and contextualized:
```bash
cat sidecar_container_as_sidecar_container_the_normal_one.yaml 
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: tokyo-tower-sidecar-info
spec:
  # so here we have two containers running and defined, one being a sidecar container
  containers:
    - name: info-of-the-day
      image: busybox
      command: ["sh", "-c", "echo '###### Tokyo Tower Will Be Opened On The 1st April and This is Not a Joke! ########' > /tokyo-towaaa/info/index.html && tail -f /dev/
null"]
      volumeMounts:
        - name: info-message
          mountPath: /tokyo-towaaa/info/

    - name: nginx
      image: nginx
      volumeMounts:
        - name: info-message
          mountPath: /usr/share/nginx/html

  volumes:
    - name: info-message
      emptyDir: {}
```

### Yaml command to container different ways to do it
. 1) using `args` to put all commands to run `sh -c` accepts command in one line with `&&` as well we will see it after:
```yaml
command: ["sh", "-c"]
args:
  - |
    echo "First command"
    echo "Second command"
    echo "Third command"
```

. 2) as a single line:
```yaml
command: ["sh", "-c", "echo one && echo two && echo three"]
sh -c lets you run multiple shell commands in a single string.
```

. 3) Use a shell script
```yaml
command: ["sh", "/scripts/startup.sh"]
```

### Debug command
- if issues: `kubectl debug pod/manga-kissa-abunai -it --image=busybox --target=log-mangakissa-events`
- make sure you have enabled the feature in your cluster to run sidecars:
```bash
sudo nano /var/lib/kubelet/config.yaml
# then add this:
featureGates:
  SidecarContainers: true
# then restart kubelet
sudo systemctl restart kubelet
```

## REMINDER ON PODS RESTART POLICIES VALUE EFFECTS (ChatGPT)
1. Always (default for Pods)
- Container restarts automatically on failure or exit.
- Used for long-running containers, like web servers.
- Required for Deployments, ReplicaSets, etc.

2. OnFailure
- Restart only if container exits with non-zero code (error).
- Does not restart if the container exits cleanly (exit 0).
- Used in Jobs (batch processing).

3. Never
- Never restarts, no matter how the container exits.
- Used when you want one-shot containers (manual runs, debugging).

_________________________________________________________________

# STORAGE

## Persistant Volumes (PVs)
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0003
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: slow
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    path: /tmp
    server: 172.17.0.2
```


Like when a pod is created it request cpu and memory from the node,
here the `PVs` are the resources configured by th ecluster admin and `PVCs` (persistant volume claims) are like the pods requestiong for resources to be mounted.
Different access depending on what it is wanted to be done, but not only,
also depends on the resource that is used for storage,
like some accept multiple entries while others have some different specifics:
**Important those are NOT GUARANTEED as no constraint is put by Kubernetes on those volumes**
- ReadWriteOnce (RWO): `single node` mount, can be `red and written` by all pods living on that node.
- ReadOnlyMany (ROX): this volume can be mounted as `read only` by `many modes`
- ReadWriteMany (RWX): this volume can be mounted as `read and write` by `many modes`
- ReadWriteOncePod (RWOP): this volume ensures that accros the whole cluster `only one pod` can `read and write` on the volume
source: (Check doc to see table of volumes and what access mode they support or not)[https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes]

eg: `hostPath` is supporting only `ReadWriteOnce`
and this is what we are probably going to use as we are running `kubeadm` locally
and not in the cloud (no `ebs` volumes for eg.) 

### Storage Class of PVs
`PVs` with annotation (might become deprecated) `volume.beta.kubernetes.io/storage-class`
or `storageClassName` (more actual way of doing it) would only be mounted by claims matching those
otherwise `PVs` without it would be bound to `PVCs` that do not specify storage class.

### Reclaim Policy
- Retain -- manual reclamation
- Recycle -- basic scrub (rm -rf /thevolume/*) and this only for `nfs` and `hostPath` types of volumes
- Delete -- delete the volume

### Affinity
Can be set only when using `local`(which can only be a static PV and not Dynamic one) `PVs`.

### Example `YAML` file showing those previous concepts with field defined
source: (PVs type: local)[https://kubernetes.io/docs/concepts/storage/volumes/#local]
source: (STorageClass Creation)[https://kubernetes.io/docs/concepts/storage/storage-classes/#local]

so when using `local` types of volumes we **MUST** set node affinity!
here we do use example from documentation using `local` volumes to set `volumeBindingMode` set to `WaitForFirstConsumer` in the first `yaml` part.
Other than that we could use are `hostPath` and `emptyDir`. Those could also be used for SSD/USB/FIlePath and depends on underlying system access to those.

```yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner # indicates that this StorageClass does not support automatic provisioning
volumeBindingMode: WaitForFirstConsumer

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: example-pv
# probably goes here
spec:
# here we set the capacity of this volume made available in the cluster to pods (in nodes following the defined affinity here)
  capacity:
    storage: 100Gi
# `FileStystem` if using file system otherwise use `Block` as well for hard drives probably
  volumeMode: Filesystem
# the access mode RWO
  accessModes:
  - ReadWriteOnce
# the reclaim policy type set to `Delete`
  persistentVolumeReclaimPolicy: Delete
# the `storageClassName` defined in the above `yaml`
  storageClassName: local-storage
# using here `local` which makes us then obliged to use node affinity
  local:
    path: /mnt/disks/ssd1
# here creating the node affinity constraint
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - example-node
```

### Little Schema To understand
-> StorageClass
  -> PV uses that storage class and defines volume size, access mode, reclaim policy, node affinity (if local type of volume)
    -> pods request the PV with the right storage class

**Note:** Delaying volume binding ensures that the PersistentVolumeClaim binding decision will also be evaluated with any other node constraints the Pod may have,
such as node resource requirements, node selectors, Pod affinity, and Pod anti-affinity.

### IN THE CLOUD
In the cloud it is different than locally running a Kubernetes cluster as for the `CSI` (Container Storage Interface) different drivers would be used
so need to check on the documentation and also what is possible to do and not.
It works like that:
- `CSI` driver is deployed in the Kubernetes cluster
- then from that moment the Cloud volumes would be available to be mounted and used. After depends on which ones are available
- different Cloud Providers have different settings in how many volumes MAX could be attached to a single node.
- Need also to check on that. eg: x36 `EBS` volumes for `AWS` on each node and there is an env var that can be modified to have control on that...check docs!.

### PVs Phases
those are the different states that PVs can have:
- `Available`: a free resource that is not yet bound to a claim
- `Bound`: the volume is bound to a claim
- `Released`: the claim has been deleted, but the associated storage resource is not yet reclaimed by the cluster
- `Failed`: the volume has failed its (automated) reclamation



## Persistant Volume Cliam PVCs
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 8Gi
  storageClassName: slow
  selector:
      release: "stable"
    matchExpressions:
      - {key: environment, operator: In, values: [dev]}
```

### Access modes and Volumes Types
Same as `PVs` ones

### Resources
Here is the difference as this is a `claim` the pod would request for a certain amount of resources (like a pod would do to request cpu and memory).

### Selectors
To match a set of specific volumes, volumes can be labelled and the `PVC` would use a selector like we do with pods.
It is ANDed:
- matchLabels - the volume must have a label with this value
- matchExpressions with operators: In, NotIn, Exists, and DoesNotExist

### Class 'storageClass'
a `PVC` can actually specify a storage class by name using: `storageClassName`
if `storageClassName: ""`:
- it will be set for `PV` taht do not have any storage class name
  - if `DefaultStorageClass` have been enabled in the kubernetes cluster.
    done by adding annotation: `storageclass.kubernetes.io/is-default-class:true` (will be deprecated, better to use `storageClassName`):
    - the `storageClassName: ""` request would be bounded to the default `storageClass` set by the kubernetes admin
  - if `DefaultStorageClass` is not enabled: the `PVC` would be bound to the latest `PV` created. Order is from the newest to the oldest if many.
    and those `PVs` need to also have `storageClassName: ""`.

Some other rules:
- can create `PVC` without `storageClassName` only when the `DefaultStorageClass` is not enabled.
- if no `StorageClassName` defined when creating a `PVC` and then you enable `DefaultStorageClass`, kubernetes would set `StorageClassName: ""` to those `PVCs`
- if `StorageClassName: ""` defined in `PVC` and then you enable `DefaultStorageClass`, kubernetes won't update those `PVCs` as those are fine with the right `:""`

## Namespaced or not?
- `PVs` are NOT namespaced
- `StorageClasses` are NOT namespaces
- `PVCs` are YES namespaces
```bash
# k api-resources | grep "storageclasses"
storageclasses                    sc           storage.k8s.io/v1                      false        StorageClass

# k api-resources | grep "pv"
persistentvolumeclaims            pvc          v1                                     true         PersistentVolumeClaim
persistentvolumes                 pv           v1                                     false        PersistentVolume
```

## Claim as Volume
source: (claims as volume)[https://kubernetes.io/docs/concepts/storage/persistent-volumes/#claims-as-volumes]
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim
```

## Raw Block as Volume
**Note:** Here instead of using `mountPath` on the pod `volume` we use `devicePath`
- `PV`
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: block-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  volumeMode: Block
  persistentVolumeReclaimPolicy: Retain
  fc:
    targetWWNs: ["50060e801049cfd1"]
    lun: 0
    readOnly: false

```
- `PVC`
```yaml apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: block-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Block
  resources:
    requests:
      storage: 10Gi
```
- `Pod`
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-block-volume
spec:
  containers:
    - name: fc-container
      image: fedora:26
      command: ["/bin/sh", "-c"]
      args: [ "tail -f /dev/null" ]
      volumeDevices:
        - name: data
# here we use `devicePath` instead of `mountPath`
          devicePath: /dev/xvda
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: block-pvc
```

## Enabling `--feature-gates` to make Cross-Namespace Volumes Possible [`Alpha`]
source: (cross namespace volumes)[https://kubernetes.io/docs/concepts/storage/persistent-volumes/#cross-namespace-data-sources]
Kubernetes supports cross namespace volume data sources.
To use cross namespace volume data sources, 
you must enable the AnyVolumeDataSource and CrossNamespaceVolumeDataSource feature gates for the kube-apiserver and kube-controller-manager. 
Also, you must enable the CrossNamespaceVolumeDataSource feature gate for the csi-provisioner.

Enabling the CrossNamespaceVolumeDataSource feature gate allows you to specify a namespace in the dataSourceRef field.


## Strictly binding a PVC with a PV
**Good if `PV` set `persistentVolumeReclaimPolicy: Retain`** 
source: (doc)[https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims]

- this will not strictly bind it:
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: foo-pvc
  namespace: foo
spec:
  storageClassName: "" # Empty string must be explicitly set otherwise default StorageClass will be set
  volumeName: foo-pv
  ...
```
- this would strictly bind it by reserving that `PV` to that `PVC` using `claimRef`:
Therefore, here it has to be set also on the `PV` side the `claimRef` referencing the corresponding `PVC`
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: foo-pv
spec:
  storageClassName: ""
# so here we use `clainRef` to bind the `PV` to a claim
  claimRef:
    name: foo-pvc
    namespace: foo
  ...
```
- Recap comnditions for this to work:
`PVC` -> referencing the `PV` using `volumeName`
`PV` -> referencing `PVC` using `claimRef`

## RECAP (ChatGPT)
### Dynamic Provisioning.
Here kubernetes creates the `PV` **automatically**

**Define:**
- a StorageClass
- a PVC (that refers to that StorageClass)
- and a Pod that uses the PVC
- Then Kubernetes automatically creates the PersistentVolume (PV)
- no need to pre-create a PV.


**Why create PVs manually (Static Provisioning)?**
Only create a PV manually when:
- Static Provisioning: have pre-existing storage (e.g., a mounted NFS share, disk partition, etc.) and want to bind it manually.
- Want to control which pod gets which exact volume (e.g., binding a specific disk to a specific application).
- For air-gapped clusters or restricted environments where can't use dynamic storage backends.
- For on-premise storage where the admin provisions and maintains volumes manually.


**So two options:**
|Option	|What You Define	|Who Creates the PV	|Use Case |
+-------+-----------------------+-----------------------+---------+
|Dynamic Provisioning	|StorageClass + PVC	|Kubernetes (automatically)	|Most common, easy, scalable|
+-----------------------+-----------------------+-------------------------------+---------------------------+
|Static Provisioning	|PV + PVC	|You (admin)	|Pre-provisioned disks, special use cases|
+-----------------------+---------------+---------------+----------------------------------------+

**Can a Pod bind to a specific PV?**
Indirectly, yes — by:
- Creating a PVC with:
  - the same:
    - storage size
    - accessModes
  - and optionally matching the volumeName or selector labels used by the PV

Then the PVC will bind to that specific PV.

# When using `WaitForFirstConsumer` in `StorageClass`, you need to:
**Important:**
Let's say we are in the example of some `scheduling` rules for pods that need to be in certain zones or nodes in the world.
Now the default behavior is `immediate` binding of the `PVC` to available volumes `PVs`. But this would by-pass the `scheduling` requirements.
Therefore, an issue as you won't get your workload following the rule set for the `scheduler` in the `pod`. 
This is when we use `WaitForFirstConsumer` for the `scheduler` to be taken into consideration by delaying the volume binding.
Now it is given the time to understand the zones and nodes, the resouces limits (taint/toleration, affinity and more ...) in each for each pods and the full environemnt for a good scheduling of the volumes.
So here to resume we are not by-passing topology rules and scheduler contraints.

- `StorageClass`: use of `WaitForFirstConsumer` with `allowedTopologies`
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
provisioner:  example.com/example
parameters:
  type: pd-standard
volumeBindingMode: WaitForFirstConsumer
allowedTopologies:
- matchLabelExpressions:
  - key: topology.kubernetes.io/zone
    values:
    - us-central-1a
    - us-central-1b
```

- Pod: not using `nodeName` but `hostName` in selector when not using affinity
Here we can see the way to use `nodeSelector` as well
```yaml
spec:
  nodeSelector:
    kubernetes.io/hostname: kube-01
```

# some extra to know
- `StorageClass`(storageclass.yaml) is not indicating the `capacity`, but the `PVC`(pvc.yaml) would indicate `capacity` (`resources` -> `request` -> `storage`)
  and then the pod would reference that `PVC`.
  This is how the `Pod` would get request resources satisfied and volume mounted ('persistentVolumeClaim' -> `claimName`). 
  `PV` would be created automatically (`Dynamic`) by kubernetes.
- You need to install `CSI` drivers if you want to extend storage to external ones (like they do in the cloud,
  see next the example with local `S3` like volume using `Minio` which can listen to a directory path...)



# example `S3` like volume using `Minio`
What is interesting with `Minio` is that it can listen to a folder path if it is used for it's bucket path
So here the solution would  be to install a `CSI` criver for the `s3` like volume.
and then have a local `Minio` OR `external` to the cluster listening on a node folder path or internal to the cluster sharing volume with the host node and being
used as `sidecar` container.
- **Deploy the `CSI` driver**: follow installation (instructions)[https://github.com/ctrox/csi-s3] of this `csi-s3` available on `github`
- **Create StorageClass**: this `StorageClass` would be using it:
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: s3-storage
provisioner: s3.csi.k8s.io
parameters:
  bucket: my-bucket
  region: us-east-1
  mounter: rclone
  options: --s3-endpoint=https://s3.amazonaws.com
```
- **Create `PVC`**: this `PVC` would reference the `StorageClass`
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: s3-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
  storageClassName: s3-storage
```
- And after just mount the volume to your `Pod`.
- But actually this is where it is interesting as you don't even need to do that if is an `object store` like `S3` or `Minio`.
  All you need here is to use the trick of shared volumes and use local `Minio` path or `S3 SDK` or `sidecar` using `mc` (Minio Client)
Minimal eg:. might need another `sidecar` for `sync data` with `Minio` server.
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: app-with-minio-sidecar
spec:
  containers:
    - name: my-app
      image: my-app-image
      volumeMounts:
        - name: shared-data
          mountPath: /app/data
    - name: minio-sidecar
      image: minio/mc
      command: ["/bin/sh", "-c"]
      args:
        - |
          mc alias set myminio http://minio.default.svc.cluster.local:9000 minio minio123
          mc cp --recursive myminio/my-bucket /shared-data
          tail -f /dev/null
      volumeMounts:
        - name: shared-data
          mountPath: /shared-data
  volumes:
    - name: shared-data
      emptyDir: {}
```

- summary by `ChatGpt`:
Can I use it locally in kubeadm with MinIO? Yes! You can:
- Deploy MinIO as a pod.
- Use an S3 CSI driver that works with any S3-compatible storage (like MinIO).
- Or use a sidecar container that downloads/upload files to MinIO.



# Full example using locally `allowedTopologies` 

## Static Way:

- label nodes
```bash
kubectl label node node1 topology.kubernetes.io/zone=us-central-1a
kubectl label node node2 topology.kubernetes.io/zone=us-central-1b
```
- create `StorageClass`:
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard-local
provisioner: kubernetes.io/no-provisioner  # no dynamic provisioning
volumeBindingMode: WaitForFirstConsumer    # bind PV only when pod is scheduled
allowedTopologies:
  - matchLabelExpressions:
      - key: topology.kubernetes.io/zone
        values:
          - us-central-1a
          - us-central-1b
```
- create a `PV` (manually not automatic which gives more control to admin, called `static` way):
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-1
spec:
  capacity:
    storage: 2Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: standard-local
  local:
    path: /mnt/disks/ssd1  # you can `mkdir -p` this on the host manually
  nodeAffinity:
    required:
      nodeSelectorTerms:
        - matchExpressions:
            - key: topology.kubernetes.io/zone
              operator: In
              values:
                - us-central-1a
```
- create a `PVC`:
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-local-claim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
  storageClassName: standard-local
```
- create a `Pod`:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: local-storage-pod
spec:
  containers:
    - name: app
      image: nginx
      volumeMounts:
        - mountPath: /usr/share/nginx/html
          name: data
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: my-local-claim
```

## Dynamic Way

- install `CSI` driver for local provisioner from github (`rancher`)[https://github.com/rancher/local-path-provisioner]
It registers a StorageClass named local-path which can be used for dynamic provisioning
```bash
kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
```
- Change the `StorageClass` with the `CSI` driver deployed to the cluster
```
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: dynamic-local
provisioner: rancher.io/local-path  # Or any CSI driver available in your cluster
volumeBindingMode: WaitForFirstConsumer
```
- get rid of the previously created `PV` and use the `PVC` and `Pod` (can change name if wanted to):
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-claim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: dynamic-local
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-using-dynamic
spec:
  containers:
    - name: app
      image: nginx
      volumeMounts:
        - mountPath: /usr/share/nginx/html
          name: web-storage
  volumes:
    - name: web-storage
      persistentVolumeClaim:
        claimName: my-claim
```

# Storage Scenarios
- 1: create volumes `static way` with `storageClassName: ""`
     show also `claimRef` for strictly binded `PV` to `PVC`
explain:
-> StorageClass
  -> PV uses that storage class and defines volume size, access mode, reclaim policy, node affinity (if local type of volume)
    -> pods request the PV with the right storage class


- 2: create default `StorageClasss` in cluster and show behavior of `storageClassName: ""`
explain:
a `PVC` can actually specify a storage class by name using: `storageClassName`
if `storageClassName: ""`:
- it will be set for `PV` that do not have any storage class name
  - if `DefaultStorageClass` have been enabled in the kubernetes cluster.
    done by adding annotation: `storageclass.kubernetes.io/is-default-class:true` (will be deprecated, better to use `storageClassName`):
    - the `storageClassName: ""` request would be bounded to the default `storageClass` set by the kubernetes admin
  - if `DefaultStorageClass` is not enabled: the `PVC` would be bound to the latest `PV` created. Order is from the newest to the oldest if many.
    and those `PVs` need to also have `storageClassName: ""`.

`WaitForFirstConsumer` and `affinity` to show how the pods are not respecting affinity when it is not set and how it by-passes the `scheduler`
can create `PVC` without `storageClassName` only when the `DefaultStorageClass` is not enabled.
if no `StorageClassName` defined when creating a `PVC` and then you enable `DefaultStorageClass`, kubernetes would set `StorageClassName: ""` to those `PVCs`
if `StorageClassName: ""` defined in `PVC` and then you enable `DefaultStorageClass`, kubernetes won't update those `PVCs` as those are fine with the right `:""`


- 3: create `StorageClass` static with `allowedTopologies`
- 4: create `dynamic` way `StorageClass`

explain: `StorageClass`(storageclass.yaml) is not indicating the `capacity`, but the `PVC`(pvc.yaml) would indicate `capacity` (`resources` -> `request` -> `storage`)
  and then the pod would reference that `PVC`.
  This is how the `Pod` would get request resources satisfied and volume mounted ('persistentVolumeClaim' -> `claimName`).
  `PV` would be created automatically (`Dynamic`) by kubernetes.

- 5: do maybe something different to show when we use or not `volumeBindingMode: WaitForFirstConsumer` on `StorageClass` just to focus on:
    `Selector`(on `PVCs`),
    `afinity`(on `PVs` to restrict on which nodes can use this `PV`),
    `allowedTopologies`(on `StorageClass` which are not namespaced but could be used to have restriction in which nodes those classes can be used using `allowedTopologies`)
    `nodeSelector`, `affinity`, or `topologySpreadConstraints` (on `Pod` in combinaison with the other ones to have fine grained control on where scheduler lands `Pods`)
    ``
+---------------+-------+----+
| Field/Concept	| PVC	| PV |
+---------------+-------+----+
| Namespaced	| ✅ Yes| ❌ No |
+---------------+-------+-------+
| selector:	| ✅ Yes (to match PV labels)	| ❌ No |
+---------------+-------------------------------+-------+
| labels:	| ✅ Yes	| ✅ Yes |
+---------------+---------------+--------+
| nodeAffinity	| ❌ No	| ✅ Yes |
+---------------+-------+--------+
| storageClassName:	| ✅ Yes | ✅ Yes |
+-----------------------+--------+--------+

- static: here create my `PV` and introduce the use of `StorageClass` and just use the strictly binded rule and can introduce the idea of `default storage class`
- dynamic: here don't create `PV` and `useStorageClass` only and use the affinity, topology examples and show how scheduler is skipped

static > StorageClass > Dynamic > Topology > Affinity + Topology > CSI Driver (Rancher)

# capacity 
PersistentVolume and PersistentVolumeClaim definitions:

Unit	Meaning	Notes
Ki	Kibibyte (2¹⁰ bytes)	1 Ki = 1024 bytes
Mi	Mebibyte (2²⁰ bytes)	1 Mi = 1024 Ki = 1,048,576 bytes
Gi	Gibibyte (2³⁰ bytes)	1 Gi = 1024 Mi = 1,073,741,824 bytes
Ti	Tebibyte (2⁴⁰ bytes)	Rarely used in small clusters
Pi	Pebibyte (2⁵⁰ bytes)	Very large storage
Only binary SI units (with i) like Gi, Mi, Ki are supported and recommended in kubernetes `yaml`.

# scenario 1 (Static)
here we have affinity set on the `PV` and at the same time in the `StorageClass` we will set the `volumeBindingMode: WaitForFirstConsumer` 
so that scheduler will have the responsibility to check on nodes available and not be bypassed

- `cat storage-class-waitforfirstconsumer.yaml`
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
# indicates that this StorageClass does not support automatic provisioning
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
```

- `cat pv-local.yaml` 
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-with-mandatory-affinity-set
spec:
  capacity:
    storage: 512Ki
  # use `Block` for SSDs for eg:.
  volumeMode: Filesystem
  accessModes:
    # RWO: `single node` mount, can be `red and written` by all pods living on that node
    - ReadWriteOnce
  # `Delete/Retain also Recycle but be carefull as it uses `rm -rf` and is only for `nfs` and `hostPath``
  persistentVolumeReclaimPolicy: Delete
  # play with this field to show behavior of by-passing scheduler and also another of `DefaultStorageClass`
  # storageClassName: ""
  storageClassName: local-storage
  # using here `local` which makes us then obliged to use node affinity
  local:
    # this path need to be created manually on node
    path: /tmp/local-pv
  # here creating the node affinity constraint
  nodeAffinity:
    # required OR preferred
    required:
      nodeSelectorTerms:
      - matchExpressions:
        # kubernetes ones
        #- key: kubernetes.io/hostname
        # custom
        - key: location
          operator: In
          values:
          - shizuoka
```

- `cat pvc-without-selector.yaml`
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-scheduled-in-node-affinity-defined-by-pv-affinity
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 512Ki
  storageClassName: local-storage
```

- `cat pod-requesting-storage.yaml` 
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-needing-storage
spec:
  containers:
    - name: i-need-storage-pod
      image: nginx
      volumeMounts:
      # here using mountpath
      - mountPath: "/tmp/local-pv"
        name: my-local-storage
  volumes:
    - name: my-local-storage
      persistentVolumeClaim:
        claimName: pvc-scheduled-in-node-affinity-defined-by-pv-affinity
```

- one `node2`: 
```bash
mkdir /tmpt/local-pv
````
- label `node2`:
```bash
k label node node2.creditzens.net lcoation=shizuoka
```
- make sure the node is labelled acording to the affinity of the pod and then apply `StorageClass`, `PV`, `PVC` and `Pod` to the cluster
```bash
k apply -f <filename>
... # do the same will all of those files
```
- here normally the pod will be scheduled in node2 which has the label corresponding to the affinity of the `pv`
```bash
 k get pods -o wide
NAME                  READY   STATUS    RESTARTS   AGE    IP               NODE                    NOMINATED NODE   READINESS GATES
pod-needing-storage   1/1     Running   0          153m   172.16.210.124   node2.creditizens.net   <none>           <none>
```
- then `exec` in `pod` to create a file with content at the volume location `/tmp/local-pv`
```bash
k exec -it pod-needing-storage -- bash
root@pod-needing-storage:/# echo "junko shibuya" > /tmp/local-pv/junko-location-now.txt
```
- now go in `node2` and you will see the file and its content in the `pv` volume located at `/tmp/local-pv`
```bash
creditizens@node2:~$ cat /tmp/local-pv/junko-location-now.txt 
junko shibuya
```
**Note:**:
- the `PV` and `PVC` would still reference to each others and be bound even if pod is deleted, so they need to be delete separately and manually
-  the volume on the `node2` is not deleted by kubernetes as it is made to persist if there is `pod` failure. so need also to be deleted manually.


# Scenario 2 (From Static to Dynamic):

Here will be showing:
- how pv bind to storageclasses: using `storageClassName: ""` and `# storageClassName: ` not set
- how we need a provisioner and install rancher or show how to install it, then use dynamic provisioning showing no need to create `PV`
- after can maybe show more Topology and how to use it
- after move to `Dynamic` fully and now no more `affinity` to control where pod would be deployed, no more control but use `Topology` on `StorageClass`
  to show that this is a way to control where those pods would be deployed.
always show where is the volume created as with `rancher` `local-path` provisioner we don't get it at the path of `PV` as here it is dynamic and 
we don't create `PVs` so the provisioner will be putting the volumes at `/opt/local-path-provisioner/pvc-<numbers>...`.
So need here to describe the `PV` created automatically by the provisioner.

### install `local-path` provisioner (Rancher)
```bash
kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
```

### patch it as `default`
```bash
kubectl patch storageclass local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
```

### use `storageClassName: local-path`
in your created `PVC` use `storageClassName: local-path`

- `PVC`s
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-following-node-affinity-defined-in-pv-with-or-without-storageclass
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 512Ki
  # if no storageClassName defined kubernetes assumes `Dynamic` type of provisioning so need to change the provisioner on StorageClass
  # storageClassName: ""
  # this is the rancher Dynamic provisioner
  storageClassName: local-path

  # `selector` can be defined with `matchLabels` and `matchExpressions`
```

- `PV`s
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-with-mandatory-affinity-set
spec:
  capacity:
    storage: 512Ki
  # use `Block` for SSDs for eg:.
  volumeMode: Filesystem
  accessModes:
    # RWO: `single node` mount, can be `red and written` by all pods living on that node
    - ReadWriteOnce
  # `Delete/Retain also Recycle but be carefull as it uses `rm -rf` and is only for `nfs` and `hostPath``
  persistentVolumeReclaimPolicy: Delete
  # play with this field to show behavior of by-passing scheduler and also another of `DefaultStorageClass`
  #storageClassName: ""
  storageClassName: local-storage
  # using here `local` which makes us then obliged to use node affinity
  local:
    # this path need to be created manually on node
    path: /tmp/local-pv
  # here creating the node affinity constraint
  nodeAffinity:
    # required OR preferred
    required:
      nodeSelectorTerms:
      - matchExpressions:
        # custom
        - key: location
          operator: Exists
```
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-2-with-mandatory-affinity-set
spec:
  capacity:
    storage: 512Ki
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  #storageClassName: ""
  # storageClassName: local-storage
  local:
    path: /tmp/local-pv
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        # custom
        - key: location
          operator: Exists
```
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-3-with-mandatory-affinity-set
spec:
  capacity:
    storage: 512Ki
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: ""
  # storageClassName: local-storage
  local:
    path: /tmp/local-pv
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        # custom
        - key: location
          operator: Exists
```

- `StorageClass`es

__________________________________________________________________
# Next
- [ ] do those kubernetes concepts:
    - [ ] Storage, Backup and Recovery
    - [ ] Resources Limits
    - [ ] Cronjob, Jobs
    - [ ] Damonsets
    - [ ] Kubernetes Kustomize
    - [ ] Helm
- [ ] update the cluster versions until we reach 1.32 (we are at 1.27)
    so we will have to do same process several times and fix any compatibility issues along the way.
    need to check supported versions ranges for each kubeadm updated version
- [ ] create thsoe scripts in bash that will upgrade controller node and worker nodes
      and then create an ansible playbook having variables set ina file for containerd version and kubeadm version to upgrade nodes
      then create a rust app that would accept as input terminal variables the versions and start the ansible playbook
      -  probably need to use ansible until it works to automate one version upgrade 1.28 to 1.29 fully
      -  then create the rust application as it would be easier as we are sure that the playbook is not failing otherwise too much debugging layers.
